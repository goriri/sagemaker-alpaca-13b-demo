{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b666ee7",
   "metadata": {},
   "source": [
    "# Running stanford_alpaca on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae555b",
   "metadata": {},
   "source": [
    "This is a sample code to run stanford_alpaca on Amazon SageMaker, for demo or research use only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5898f9b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.146.0.tar.gz (718 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.5/718.5 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.26.71)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.71 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.71)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.71->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.146.0-py2.py3-none-any.whl size=964936 sha256=c0f4b2d2051f88eadc74daf58f1c91368e4c86cf6993ef37c91f5aba83f35759\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/08/f6/9a/3abd169a1b427683e78872b737fbab7831c8310fbec4c0acef\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "Successfully installed sagemaker-2.146.0\n"
     ]
    }
   ],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6387eff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de03012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stanford_alpaca'...\n",
      "remote: Enumerating objects: 111, done.\u001b[K\n",
      "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 111 (delta 49), reused 47 (delta 45), pack-reused 48\u001b[K\n",
      "Receiving objects: 100% (111/111), 9.14 MiB | 4.85 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n"
     ]
    }
   ],
   "source": [
    "## download training script from github\n",
    "!git clone https://github.com/tatsu-lab/stanford_alpaca.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f35cec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"instruction\": \"Give three tips for staying healthy.\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"What are the three primary colors?\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"The three primary colors are red, blue, and yellow.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Describe the structure of an atom.\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"How can we reduce air pollution?\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Describe a time when you had to make a difficult decision.\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Identify the odd one out.\",\n",
      "        \"input\": \"Twitter, Instagram, Telegram\",\n",
      "        \"output\": \"Telegram\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Explain why the following fraction is equivalent to 1/4\",\n",
      "        \"input\": \"4/16\",\n",
      "        \"output\": \"The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Write a short story in third person narration about a protagonist who has to make an important career decision.\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"John was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \\n\\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\\n\\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Render a 3D model of a house\",\n",
      "        \"input\": \"\",\n"
     ]
    }
   ],
   "source": [
    "## check data\n",
    "!head -n 44 stanford_alpaca/alpaca_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3dca9",
   "metadata": {},
   "source": [
    "**Generate sample dataset for debug use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841d6c55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_dataset.json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Give three tips for staying healthy.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the three primary colors?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The three primary colors are red, blue, and yellow.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe the structure of an atom.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How can we reduce air pollution?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe a time when you had to make a difficult decision.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Identify the odd one out.\",\n",
    "        \"input\": \"Twitter, Instagram, Telegram\",\n",
    "        \"output\": \"Telegram\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain why the following fraction is equivalent to 1/4\",\n",
    "        \"input\": \"4/16\",\n",
    "        \"output\": \"The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short story in third person narration about a protagonist who has to make an important career decision.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"John was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \\n\\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\\n\\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88cf40",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3081c5b",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a9df6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (4.63.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.8)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.13.4\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0239c2d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.040936946868896484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 48 files",
       "rate": null,
       "total": 48,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ada0a2556314176a898776c01205361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 48 files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.05298018455505371,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)5d4de5ce/config.json",
       "rate": null,
       "total": 427,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1fce289e4b4ea2841c0c4881439edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5d4de5ce/config.json:   0%|          | 0.00/427 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07458114624023438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 124,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e76e8f380174ccd9ba1814393f342ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03794097900390625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00002-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31cba363ea94ef7a400d853a1e2f02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04487872123718262,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00004-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60025c12ccd4064b9c793c9ed9713b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0348358154296875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00001-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d66dfca9b64f7388a0bff39d12d54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031476736068725586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00000-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67c148d1f5e47eca088bdca4add76e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00000-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.044585466384887695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00005-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd51c338dab498abeaf867d8559f636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011979103088378906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00003-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557a031a66434e2495c49ef8e9d98487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018284320831298828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00007-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d785d2c51b7943d68f90fc3ec53ec5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022661447525024414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00006-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519399b7807841dc893335f64125f1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008650779724121094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00008-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662eaa4ef5ce4b1fbd4a2061494bcd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011261463165283203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00009-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1c1efe8cc54c5e88b76bda365ef3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00009-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014629125595092773,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00010-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb92f146ad6042ed8f5e7e7ff6f4dc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00010-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011966943740844727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00011-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c0dadae5424117b326aa59d6a4a863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00011-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0103607177734375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00012-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eef3d13ffd64a3892387a793826fc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00012-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007249593734741211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00013-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16897f2f6fb540a9b8ef3459a07c0294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00013-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010473489761352539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00014-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cc7248bb0f467abf7e81428fe538e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00014-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010521173477172852,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00015-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0a7a7200014c749bf2922f9332f23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00015-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01663804054260254,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00016-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37be8a997fca4a9bba2bebe6e7a0158e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00016-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018157243728637695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00017-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73b73f3d3c04f40b2ac0f192f6f7af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00017-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014000177383422852,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00018-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b59b551a6d4b58b100d8ec0ce84fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00018-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010498046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00019-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d560cbf46b04800a3629ab8215a37cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00019-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007833242416381836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00020-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2402798d459e4efb9fb3092406c232fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00020-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008484840393066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00021-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392f6a6b37094eb5bef2abbd6fe10312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00021-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012969970703125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00022-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12286d21e5934d988b161647532233e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00022-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010684013366699219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00023-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5685720e50485ab3edb9be99372326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00023-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012723207473754883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00024-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0380f4e994647bbbb30953677225c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00024-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007920980453491211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00025-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8110ce2e0541498995f1676ffa9c3c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00025-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013361454010009766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00026-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4116177b5ed246c8948470aeea5c8f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00026-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011435747146606445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00027-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55b1ddf65a14fef9173cd5fdc56b751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00027-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007740020751953125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00028-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f87025371447399ac97ce088b9b85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00028-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008211612701416016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00029-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d71e4dc78a4002a5ca7812b3d7d1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00029-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019795656204223633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00031-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c8bad0ad2c4c0a80c1f78c164f58b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00031-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019674062728881836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00030-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68f9c92df414d9ca0e3795862da91a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00030-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011325836181640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00033-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e01a3eef7145ffa180a68722a5125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00033-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01817464828491211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00032-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7810aae4cf49e19a9d6770d1d9f874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00032-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016881704330444336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00034-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b688c5bebea4690820adf5f9df78985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00034-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016028404235839844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00035-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fed7ee4af1647ea8c1baeeda803550a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00035-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009464025497436523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00036-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0f6bf86e17405a84b5813445d33a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00036-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008689165115356445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00037-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef1fa50f7a340f3b473071115d7d446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00037-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011987924575805664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00038-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c034433e1ba04621a3a92395a2b5e210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00038-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013681650161743164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00039-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd78b9dcde545269be7e93f92d2dfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00039-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015734434127807617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00040-of-00041.bin",
       "rate": null,
       "total": 951607026,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3ffa42258245fc8f69a3b19f19dced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00040-of-00041.bin:   0%|          | 0.00/952M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01068735122680664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00041-of-00041.bin",
       "rate": null,
       "total": 983051724,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26e9d893d67432692fa640ae487edde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00041-of-00041.bin:   0%|          | 0.00/983M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01754903793334961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)model.bin.index.json",
       "rate": null,
       "total": 31813,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a1cef9478d497e9c1a346cb3445fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/31.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01732039451599121,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 2,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da179f5759c4a7e991e0c261890ec2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01724982261657715,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.model",
       "rate": null,
       "total": 499723,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a33ec6d6c7464b9e008c0b1108a92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010498285293579102,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 141,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a8156f9f344f80a3279d867cb02418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "path_str = r\"../13bmodel\"\n",
    "local_cache_path = Path(path_str)\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"decapoda-research/llama-13b-hf\"\n",
    "model_name_s3 = \"ds-llama-13b\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be704ef5",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd09c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./13bmodel/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce/config.json\n",
      "./13bmodel/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce/\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "paths = os.walk(path_str)#glob(r'./model/*')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad98ac2f-3dba-4130-a499-c3bcd9b95c22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: region=us-east-1\n",
      "env: sagemaker_default_bucket=sagemaker-us-east-1-348052051973\n",
      "env: local_model_path=./13bmodel/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce/\n",
      "env: model_name_s3=$model_name_s3\n"
     ]
    }
   ],
   "source": [
    "%set_env region=$region\n",
    "%set_env sagemaker_default_bucket=$sagemaker_default_bucket \n",
    "%set_env local_model_path=$local_model_path \n",
    "%set_env model_name_s3 = $model_name_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5716ad8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/${model_name_s3}/pretrain/ \n",
    "\n",
    "rm -rf $local_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802c0a9a",
   "metadata": {},
   "source": [
    "## Prepare a docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82393a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "numpy\n",
    "rouge_score\n",
    "fire\n",
    "openai\n",
    "transformers>=4.26.1\n",
    "torch\n",
    "sentencepiece\n",
    "tokenizers==0.12.1\n",
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48ff3a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile2\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile2\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN python3 -m pip install -r requirements.txt \n",
    "RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
    "RUN pip3 uninstall -y deepspeed && pip3 install deepspeed\n",
    "\n",
    "# Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e8b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.${region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d1962",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a814f9d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  52.06MB\n",
      "Step 1/9 : From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      "1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04: Pulling from huggingface-pytorch-training\n",
      "Digest: sha256:6465c5dd6672419b1a60cb47dab82a0f4f1cca22abe3ba7ed9af0c313836df26\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      " ---> c5a6ef695006\n",
      "Step 2/9 : ENV LANG=C.UTF-8\n",
      " ---> Running in bff6b98f3cb0\n",
      "Removing intermediate container bff6b98f3cb0\n",
      " ---> f86efda73432\n",
      "Step 3/9 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in aa64d3d4994d\n",
      "Removing intermediate container aa64d3d4994d\n",
      " ---> 65932c4792b1\n",
      "Step 4/9 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Running in 00a3eb98575f\n",
      "Removing intermediate container 00a3eb98575f\n",
      " ---> f2bb3986f066\n",
      "Step 5/9 : COPY requirements.txt ./\n",
      " ---> d49d1aaacfab\n",
      "Step 6/9 : RUN python3 -m pip install -r requirements.txt\n",
      " ---> Running in a7b9f9b09953\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.23.5)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 8.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.3/70.3 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.26.1\n",
      "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 99.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.13.1+cu117)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.97)\n",
      "Collecting tokenizers==0.12.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 99.8 MB/s eta 0:00:00\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 106.5 MB/s eta 0:00:00\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 17.4 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 97.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (3.8.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.26.1->-r requirements.txt (line 5)) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.26.1->-r requirements.txt (line 5)) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.26.1->-r requirements.txt (line 5)) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.26.1->-r requirements.txt (line 5)) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.26.1->-r requirements.txt (line 5)) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (3.20.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (5.9.4)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 28.7 MB/s eta 0:00:00\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.2/199.2 kB 38.4 MB/s eta 0:00:00\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (8.1.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (1.4.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 12.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.2.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: rouge_score, fire, pathtools\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=61e7d72a37cc3cb70205aad3b20de22b7afdaba2db6b8d9bd33cce6bb9ebf60e\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=aabf1bd2af2a3056ada4a4a5b5f5d35c9d9bd6e278cdf645d4951103c4f31ec1\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=b1558e63d6a54f370d3717338b747419855a0598c4772da7801b1b4d93b37aa3\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built rouge_score fire pathtools\n",
      "Installing collected packages: tokenizers, pathtools, termcolor, smmap, setproctitle, sentry-sdk, nltk, docker-pycreds, absl-py, rouge_score, gitdb, fire, transformers, openai, GitPython, wandb\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.0\n",
      "    Uninstalling transformers-4.26.0:\n",
      "      Successfully uninstalled transformers-4.26.0\n",
      "Successfully installed GitPython-3.1.31 absl-py-1.4.0 docker-pycreds-0.4.0 fire-0.5.0 gitdb-4.0.10 nltk-3.8.1 openai-0.27.4 pathtools-0.1.2 rouge_score-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 termcolor-2.2.0 tokenizers-0.12.1 transformers-4.28.0 wandb-0.14.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container a7b9f9b09953\n",
      " ---> 98d57cb9f62a\n",
      "Step 7/9 : RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
      " ---> Running in 47830627844c\n",
      "Collecting git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision 68d640f7c368bcaaaecfc678f11908ebbd3d6176) to /tmp/pip-req-build-rbvbxz_r\n",
      "\u001b[91m  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-rbvbxz_r\n",
      "\u001b[0m\u001b[91m  Running command git rev-parse -q --verify 'sha^68d640f7c368bcaaaecfc678f11908ebbd3d6176'\n",
      "\u001b[0m\u001b[91m  Running command git fetch -q https://github.com/huggingface/transformers.git 68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
      "\u001b[0m\u001b[91m  Running command git checkout -q 68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
      "\u001b[0m  Resolved https://github.com/huggingface/transformers.git to commit 68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev0) (1.26.14)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6688302 sha256=d0bed5d06fcefa45699909770fb82b8bbbe2b151f914343b9ac8a20790929bb1\n",
      "  Stored in directory: /root/.cache/pip/wheels/06/8a/41/99f8e8f349a79fbbb2eb58ab9103fe9e001f7228cabd993edd\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.28.0\n",
      "    Uninstalling transformers-4.28.0:\n",
      "      Successfully uninstalled transformers-4.28.0\n",
      "Successfully installed transformers-4.27.0.dev0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 47830627844c\n",
      " ---> d50d04564f7c\n",
      "Step 8/9 : RUN pip3 uninstall -y deepspeed && pip3 install deepspeed\n",
      " ---> Running in 44e9069ea1c1\n",
      "Found existing installation: deepspeed 0.6.1+06f2048\n",
      "Uninstalling deepspeed-0.6.1+06f2048:\n",
      "  Successfully uninstalled deepspeed-0.6.1+06f2048\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mCollecting deepspeed\n",
      "  Downloading deepspeed-0.9.0.tar.gz (764 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 764.8/764.8 kB 29.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.11.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed) (5.9.4)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.10.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from deepspeed) (1.13.1+cu117)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from deepspeed) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic->deepspeed) (4.4.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py): started\n",
      "  Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.9.0-py3-none-any.whl size=797698 sha256=f273c140452baa6710d4bf47ed7d2a9c628d7c854185a849e15644f4d7d6b4da\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/bc/03/7c71f2235ee568d187c1114c2e5f1995c4dfff7a3f58d1b6c8\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: deepspeed\n",
      "Successfully installed deepspeed-0.9.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 44e9069ea1c1\n",
      " ---> 8378814873dc\n",
      "Step 9/9 : ENV NVIDIA_VISIBLE_DEVICES=\"all\"\n",
      " ---> Running in db8e485b94a2\n",
      "Removing intermediate container db8e485b94a2\n",
      " ---> 17e8b0798ccd\n",
      "Successfully built 17e8b0798ccd\n",
      "Successfully tagged sagemaker-alpaca-demo:latest\n",
      "The push refers to repository [348052051973.dkr.ecr.us-east-1.amazonaws.com/sagemaker-alpaca-demo]\n",
      "053922fc39b4: Preparing\n",
      "63e21731de9f: Preparing\n",
      "63b01951b1c1: Preparing\n",
      "84fe137bc452: Preparing\n",
      "f8dae5c3df1e: Preparing\n",
      "e3221f18601a: Preparing\n",
      "b6f286626882: Preparing\n",
      "76fe97d80cdb: Preparing\n",
      "f5f76489fff8: Preparing\n",
      "621c3f07daa7: Preparing\n",
      "9b484bb42e11: Preparing\n",
      "54c7c0b58471: Preparing\n",
      "c34adc3ab668: Preparing\n",
      "bbf651e48b84: Preparing\n",
      "f61045791108: Preparing\n",
      "4e2ac0cda74a: Preparing\n",
      "b6f286626882: Waiting\n",
      "658a33d555eb: Preparing\n",
      "bd16d9a61a98: Preparing\n",
      "76fe97d80cdb: Waiting\n",
      "f0c0cd2accfa: Preparing\n",
      "1275469c066c: Preparing\n",
      "f5f76489fff8: Waiting\n",
      "b802dd3babf4: Preparing\n",
      "a3834ec63558: Preparing\n",
      "63edcef6dedf: Preparing\n",
      "621c3f07daa7: Waiting\n",
      "0154e84cc2dd: Preparing\n",
      "7085d1c151f6: Preparing\n",
      "9b484bb42e11: Waiting\n",
      "a77a2104cfb6: Preparing\n",
      "6808e7f9da2f: Preparing\n",
      "3bc059a9dec6: Preparing\n",
      "de783f3fec23: Preparing\n",
      "18ca52d74b2f: Preparing\n",
      "73df6ccd636c: Preparing\n",
      "6738b73ff7a8: Preparing\n",
      "2a8292d9bfcc: Preparing\n",
      "5b75a5ef32a7: Preparing\n",
      "25a5f55a11f0: Preparing\n",
      "707f484816ae: Preparing\n",
      "0430aa1e47d4: Preparing\n",
      "65448e793131: Preparing\n",
      "15af6e2d42ba: Preparing\n",
      "b46caef92993: Preparing\n",
      "53ce33a12646: Preparing\n",
      "aad68760f4ce: Preparing\n",
      "323d67ab1719: Preparing\n",
      "e72743a0fdfe: Preparing\n",
      "3996353f5820: Preparing\n",
      "ea87e0b9c30f: Preparing\n",
      "af18356cdf10: Preparing\n",
      "f6e30dd4497e: Preparing\n",
      "99832d04a153: Preparing\n",
      "a5981ed7a378: Preparing\n",
      "250519a2f830: Preparing\n",
      "6cadbde53f94: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "54c7c0b58471: Waiting\n",
      "4e2ac0cda74a: Waiting\n",
      "c34adc3ab668: Waiting\n",
      "bbf651e48b84: Waiting\n",
      "658a33d555eb: Waiting\n",
      "bd16d9a61a98: Waiting\n",
      "f61045791108: Waiting\n",
      "f0c0cd2accfa: Waiting\n",
      "1275469c066c: Waiting\n",
      "b802dd3babf4: Waiting\n",
      "a3834ec63558: Waiting\n",
      "63edcef6dedf: Waiting\n",
      "0154e84cc2dd: Waiting\n",
      "7085d1c151f6: Waiting\n",
      "a77a2104cfb6: Waiting\n",
      "6808e7f9da2f: Waiting\n",
      "3bc059a9dec6: Waiting\n",
      "de783f3fec23: Waiting\n",
      "18ca52d74b2f: Waiting\n",
      "73df6ccd636c: Waiting\n",
      "6738b73ff7a8: Waiting\n",
      "2a8292d9bfcc: Waiting\n",
      "5b75a5ef32a7: Waiting\n",
      "25a5f55a11f0: Waiting\n",
      "707f484816ae: Waiting\n",
      "0430aa1e47d4: Waiting\n",
      "65448e793131: Waiting\n",
      "15af6e2d42ba: Waiting\n",
      "b46caef92993: Waiting\n",
      "53ce33a12646: Waiting\n",
      "aad68760f4ce: Waiting\n",
      "323d67ab1719: Waiting\n",
      "e72743a0fdfe: Waiting\n",
      "3996353f5820: Waiting\n",
      "ea87e0b9c30f: Waiting\n",
      "af18356cdf10: Waiting\n",
      "f6e30dd4497e: Waiting\n",
      "99832d04a153: Waiting\n",
      "a5981ed7a378: Waiting\n",
      "250519a2f830: Waiting\n",
      "6cadbde53f94: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "e3221f18601a: Waiting\n",
      "84fe137bc452: Pushed\n",
      "053922fc39b4: Pushed\n",
      "f8dae5c3df1e: Pushed\n",
      "e3221f18601a: Pushed\n",
      "f5f76489fff8: Pushed\n",
      "621c3f07daa7: Pushed\n",
      "63e21731de9f: Pushed\n",
      "9b484bb42e11: Pushed\n",
      "b6f286626882: Pushed\n",
      "c34adc3ab668: Pushed\n",
      "bbf651e48b84: Pushed\n",
      "f61045791108: Pushed\n",
      "63b01951b1c1: Pushed\n",
      "4e2ac0cda74a: Pushed\n",
      "bd16d9a61a98: Pushed\n",
      "1275469c066c: Pushed\n",
      "b802dd3babf4: Pushed\n",
      "a3834ec63558: Pushed\n",
      "54c7c0b58471: Pushed\n",
      "63edcef6dedf: Pushed\n",
      "76fe97d80cdb: Pushed\n",
      "a77a2104cfb6: Pushed\n",
      "7085d1c151f6: Pushed\n",
      "3bc059a9dec6: Pushed\n",
      "de783f3fec23: Pushed\n",
      "18ca52d74b2f: Pushed\n",
      "73df6ccd636c: Pushed\n",
      "6738b73ff7a8: Pushed\n",
      "2a8292d9bfcc: Pushed\n",
      "f0c0cd2accfa: Pushed\n",
      "0154e84cc2dd: Pushed\n",
      "707f484816ae: Pushed\n",
      "0430aa1e47d4: Pushed\n",
      "65448e793131: Pushed\n",
      "5b75a5ef32a7: Pushed\n",
      "15af6e2d42ba: Pushed\n",
      "6808e7f9da2f: Pushed\n",
      "53ce33a12646: Pushed\n",
      "aad68760f4ce: Pushed\n",
      "e72743a0fdfe: Pushed\n",
      "3996353f5820: Pushed\n",
      "658a33d555eb: Pushed\n",
      "b46caef92993: Pushed\n",
      "f6e30dd4497e: Pushed\n",
      "99832d04a153: Pushed\n",
      "a5981ed7a378: Pushed\n",
      "250519a2f830: Pushed\n",
      "6cadbde53f94: Pushed\n",
      "0002c93bdb37: Pushed\n",
      "ea87e0b9c30f: Pushed\n",
      "25a5f55a11f0: Pushed\n",
      "323d67ab1719: Pushed\n",
      "af18356cdf10: Pushed\n",
      "latest: digest: sha256:5c34094307a109993ffb962495a95988f444b61e3300d0c1aba5cc5284e7ba60 size: 11463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-alpaca-demo\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520e4e7",
   "metadata": {},
   "source": [
    "**Generate deepspeed config file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04608579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ds.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ds.json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"auto_cast\": false,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": \"auto\",\n",
    "      \"betas\": \"auto\",\n",
    "      \"eps\": \"auto\",\n",
    "      \"weight_decay\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": \"auto\",\n",
    "      \"warmup_max_lr\": \"auto\",\n",
    "      \"warmup_num_steps\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"sub_group_size\": 1e9,\n",
    "    \"reduce_bucket_size\": \"auto\",\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4edf0d",
   "metadata": {},
   "source": [
    "**Generate training entrypoint script.**\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4d7b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train-13b.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile train-13b.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/$MODEL_NAME_S3/pretrain/* /tmp/llama_pretrain/\n",
    "\n",
    "deepspeed --num_gpus=8 stanford_alpaca/train.py \\\n",
    "    --deepspeed ds.json \\\n",
    "    --model_name_or_path \"/tmp/llama_pretrain/\" \\\n",
    "    --data_path stanford_alpaca/alpaca_data.json \\\n",
    "    --output_dir \"/tmp/llama_out\" \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size  1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --save_steps 2000 \\\n",
    "    --save_total_limit 3 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --cache_dir '/tmp' \\\n",
    "    --fp16_full_eval \\\n",
    "    --fp16 \\\n",
    "    --report_to \"none\"\n",
    "\n",
    "./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET//$MODEL_NAME_S3/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f06196b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'348052051973.dkr.ecr.us-east-1.amazonaws.com/sagemaker-alpaca-demo:latest'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-alpaca-demo:latest\".format(account, region)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "118e86c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "# train_data_path = f's3://{sagemaker_default_bucket}/ds-llama/train_data/'\n",
    "\n",
    "# inputs = {'train': train_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf5024",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modify train.py a little about how to save model\n",
    "\n",
    "Modify the model save methods in training script, change from \n",
    "\n",
    "```\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "trainer.save_model(training_args.output_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4732a758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## rename orignal train.py, in case to use further\n",
    "!mv stanford_alpaca/train.py stanford_alpaca/train_bak.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49434d18",
   "metadata": {
    "tags": []
   },
   "source": [
    "**The modified training script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4d71ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing stanford_alpaca/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stanford_alpaca/train.py\n",
    "\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "import utils\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = utils.jload(data_path)\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "    if \"llama\" in model_args.model_name_or_path:\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "    trainer.train()\n",
    "#     trainer.save_state()\n",
    "#     safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cbcf5",
   "metadata": {},
   "source": [
    "Everything is ready, let's launch the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486913ec",
   "metadata": {},
   "source": [
    "## Create SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b199e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-alpaca-demo-2023-04-14-08-09-04-934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-14 08:09:10 Starting - Starting the training job......\n",
      "2023-04-14 08:09:55 Starting - Preparing the instances for training.........\n",
      "2023-04-14 08:11:36 Downloading - Downloading input data...\n",
      "2023-04-14 08:11:51 Training - Downloading the training image.....................\n",
      "2023-04-14 08:15:33 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:27,684 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:27,778 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:27,787 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:27,789 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:28,668 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:28,772 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:28,878 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:28,887 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"sagemaker-alpaca-demo-2023-04-14-08-09-04-934\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/sagemaker-alpaca-demo-2023-04-14-08-09-04-934/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-13b.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-13b.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-13b.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-13b.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/sagemaker-alpaca-demo-2023-04-14-08-09-04-934/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"sagemaker-alpaca-demo-2023-04-14-08-09-04-934\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/sagemaker-alpaca-demo-2023-04-14-08-09-04-934/source/sourcedir.tar.gz\",\"module_name\":\"train-13b.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-13b.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./train-13b.sh \"\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:33.198: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:33,202 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-04-14 08:16:33,219 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00008-of-00041.bin /tmp/llama_pretrain/pytorch_model-00008-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00007-of-00041.bin /tmp/llama_pretrain/pytorch_model-00007-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00035-of-00041.bin /tmp/llama_pretrain/pytorch_model-00035-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00009-of-00041.bin /tmp/llama_pretrain/pytorch_model-00009-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00033-of-00041.bin /tmp/llama_pretrain/pytorch_model-00033-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00000-of-00041.bin /tmp/llama_pretrain/pytorch_model-00000-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00023-of-00041.bin /tmp/llama_pretrain/pytorch_model-00023-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00036-of-00041.bin /tmp/llama_pretrain/pytorch_model-00036-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00031-of-00041.bin /tmp/llama_pretrain/pytorch_model-00031-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00015-of-00041.bin /tmp/llama_pretrain/pytorch_model-00015-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00005-of-00041.bin /tmp/llama_pretrain/pytorch_model-00005-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00006-of-00041.bin /tmp/llama_pretrain/pytorch_model-00006-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00014-of-00041.bin /tmp/llama_pretrain/pytorch_model-00014-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00037-of-00041.bin /tmp/llama_pretrain/pytorch_model-00037-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00032-of-00041.bin /tmp/llama_pretrain/pytorch_model-00032-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00004-of-00041.bin /tmp/llama_pretrain/pytorch_model-00004-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00016-of-00041.bin /tmp/llama_pretrain/pytorch_model-00016-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00028-of-00041.bin /tmp/llama_pretrain/pytorch_model-00028-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00022-of-00041.bin /tmp/llama_pretrain/pytorch_model-00022-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00025-of-00041.bin /tmp/llama_pretrain/pytorch_model-00025-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00001-of-00041.bin /tmp/llama_pretrain/pytorch_model-00001-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00011-of-00041.bin /tmp/llama_pretrain/pytorch_model-00011-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00019-of-00041.bin /tmp/llama_pretrain/pytorch_model-00019-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00018-of-00041.bin /tmp/llama_pretrain/pytorch_model-00018-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00029-of-00041.bin /tmp/llama_pretrain/pytorch_model-00029-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00021-of-00041.bin /tmp/llama_pretrain/pytorch_model-00021-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00030-of-00041.bin /tmp/llama_pretrain/pytorch_model-00030-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00013-of-00041.bin /tmp/llama_pretrain/pytorch_model-00013-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00002-of-00041.bin /tmp/llama_pretrain/pytorch_model-00002-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00020-of-00041.bin /tmp/llama_pretrain/pytorch_model-00020-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00024-of-00041.bin /tmp/llama_pretrain/pytorch_model-00024-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00041-of-00041.bin /tmp/llama_pretrain/pytorch_model-00041-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00003-of-00041.bin /tmp/llama_pretrain/pytorch_model-00003-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00038-of-00041.bin /tmp/llama_pretrain/pytorch_model-00038-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00040-of-00041.bin /tmp/llama_pretrain/pytorch_model-00040-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00010-of-00041.bin /tmp/llama_pretrain/pytorch_model-00010-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00027-of-00041.bin /tmp/llama_pretrain/pytorch_model-00027-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00026-of-00041.bin /tmp/llama_pretrain/pytorch_model-00026-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00034-of-00041.bin /tmp/llama_pretrain/pytorch_model-00034-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00012-of-00041.bin /tmp/llama_pretrain/pytorch_model-00012-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00017-of-00041.bin /tmp/llama_pretrain/pytorch_model-00017-of-00041.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-348052051973/ds-llama-13b/pretrain/pytorch_model-00039-of-00041.bin /tmp/llama_pretrain/pytorch_model-00039-of-00041.bin\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:43,843] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:43,922] [INFO] [runner.py:540:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None stanford_alpaca/train.py --deepspeed ds.json --model_name_or_path /tmp/llama_pretrain/ --data_path stanford_alpaca/alpaca_data.json --output_dir /tmp/llama_out --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy no --save_steps 2000 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --cache_dir /tmp --fp16_full_eval --fp16 --report_to none\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:222:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:222:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:222:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:222:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:247:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:46,572] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:16:51,599] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:17:04,277] [INFO] [partition_parameters.py:436:__exit__] finished initializing model with 13.02B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:31,  1.29it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:30,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:30,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:30,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:31,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:31,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:32,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   2%|▏         | 1/41 [00:00<00:34,  1.16it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:29,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:30,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:31,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:31,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:31,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:32,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:31,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▍         | 2/41 [00:01<00:33,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:29,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:29,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:30,  1.26it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:30,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:30,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:30,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:30,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 3/41 [00:02<00:32,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:28,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:29,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:29,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:29,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:29,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:29,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:29,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|▉         | 4/41 [00:03<00:32,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:03<00:27,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:03<00:29,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:04<00:29,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:04<00:29,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:04<00:29,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:04<00:29,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:04<00:29,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:04<00:26,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 5/41 [00:04<00:31,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:04<00:28,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:04<00:28,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:04<00:28,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:04<00:28,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:04<00:28,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:04<00:28,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:05<00:25,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▍        | 6/41 [00:05<00:30,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:05<00:27,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:05<00:27,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:05<00:27,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:05<00:27,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:05<00:27,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:05<00:27,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:25,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 7/41 [00:06<00:29,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:26,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:26,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:26,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:26,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:26,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:26,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:06<00:24,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|█▉        | 8/41 [00:06<00:28,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:07<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:07<00:25,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:07<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:07<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:07<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:07<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:07<00:23,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  22%|██▏       | 9/41 [00:07<00:27,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:08<00:25,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:08<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:08<00:25,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:08<00:22,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:08<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:08<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:08<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 10/41 [00:08<00:26,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:09<00:21,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:08<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:08<00:24,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:08<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:08<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:08<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:08<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 11/41 [00:09<00:25,  1.16it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:09<00:20,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:09<00:23,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:09<00:23,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:09<00:23,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:09<00:23,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:09<00:23,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:09<00:23,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:10<00:20,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▉       | 12/41 [00:10<00:24,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:10<00:22,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:10<00:22,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:10<00:22,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:10<00:22,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:10<00:22,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:10<00:22,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:11<00:19,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 13/41 [00:11<00:23,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:11<00:21,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:11<00:21,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:11<00:21,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:11<00:21,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:11<00:21,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:11<00:21,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:12<00:18,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  34%|███▍      | 14/41 [00:12<00:23,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:12<00:18,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:12<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 15/41 [00:12<00:22,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:13<00:17,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:13<00:19,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:13<00:19,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:13<00:19,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:13<00:19,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:13<00:19,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:13<00:19,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 16/41 [00:13<00:21,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:14<00:16,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:14<00:18,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:14<00:18,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:14<00:18,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:14<00:18,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:14<00:18,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:14<00:18,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  41%|████▏     | 17/41 [00:14<00:20,  1.16it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:15<00:15,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:15<00:17,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:15<00:17,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:15<00:17,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:15<00:17,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:15<00:17,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:15<00:17,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  44%|████▍     | 18/41 [00:15<00:19,  1.16it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:15<00:15,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:16<00:14,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  46%|████▋     | 19/41 [00:16<00:19,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:16<00:16,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:17<00:13,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  49%|████▉     | 20/41 [00:17<00:18,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:17<00:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:17<00:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:17<00:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:17<00:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:17<00:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:17<00:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:18<00:12,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  51%|█████     | 21/41 [00:18<00:17,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:18<00:14,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:18<00:14,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:18<00:14,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:18<00:14,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:18<00:14,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:18<00:14,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:18<00:12,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  54%|█████▎    | 22/41 [00:19<00:16,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:19<00:13,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:19<00:13,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:19<00:13,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:19<00:13,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:19<00:13,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:19<00:13,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:19<00:11,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  56%|█████▌    | 23/41 [00:19<00:15,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:20<00:10,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  59%|█████▊    | 24/41 [00:20<00:14,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:21<00:09,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:21<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:20<00:12,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 25/41 [00:21<00:13,  1.16it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:21<00:08,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:21<00:11,  1.26it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:21<00:11,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:21<00:11,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:21<00:11,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:21<00:11,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:21<00:11,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:22<00:08,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 26/41 [00:22<00:12,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:22<00:10,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:22<00:10,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:22<00:10,  1.26it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:22<00:10,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:22<00:10,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:22<00:10,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:23<00:07,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:23<00:09,  1.29it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:23<00:09,  1.29it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:23<00:09,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  66%|██████▌   | 27/41 [00:23<00:11,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:23<00:09,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:23<00:09,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:23<00:09,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:24<00:06,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:23<00:08,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:24<00:08,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:24<00:08,  1.29it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:24<00:08,  1.29it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:24<00:08,  1.29it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:24<00:08,  1.29it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 28/41 [00:24<00:11,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:24<00:05,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:24<00:07,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:24<00:07,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:24<00:07,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:24<00:07,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:24<00:07,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:24<00:07,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████   | 29/41 [00:24<00:10,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:25<00:05,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:25<00:06,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:25<00:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:25<00:06,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:25<00:06,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:25<00:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:25<00:06,  1.30it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 30/41 [00:25<00:09,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:26<00:04,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:26<00:05,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:26<00:06,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:26<00:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:26<00:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:26<00:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:26<00:06,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 31/41 [00:26<00:08,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:27<00:03,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:26<00:05,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:26<00:05,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:27<00:05,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:27<00:05,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:27<00:05,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:27<00:05,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  78%|███████▊  | 32/41 [00:27<00:07,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:27<00:03,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:27<00:04,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:27<00:04,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:27<00:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:27<00:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:27<00:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:27<00:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:28<00:02,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 33/41 [00:28<00:06,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:28<00:03,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:28<00:03,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:28<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:28<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:28<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:28<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:29<00:01,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:29<00:02,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:29<00:02,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 34/41 [00:29<00:05,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:29<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:29<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:29<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:29<00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:30<00:00,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:29<00:02,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:29<00:02,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▌ | 35/41 [00:30<00:05,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:30<00:02,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:30<00:02,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:30<00:02,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:30<00:02,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:30<00:01,  1.34it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:30<00:00,  1.31it/s]#015Loading checkpoint shards: 100%|██████████| 41/41 [00:30<00:00,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:30<00:01,  1.35it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:30<00:01,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:30<00:01,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:30<00:01,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:30<00:01,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 36/41 [00:30<00:04,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:31<00:00,  1.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:31<00:00,  1.35it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:31<00:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:31<00:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:31<00:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:31<00:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 37/41 [00:31<00:03,  1.17it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.27it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.31it/s]#015Loading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.26it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.26it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.31it/s]#015Loading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.26it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:32<00:00,  1.26it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 38/41 [00:32<00:02,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▌| 39/41 [00:33<00:01,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  98%|█████████▊| 40/41 [00:34<00:00,  1.20it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:35<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 41/41 [00:35<00:00,  1.17it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.46989893913269 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.35018491744995 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.450198888778687 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.449986696243286 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.44725751876831 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.4505774974823 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.45079779624939 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 23.449790954589844 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.333889484405518 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.331732034683228 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.331485271453857 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.331409931182861 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.328175067901611 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.330965042114258 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.131369829177856 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.330767631530762 seconds\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 414720 in 81 params\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00042557716369628906 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00040340423583984375 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003573894500732422 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00036263465881347656 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0006015300750732422 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003573894500732422 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00034999847412109375 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.715: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.717: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.717: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.717: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.717: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.718: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.719: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.745 algo-1:510 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.747 algo-1:514 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.748 algo-1:513 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.748 algo-1:516 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.749 algo-1:512 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.750 algo-1:511 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.752 algo-1:515 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.766 algo-1:510 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.768 algo-1:514 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.768 algo-1:513 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.769 algo-1:516 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.769 algo-1:512 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.772 algo-1:511 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.778 algo-1:515 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00035762786865234375 seconds\u001b[0m\n",
      "\u001b[34m0%|          | 0/6501 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.901: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.932 algo-1:509 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:28.954 algo-1:509 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m0%|          | 1/6501 [00:03<6:34:05,  3.64s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 1.2475, 'learning_rate': 0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/6501 [00:03<6:34:05,  3.64s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/6501 [00:04<4:08:34,  2.29s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 1.355, 'learning_rate': 0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/6501 [00:04<4:08:34,  2.29s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/6501 [00:05<3:01:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 1.8795, 'learning_rate': 0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/6501 [00:05<3:01:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/6501 [00:06<2:32:35,  1.41s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 1.5724, 'learning_rate': 0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/6501 [00:06<2:32:35,  1.41s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:37,395] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 5/6501 [00:08<2:42:25,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3311, 'learning_rate': 0.0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/6501 [00:08<2:42:25,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/6501 [00:10<2:42:09,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2692, 'learning_rate': 2.626495350371936e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/6501 [00:10<2:42:09,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/6501 [00:11<2:41:59,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1787, 'learning_rate': 4.162896638657993e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 7/6501 [00:11<2:41:59,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/6501 [00:13<2:39:26,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6913, 'learning_rate': 4.162896638657993e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 8/6501 [00:13<2:39:26,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:43,380] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 9/6501 [00:14<2:42:48,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2817, 'learning_rate': 5.252990700743872e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 9/6501 [00:14<2:42:48,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 10/6501 [00:15<2:33:02,  1.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1048, 'learning_rate': 6.098533345119624e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 10/6501 [00:15<2:33:02,  1.41s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:46,242] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 11/6501 [00:17<2:40:47,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1508, 'learning_rate': 6.7893919890299284e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 11/6501 [00:17<2:40:47,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:47,943] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 12/6501 [00:19<2:47:48,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.066, 'learning_rate': 7.373504649628066e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 12/6501 [00:19<2:47:48,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:49,684] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 13/6501 [00:20<2:53:59,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1137, 'learning_rate': 7.879486051115807e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 13/6501 [00:20<2:53:59,  1.61s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 14/6501 [00:22<3:00:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0178, 'learning_rate': 8.325793277315987e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 14/6501 [00:22<3:00:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:53,236] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 15/6501 [00:24<3:02:58,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8385, 'learning_rate': 8.72502869549156e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 15/6501 [00:24<3:02:58,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:54,829] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 16/6501 [00:26<2:59:41,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2034, 'learning_rate': 9.086181061280522e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 16/6501 [00:26<2:59:41,  1.66s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 17/6501 [00:27<3:04:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2315, 'learning_rate': 9.415887339401865e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 17/6501 [00:27<3:04:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:57,941] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 18/6501 [00:29<2:51:23,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8365, 'learning_rate': 9.719187714029216e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 18/6501 [00:29<2:51:23,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:19:59,934] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 19/6501 [00:31<3:04:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7836, 'learning_rate': 1e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 19/6501 [00:31<3:04:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 20/6501 [00:32<2:52:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8836, 'learning_rate': 1e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 20/6501 [00:32<2:52:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:02,835] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 21/6501 [00:34<2:51:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8713, 'learning_rate': 1.0261429983777618e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 21/6501 [00:34<2:51:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 22/6501 [00:35<2:54:24,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0675, 'learning_rate': 1.0505981401487744e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 22/6501 [00:35<2:54:24,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:06,035] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 23/6501 [00:37<2:51:19,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1686, 'learning_rate': 1.073570214736208e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 23/6501 [00:37<2:51:19,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:07,940] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 24/6501 [00:39<3:01:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1027, 'learning_rate': 1.0952288627687922e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 24/6501 [00:39<3:01:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:09,849] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 25/6501 [00:41<3:08:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0396, 'learning_rate': 1.1157161862776595e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 25/6501 [00:41<3:08:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:11,619] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 26/6501 [00:42<3:09:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0324, 'learning_rate': 1.1351524045863495e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 26/6501 [00:42<3:09:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 27/6501 [00:44<3:09:27,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2518, 'learning_rate': 1.1536401288286059e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 27/6501 [00:44<3:09:27,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:15,287] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 28/6501 [00:46<3:14:31,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6093, 'learning_rate': 1.1712676411652457e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 28/6501 [00:46<3:14:31,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:17,025] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 29/6501 [00:48<3:12:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0521, 'learning_rate': 1.1881114444703123e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 29/6501 [00:48<3:12:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:18,787] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 30/6501 [00:49<3:11:38,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3274, 'learning_rate': 1.2042382689773802e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 30/6501 [00:49<3:11:38,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:20,350] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 31/6501 [00:51<3:04:42,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0922, 'learning_rate': 1.2197066690239248e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 31/6501 [00:51<3:04:42,  1.71s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 32/6501 [00:53<3:06:23,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2747, 'learning_rate': 1.2345683064401153e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 32/6501 [00:53<3:06:23,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:23,837] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 33/6501 [00:55<3:06:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3091, 'learning_rate': 1.248868991597398e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 33/6501 [00:55<3:06:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:25,600] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 34/6501 [00:56<3:07:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7448, 'learning_rate': 1.2626495350371936e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 34/6501 [00:56<3:07:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:27,693] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 35/6501 [00:58<3:18:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0719, 'learning_rate': 1.2759464495897799e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 35/6501 [00:58<3:18:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 36/6501 [01:00<3:13:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3683, 'learning_rate': 1.2887925334149553e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 36/6501 [01:00<3:13:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:31,114] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 37/6501 [01:02<3:11:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0709, 'learning_rate': 1.3012173574060929e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 37/6501 [01:02<3:11:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:33,067] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 38/6501 [01:04<3:17:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8941, 'learning_rate': 1.313247675185968e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 38/6501 [01:04<3:17:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 39/6501 [01:05<3:09:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.971, 'learning_rate': 1.3249077699938515e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 39/6501 [01:05<3:09:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:36,624] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 40/6501 [01:07<3:15:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8928, 'learning_rate': 1.3362197497734014e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 40/6501 [01:07<3:15:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:38,327] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 41/6501 [01:09<3:12:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8873, 'learning_rate': 1.3472037994747689e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 41/6501 [01:09<3:12:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:40,002] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 42/6501 [01:11<3:08:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0589, 'learning_rate': 1.3578783978059857e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 42/6501 [01:11<3:08:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:42,061] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 43/6501 [01:13<3:18:28,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8448, 'learning_rate': 1.3682605042803869e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 43/6501 [01:13<3:18:28,  1.84s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 44/6501 [01:15<3:18:33,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9324, 'learning_rate': 1.3783657213148532e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 44/6501 [01:15<3:18:33,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:45,494] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 45/6501 [01:16<3:10:08,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1365, 'learning_rate': 1.3882084352687208e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 45/6501 [01:16<3:10:08,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:47,281] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 46/6501 [01:18<3:10:44,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0231, 'learning_rate': 1.3978019396235432e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 46/6501 [01:18<3:10:44,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:49,330] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 47/6501 [01:20<3:19:36,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0214, 'learning_rate': 1.407158542950524e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 47/6501 [01:20<3:19:36,  1.86s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 48/6501 [01:22<3:15:28,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0472, 'learning_rate': 1.4162896638657994e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 48/6501 [01:22<3:15:28,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:52,872] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 49/6501 [01:24<3:15:19,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9495, 'learning_rate': 1.4252059148112174e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 49/6501 [01:24<3:15:19,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:54,713] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 50/6501 [01:25<3:16:05,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8125, 'learning_rate': 1.4339171762024393e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 50/6501 [01:25<3:16:05,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:56,472] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 51/6501 [01:27<3:13:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8443, 'learning_rate': 1.442432662243561e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 51/6501 [01:27<3:13:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 52/6501 [01:29<3:02:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8585, 'learning_rate': 1.450760979507506e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 52/6501 [01:29<3:02:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:20:59,686] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 53/6501 [01:30<3:04:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1492, 'learning_rate': 1.4589101792159104e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 53/6501 [01:30<3:04:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:01,413] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 54/6501 [01:32<3:04:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9144, 'learning_rate': 1.4668878040145737e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 54/6501 [01:32<3:04:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:03,223] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 55/6501 [01:34<3:07:43,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9902, 'learning_rate': 1.4747009299256131e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 55/6501 [01:34<3:07:43,  1.75s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 56/6501 [01:36<3:06:44,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0635, 'learning_rate': 1.4823562040611184e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 56/6501 [01:36<3:06:44,  1.74s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 57/6501 [01:37<3:07:11,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9469, 'learning_rate': 1.4898598786020071e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 57/6501 [01:37<3:07:11,  1.74s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 58/6501 [01:39<3:14:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9513, 'learning_rate': 1.4972178414773088e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 58/6501 [01:39<3:14:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:10,352] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 59/6501 [01:41<3:10:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7458, 'learning_rate': 1.5044356441210549e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 59/6501 [01:41<3:10:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 60/6501 [01:42<2:43:35,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3869, 'learning_rate': 1.5115185266345915e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 60/6501 [01:42<2:43:35,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:13,303] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 61/6501 [01:44<2:59:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0445, 'learning_rate': 1.5184714406400147e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 61/6501 [01:44<2:59:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 62/6501 [01:46<2:59:07,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8473, 'learning_rate': 1.5252990700743875e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 62/6501 [01:46<2:59:07,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:16,683] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 63/6501 [01:47<3:00:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2521, 'learning_rate': 1.5320058501434588e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 63/6501 [01:47<3:00:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:18,536] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 64/6501 [01:49<3:05:58,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8233, 'learning_rate': 1.5385959846269733e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 64/6501 [01:49<3:05:58,  1.73s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 65/6501 [01:51<3:13:17,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1551, 'learning_rate': 1.5450734617046664e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 65/6501 [01:51<3:13:17,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:22,131] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 66/6501 [01:53<3:07:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3705, 'learning_rate': 1.5514420684521487e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 66/6501 [01:53<3:07:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:23,986] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 67/6501 [01:55<3:11:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2822, 'learning_rate': 1.5577054041386156e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 67/6501 [01:55<3:11:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:25,788] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 68/6501 [01:56<3:11:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3344, 'learning_rate': 1.5638668924432865e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 68/6501 [01:56<3:11:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 69/6501 [01:58<3:07:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0925, 'learning_rate': 1.569929792694405e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 69/6501 [01:58<3:07:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:29,251] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 70/6501 [02:00<3:09:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8697, 'learning_rate': 1.5758972102231614e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 70/6501 [02:00<3:09:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:31,357] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 71/6501 [02:02<3:20:15,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0168, 'learning_rate': 1.581772105914884e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 71/6501 [02:02<3:20:15,  1.87s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 72/6501 [02:04<3:17:59,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9932, 'learning_rate': 1.5875573050310447e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 72/6501 [02:04<3:17:59,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:34,843] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 73/6501 [02:06<3:12:46,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9438, 'learning_rate': 1.59325550536788e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 73/6501 [02:06<3:12:46,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:36,573] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 74/6501 [02:07<3:10:31,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1074, 'learning_rate': 1.598869284810595e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 74/6501 [02:07<3:10:31,  1.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 75/6501 [02:09<3:09:47,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2677, 'learning_rate': 1.6044011083361118e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 75/6501 [02:09<3:09:47,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:40,236] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 76/6501 [02:11<3:14:04,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.711, 'learning_rate': 1.6098533345119626e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 76/6501 [02:11<3:14:04,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:42,345] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 77/6501 [02:13<3:23:33,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1018, 'learning_rate': 1.6152282215342255e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 77/6501 [02:13<3:23:33,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:44,196] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 78/6501 [02:15<3:21:55,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5273, 'learning_rate': 1.6205279328431792e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 78/6501 [02:15<3:21:55,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:46,192] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 79/6501 [02:17<3:25:24,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0536, 'learning_rate': 1.6257545423516383e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 79/6501 [02:17<3:25:24,  1.92s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 80/6501 [02:19<3:16:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7132, 'learning_rate': 1.6309100393175807e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 80/6501 [02:19<3:16:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 81/6501 [02:20<3:11:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0172, 'learning_rate': 1.635996332889724e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 81/6501 [02:20<3:11:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:51,222] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 82/6501 [02:22<3:09:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0103, 'learning_rate': 1.641015256352047e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 82/6501 [02:22<3:09:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:53,153] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 83/6501 [02:24<3:14:15,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1072, 'learning_rate': 1.645968571090859e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 83/6501 [02:24<3:14:15,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:55,033] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 84/6501 [02:26<3:16:16,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0482, 'learning_rate': 1.6508579703059144e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 84/6501 [02:26<3:16:16,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:21:57,001] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 85/6501 [02:28<3:20:28,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2218, 'learning_rate': 1.6556850824851286e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 85/6501 [02:28<3:20:28,  1.87s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 86/6501 [02:29<3:13:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2485, 'learning_rate': 1.6604514746607367e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 86/6501 [02:29<3:13:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:00,374] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 87/6501 [02:31<3:10:17,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.136, 'learning_rate': 1.6651586554631973e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 87/6501 [02:31<3:10:17,  1.78s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 88/6501 [02:33<3:20:26,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2114, 'learning_rate': 1.6698080779877177e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 88/6501 [02:33<3:20:26,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:04,397] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 89/6501 [02:35<3:22:00,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.792, 'learning_rate': 1.674401142487045e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 89/6501 [02:35<3:22:00,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:06,093] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 90/6501 [02:37<3:15:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0574, 'learning_rate': 1.6789391989029928e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 90/6501 [02:37<3:15:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:07,690] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 91/6501 [02:38<3:08:10,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1381, 'learning_rate': 1.6834235492481704e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 91/6501 [02:38<3:08:10,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:09,852] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 92/6501 [02:41<3:20:59,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3672, 'learning_rate': 1.6878554498484106e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 92/6501 [02:41<3:20:59,  1.88s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 93/6501 [02:42<3:18:59,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0595, 'learning_rate': 1.692236113455579e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 93/6501 [02:42<3:18:59,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:13,949] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 94/6501 [02:45<3:32:13,  1.99s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 94/6501 [02:45<3:32:13,  1.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.917, 'learning_rate': 1.696566711239633e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 95/6501 [02:46<3:00:31,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3864, 'learning_rate': 1.7008483746681347e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 95/6501 [02:46<3:00:31,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:16,586] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 96/6501 [02:47<2:58:48,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0624, 'learning_rate': 1.7050821972807545e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 96/6501 [02:47<2:58:48,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:18,412] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 97/6501 [02:49<3:03:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1786, 'learning_rate': 1.709269236365728e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 97/6501 [02:49<3:03:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 98/6501 [02:51<3:04:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.276, 'learning_rate': 1.7134105145446995e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 98/6501 [02:51<3:04:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:22,020] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 99/6501 [02:53<3:08:31,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9462, 'learning_rate': 1.7175070212718923e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 99/6501 [02:53<3:08:31,  1.77s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 100/6501 [02:54<3:00:55,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.071, 'learning_rate': 1.721559714253104e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 100/6501 [02:54<3:00:55,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:25,326] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 101/6501 [02:56<3:03:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3289, 'learning_rate': 1.7255695207896222e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 101/6501 [02:56<3:03:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:26,949] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 102/6501 [02:58<3:00:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 102/6501 [02:58<3:00:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9908, 'learning_rate': 1.7295373390517673e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:28,961] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 103/6501 [03:00<3:10:34,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2879, 'learning_rate': 1.7334640392864517e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 103/6501 [03:00<3:10:34,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:30,689] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 104/6501 [03:01<3:08:38,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9022, 'learning_rate': 1.7373504649628066e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 104/6501 [03:01<3:08:38,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:32,651] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 105/6501 [03:03<3:14:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8855, 'learning_rate': 1.7411974338596505e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 105/6501 [03:03<3:14:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:34,549] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 106/6501 [03:05<3:17:00,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1525, 'learning_rate': 1.745005739098312e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 106/6501 [03:05<3:17:00,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:36,326] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 107/6501 [03:07<3:14:41,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0589, 'learning_rate': 1.7487761501240622e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 107/6501 [03:07<3:14:41,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:38,111] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 108/6501 [03:09<3:13:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7484, 'learning_rate': 1.7525094136392006e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 108/6501 [03:09<3:13:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 109/6501 [03:11<3:16:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1752, 'learning_rate': 1.756206254490622e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 109/6501 [03:11<3:16:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:41,466] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 110/6501 [03:12<3:03:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0669, 'learning_rate': 1.759867376514502e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 110/6501 [03:12<3:03:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:43,027] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 111/6501 [03:14<2:58:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9774, 'learning_rate': 1.7634934633405684e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 111/6501 [03:14<2:58:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:44,705] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 112/6501 [03:15<2:58:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9108, 'learning_rate': 1.7670851791582484e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 112/6501 [03:15<2:58:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 113/6501 [03:17<2:59:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5404, 'learning_rate': 1.770643169446852e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 113/6501 [03:17<2:59:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:48,201] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 114/6501 [03:19<3:02:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1421, 'learning_rate': 1.774168061671785e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 114/6501 [03:19<3:02:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:50,092] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 115/6501 [03:21<3:08:08,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3134, 'learning_rate': 1.7776604659486815e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 115/6501 [03:21<3:08:08,  1.77s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 116/6501 [03:22<2:47:27,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1289, 'learning_rate': 1.7811209756772084e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 116/6501 [03:22<2:47:27,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:52,893] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 117/6501 [03:24<2:50:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2254, 'learning_rate': 1.7845501681461862e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 117/6501 [03:24<2:50:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 118/6501 [03:25<2:31:59,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3005, 'learning_rate': 1.7879486051115808e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 118/6501 [03:25<2:31:59,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:55,730] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 119/6501 [03:26<2:44:28,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2926, 'learning_rate': 1.7913168333487983e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 119/6501 [03:26<2:44:28,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:22:57,544] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 120/6501 [03:28<2:53:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 120/6501 [03:28<2:53:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1187, 'learning_rate': 1.7946553851806527e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 121/6501 [03:30<3:00:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2003, 'learning_rate': 1.7979647789822748e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 121/6501 [03:30<3:00:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:00,989] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 122/6501 [03:32<2:56:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6058, 'learning_rate': 1.801245519664167e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 122/6501 [03:32<2:56:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:03,114] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 123/6501 [03:34<3:11:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0267, 'learning_rate': 1.8044980991345203e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 123/6501 [03:34<3:11:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:05,136] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 124/6501 [03:36<3:18:29,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0471, 'learning_rate': 1.80772299674186e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 124/6501 [03:36<3:18:29,  1.87s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 125/6501 [03:37<3:05:56,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2458, 'learning_rate': 1.8109206796990145e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 125/6501 [03:37<3:05:56,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:08,165] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 126/6501 [03:39<2:59:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1318, 'learning_rate': 1.8140916034893422e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 126/6501 [03:39<2:59:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 127/6501 [03:41<3:02:52,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1366, 'learning_rate': 1.8172362122561044e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 127/6501 [03:41<3:02:52,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:11,548] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 128/6501 [03:42<2:58:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2109, 'learning_rate': 1.820354939175809e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 128/6501 [03:42<2:58:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:13,353] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 129/6501 [03:44<3:02:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7544, 'learning_rate': 1.8234482068163232e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 129/6501 [03:44<3:02:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 130/6501 [03:46<3:07:55,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1613, 'learning_rate': 1.82651642748048e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 130/6501 [03:46<3:07:55,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:16,970] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 131/6501 [03:48<3:06:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1858, 'learning_rate': 1.8295600035358877e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 131/6501 [03:48<3:06:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:18,139] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 132/6501 [03:49<2:47:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0963, 'learning_rate': 1.8325793277315986e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 132/6501 [03:49<2:47:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:20,116] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 133/6501 [03:51<3:00:24,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2805, 'learning_rate': 1.8355747835022643e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 133/6501 [03:51<3:00:24,  1.70s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 134/6501 [03:53<3:02:59,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1793, 'learning_rate': 1.8385467452603553e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 134/6501 [03:53<3:02:59,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:23,786] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 135/6501 [03:54<3:08:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0006, 'learning_rate': 1.8414955786770164e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 135/6501 [03:54<3:08:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:25,760] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 136/6501 [03:56<3:14:31,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3029, 'learning_rate': 1.8444216409520775e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 136/6501 [03:56<3:14:31,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:27,489] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 137/6501 [03:58<3:11:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1606, 'learning_rate': 1.847325281073714e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 137/6501 [03:58<3:11:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:29,346] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 138/6501 [04:00<3:12:51,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8652, 'learning_rate': 1.8502068400682386e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 138/6501 [04:00<3:12:51,  1.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 139/6501 [04:02<3:16:48,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8513, 'learning_rate': 1.8530666512404662e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 139/6501 [04:02<3:16:48,  1.86s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 140/6501 [04:04<3:16:14,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7727, 'learning_rate': 1.8559050404050735e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 140/6501 [04:04<3:16:14,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:35,166] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 141/6501 [04:06<3:22:08,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0435, 'learning_rate': 1.8587223261093603e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 141/6501 [04:06<3:22:08,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:36,852] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 142/6501 [04:08<3:15:04,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0051, 'learning_rate': 1.8615188198477887e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 142/6501 [04:08<3:15:04,  1.84s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 143/6501 [04:09<3:09:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0454, 'learning_rate': 1.864294826268665e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 143/6501 [04:09<3:09:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:40,589] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 144/6501 [04:11<3:18:34,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1986, 'learning_rate': 1.8670506433733053e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 144/6501 [04:11<3:18:34,  1.87s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 145/6501 [04:13<3:20:16,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9914, 'learning_rate': 1.8697865627080094e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 145/6501 [04:13<3:20:16,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:44,444] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 146/6501 [04:15<3:21:24,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0783, 'learning_rate': 1.872502869549156e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 146/6501 [04:15<3:21:24,  1.90s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 147/6501 [04:17<3:12:16,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.99, 'learning_rate': 1.87519984308171e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 147/6501 [04:17<3:12:16,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:47,683] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 148/6501 [04:18<3:06:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1686, 'learning_rate': 1.877877756571419e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 148/6501 [04:18<3:06:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:49,637] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 149/6501 [04:20<3:12:19,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8947, 'learning_rate': 1.8805368775309735e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 149/6501 [04:20<3:12:19,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:51,372] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 150/6501 [04:22<3:09:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7884, 'learning_rate': 1.883177467880373e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 150/6501 [04:22<3:09:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:53,154] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 151/6501 [04:24<3:09:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 151/6501 [04:24<3:09:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4179, 'learning_rate': 1.885799784101742e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 152/6501 [04:26<3:05:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1691, 'learning_rate': 1.888404077388832e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 152/6501 [04:26<3:05:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:56,520] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 153/6501 [04:27<3:03:41,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3697, 'learning_rate': 1.8909905937914125e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 153/6501 [04:27<3:03:41,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:23:58,302] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 154/6501 [04:29<3:05:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9319, 'learning_rate': 1.893559574354774e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 154/6501 [04:29<3:05:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:00,471] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 155/6501 [04:31<3:18:21,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0929, 'learning_rate': 1.8961112552545314e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 155/6501 [04:31<3:18:21,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:02,424] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 156/6501 [04:33<3:20:48,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2324, 'learning_rate': 1.898645867926918e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 156/6501 [04:33<3:20:48,  1.90s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 157/6501 [04:35<3:19:40,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1789, 'learning_rate': 1.9011636391947506e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 157/6501 [04:35<3:19:40,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:06,148] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 158/6501 [04:37<3:18:43,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0567, 'learning_rate': 1.9036647913892404e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 158/6501 [04:37<3:18:43,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:07,917] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 159/6501 [04:39<3:15:09,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9491, 'learning_rate': 1.9061495424678064e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 159/6501 [04:39<3:15:09,  1.85s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 160/6501 [04:40<3:15:02,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3241, 'learning_rate': 1.9086181061280525e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 160/6501 [04:40<3:15:02,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:11,401] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 161/6501 [04:42<3:08:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2522, 'learning_rate': 1.9110706919180554e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 161/6501 [04:42<3:08:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:13,109] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 162/6501 [04:44<3:06:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2904, 'learning_rate': 1.913507505343108e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 162/6501 [04:44<3:06:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:14,986] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 163/6501 [04:46<3:09:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0443, 'learning_rate': 1.915928747969051e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 163/6501 [04:46<3:09:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:16,763] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 164/6501 [04:47<3:09:04,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1727, 'learning_rate': 1.918334617522322e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 164/6501 [04:47<3:09:04,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:18,440] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 165/6501 [04:49<3:05:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.817, 'learning_rate': 1.9207253079868542e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 165/6501 [04:49<3:05:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 166/6501 [04:51<3:10:10,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3143, 'learning_rate': 1.9231010096979303e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 166/6501 [04:51<3:10:10,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:21,861] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 167/6501 [04:53<3:01:05,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0894, 'learning_rate': 1.9254619094331192e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 167/6501 [04:53<3:01:05,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:23,501] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 168/6501 [04:54<2:58:39,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8979, 'learning_rate': 1.9278081905003905e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 168/6501 [04:54<2:58:39,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:25,093] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 169/6501 [04:56<2:55:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.097, 'learning_rate': 1.9301400328235263e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 169/6501 [04:56<2:55:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 170/6501 [04:58<3:05:14,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4849, 'learning_rate': 1.9324576130249116e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 170/6501 [04:58<3:05:14,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:28,920] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 171/6501 [05:00<3:08:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1288, 'learning_rate': 1.9347611045058135e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 171/6501 [05:00<3:08:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:30,654] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 172/6501 [05:01<3:06:41,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.477, 'learning_rate': 1.9370506775242385e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 172/6501 [05:01<3:06:41,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:32,304] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 173/6501 [05:03<3:02:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3329, 'learning_rate': 1.9393264992704404e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 173/6501 [05:03<3:02:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:34,231] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 174/6501 [05:05<3:08:56,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1633, 'learning_rate': 1.9415887339401866e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 174/6501 [05:05<3:08:56,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:36,086] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 175/6501 [05:07<3:10:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7257, 'learning_rate': 1.9438375428058432e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 175/6501 [05:07<3:10:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:38,158] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 176/6501 [05:09<3:19:10,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0941, 'learning_rate': 1.946073084285364e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 176/6501 [05:09<3:19:10,  1.89s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 177/6501 [05:11<3:13:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3061, 'learning_rate': 1.948295514009258e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 177/6501 [05:11<3:13:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:41,784] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 178/6501 [05:12<3:16:02,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1619, 'learning_rate': 1.9505049848856045e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 178/6501 [05:12<3:16:02,  1.86s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 179/6501 [05:14<3:09:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9785, 'learning_rate': 1.9527016471631806e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 179/6501 [05:14<3:09:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:45,545] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 180/6501 [05:16<3:19:09,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8563, 'learning_rate': 1.9548856484927726e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 180/6501 [05:16<3:19:09,  1.89s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 181/6501 [05:18<3:12:41,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2436, 'learning_rate': 1.9570571339867317e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 181/6501 [05:18<3:12:41,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:48,856] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 182/6501 [05:20<3:06:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2021, 'learning_rate': 1.9592162462768263e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 182/6501 [05:20<3:06:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 183/6501 [05:22<3:16:12,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4355, 'learning_rate': 1.9613631255704654e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 183/6501 [05:22<3:16:12,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:52,053] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 184/6501 [05:23<2:52:24,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1815, 'learning_rate': 1.9634979097053283e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 184/6501 [05:23<2:52:24,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:53,824] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 185/6501 [05:25<2:56:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3925, 'learning_rate': 1.9656207342024703e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 185/6501 [05:25<2:56:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:55,666] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 186/6501 [05:26<3:01:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3472, 'learning_rate': 1.967731732317948e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 186/6501 [05:26<3:01:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:24:57,618] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 187/6501 [05:28<3:08:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.153, 'learning_rate': 1.969831035093013e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 187/6501 [05:28<3:08:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 188/6501 [05:30<3:09:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2275, 'learning_rate': 1.971918771402922e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 188/6501 [05:30<3:09:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 189/6501 [05:32<3:13:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3086, 'learning_rate': 1.973995068004415e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 189/6501 [05:32<3:13:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:02,993] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 190/6501 [05:34<3:06:41,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1357, 'learning_rate': 1.9760600495818933e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 190/6501 [05:34<3:06:41,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:05,149] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 191/6501 [05:36<3:18:41,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3993, 'learning_rate': 1.9781138387923492e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 191/6501 [05:36<3:18:41,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:06,908] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 192/6501 [05:38<3:14:33,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2616, 'learning_rate': 1.980156556309086e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 192/6501 [05:38<3:14:33,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:08,517] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 193/6501 [05:39<3:06:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.889, 'learning_rate': 1.98218832086426e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 193/6501 [05:39<3:06:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:10,186] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 194/6501 [05:41<3:03:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2779, 'learning_rate': 1.984209249290298e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 194/6501 [05:41<3:03:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 195/6501 [05:42<2:59:19,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0565, 'learning_rate': 1.9862194565602044e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 195/6501 [05:42<2:59:19,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:13,697] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 196/6501 [05:44<3:05:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3294, 'learning_rate': 1.9882190558268157e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 196/6501 [05:44<3:05:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 197/6501 [05:46<3:01:04,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.324, 'learning_rate': 1.990208158461016e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 197/6501 [05:46<3:01:04,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:17,432] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 198/6501 [05:48<3:13:02,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3226, 'learning_rate': 1.9921868740889608e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 198/6501 [05:48<3:13:02,  1.84s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 199/6501 [05:50<3:06:52,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3337, 'learning_rate': 1.99415531062833e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 199/6501 [05:50<3:06:52,  1.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 200/6501 [05:52<3:09:50,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8413, 'learning_rate': 1.9961135743236456e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 200/6501 [05:52<3:09:50,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:22,286] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 201/6501 [05:53<2:54:57,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0658, 'learning_rate': 1.9980617697806833e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 201/6501 [05:53<2:54:57,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:24,081] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 202/6501 [05:55<2:58:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2011, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 202/6501 [05:55<2:58:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:25,856] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 203/6501 [05:57<3:01:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9287, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 203/6501 [05:57<3:01:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:27,506] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 204/6501 [05:58<2:58:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4005, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 204/6501 [05:58<2:58:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:29,466] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 205/6501 [06:00<3:06:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9303, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 205/6501 [06:00<3:06:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 206/6501 [06:02<3:14:46,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3772, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 206/6501 [06:02<3:14:46,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:33,165] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 207/6501 [06:04<3:08:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0069, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 207/6501 [06:04<3:08:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 208/6501 [06:06<3:05:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9729, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 208/6501 [06:06<3:05:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:36,476] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 209/6501 [06:07<3:00:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1396, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 209/6501 [06:07<3:00:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:37,929] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 210/6501 [06:09<2:52:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2028, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 210/6501 [06:09<2:52:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 211/6501 [06:10<2:29:52,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1367, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 211/6501 [06:10<2:29:52,  1.43s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 212/6501 [06:11<2:17:01,  1.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2451, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 212/6501 [06:11<2:17:01,  1.31s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:41,709] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 213/6501 [06:12<2:33:09,  1.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0097, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 213/6501 [06:12<2:33:09,  1.46s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 214/6501 [06:14<2:27:33,  1.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0483, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 214/6501 [06:14<2:27:33,  1.41s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:44,794] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 215/6501 [06:15<2:39:52,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2097, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 215/6501 [06:15<2:39:52,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:46,342] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 216/6501 [06:17<2:40:31,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3725, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 216/6501 [06:17<2:40:31,  1.53s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 217/6501 [06:19<2:47:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.974, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 217/6501 [06:19<2:47:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 218/6501 [06:20<2:45:05,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0129, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 218/6501 [06:20<2:45:05,  1.58s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 219/6501 [06:22<2:46:59,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7964, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 219/6501 [06:22<2:46:59,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:53,098] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 220/6501 [06:24<2:54:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9846, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 220/6501 [06:24<2:54:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:54,718] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 221/6501 [06:25<2:53:05,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3452, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 221/6501 [06:25<2:53:05,  1.65s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 222/6501 [06:27<3:00:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8535, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 222/6501 [06:27<3:00:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:25:58,063] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 223/6501 [06:29<2:51:53,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8603, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 223/6501 [06:29<2:51:53,  1.64s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 224/6501 [06:30<2:52:32,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0444, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 224/6501 [06:30<2:52:32,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:01,330] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 225/6501 [06:32<2:51:03,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6628, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 225/6501 [06:32<2:51:03,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:03,089] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 226/6501 [06:34<2:54:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.161, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 226/6501 [06:34<2:54:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 227/6501 [06:35<2:42:33,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9839, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 227/6501 [06:35<2:42:33,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:06,285] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 228/6501 [06:37<2:53:52,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9887, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 228/6501 [06:37<2:53:52,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:08,117] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 229/6501 [06:39<2:59:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1553, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 229/6501 [06:39<2:59:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 230/6501 [06:40<2:55:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3731, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 230/6501 [06:40<2:55:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:11,674] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 231/6501 [06:42<3:04:14,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9644, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 231/6501 [06:42<3:04:14,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:13,369] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 232/6501 [06:44<3:02:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1287, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 232/6501 [06:44<3:02:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 233/6501 [06:46<2:56:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3312, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 233/6501 [06:46<2:56:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:16,591] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 234/6501 [06:47<2:55:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8743, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 234/6501 [06:47<2:55:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:18,326] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 235/6501 [06:49<2:57:10,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0031, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 235/6501 [06:49<2:57:10,  1.70s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 236/6501 [06:51<2:55:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3678, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 236/6501 [06:51<2:55:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:21,922] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 237/6501 [06:53<3:03:57,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0423, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 237/6501 [06:53<3:03:57,  1.76s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 238/6501 [06:54<2:39:04,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9744, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 238/6501 [06:54<2:39:04,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:24,695] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 239/6501 [06:55<2:47:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1954, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 239/6501 [06:55<2:47:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:26,367] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 240/6501 [06:57<2:49:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.051, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 240/6501 [06:57<2:49:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:28,154] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 241/6501 [06:59<2:54:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3181, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 241/6501 [06:59<2:54:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:29,925] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 242/6501 [07:01<2:57:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4783, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 242/6501 [07:01<2:57:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:32,079] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 243/6501 [07:03<3:11:49,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1406, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 243/6501 [07:03<3:11:49,  1.84s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 244/6501 [07:05<3:15:15,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4495, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 244/6501 [07:05<3:15:15,  1.87s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 245/6501 [07:06<3:07:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0063, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 245/6501 [07:06<3:07:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 246/6501 [07:08<3:14:11,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8486, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 246/6501 [07:08<3:14:11,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:39,277] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 247/6501 [07:10<3:06:10,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9567, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 247/6501 [07:10<3:06:10,  1.79s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 248/6501 [07:11<2:43:04,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.115, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 248/6501 [07:11<2:43:04,  1.56s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 249/6501 [07:12<2:24:27,  1.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2846, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 249/6501 [07:12<2:24:27,  1.39s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:43,242] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 250/6501 [07:14<2:41:56,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9569, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 250/6501 [07:14<2:41:56,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:45,102] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 251/6501 [07:16<2:51:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 251/6501 [07:16<2:51:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3561, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:46,693] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 252/6501 [07:17<2:49:44,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3168, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 252/6501 [07:17<2:49:44,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:48,829] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 253/6501 [07:20<3:05:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7997, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 253/6501 [07:20<3:05:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 254/6501 [07:21<3:06:48,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5443, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 254/6501 [07:21<3:06:48,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:52,550] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 255/6501 [07:23<3:10:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9885, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 255/6501 [07:23<3:10:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:54,333] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 256/6501 [07:25<3:08:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1955, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 256/6501 [07:25<3:08:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:56,348] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 257/6501 [07:27<3:14:56,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2343, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 257/6501 [07:27<3:14:56,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:26:58,373] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 258/6501 [07:29<3:19:37,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1921, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 258/6501 [07:29<3:19:37,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:00,260] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 259/6501 [07:31<3:18:36,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8687, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 259/6501 [07:31<3:18:36,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:02,321] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 260/6501 [07:33<3:23:19,  1.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6678, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 260/6501 [07:33<3:23:19,  1.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 261/6501 [07:35<3:14:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1467, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 261/6501 [07:35<3:14:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:05,662] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 262/6501 [07:36<3:08:03,  1.81s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 262/6501 [07:36<3:08:03,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4756, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 263/6501 [07:38<3:02:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1848, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 263/6501 [07:38<3:02:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:09,332] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 264/6501 [07:40<3:11:19,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4871, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 264/6501 [07:40<3:11:19,  1.84s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 265/6501 [07:42<3:08:15,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2373, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 265/6501 [07:42<3:08:15,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:13,041] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 266/6501 [07:44<3:13:03,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1291, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 266/6501 [07:44<3:13:03,  1.86s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 267/6501 [07:45<3:02:46,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.373, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 267/6501 [07:45<3:02:46,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:16,280] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 268/6501 [07:47<3:01:13,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2186, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 268/6501 [07:47<3:01:13,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:17,573] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 269/6501 [07:48<2:47:07,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3148, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 269/6501 [07:48<2:47:07,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:19,415] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 270/6501 [07:50<2:54:21,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1728, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 270/6501 [07:50<2:54:21,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:21,202] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 271/6501 [07:52<2:57:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 271/6501 [07:52<2:57:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0291, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 272/6501 [07:53<2:53:39,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2787, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 272/6501 [07:53<2:53:39,  1.67s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 273/6501 [07:54<2:31:24,  1.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3882, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 273/6501 [07:54<2:31:24,  1.46s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:25,511] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 274/6501 [07:56<2:40:59,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3401, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 274/6501 [07:56<2:40:59,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:27,279] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 275/6501 [07:58<2:47:43,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.16, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 275/6501 [07:58<2:47:43,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:29,023] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 276/6501 [08:00<2:51:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5523, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 276/6501 [08:00<2:51:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 277/6501 [08:01<2:51:40,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1132, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 277/6501 [08:01<2:51:40,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:32,796] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 278/6501 [08:03<3:06:02,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9343, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 278/6501 [08:03<3:06:02,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:34,432] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 279/6501 [08:05<3:01:05,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2416, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 279/6501 [08:05<3:01:05,  1.75s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 280/6501 [08:07<2:55:19,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0004, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 280/6501 [08:07<2:55:19,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:37,791] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 281/6501 [08:08<2:58:36,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4792, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 281/6501 [08:08<2:58:36,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:39,274] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 282/6501 [08:10<2:51:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1385, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 282/6501 [08:10<2:51:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:41,145] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 283/6501 [08:12<2:57:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2196, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 283/6501 [08:12<2:57:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:42,901] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 284/6501 [08:14<2:59:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8958, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 284/6501 [08:14<2:59:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:44,757] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 285/6501 [08:15<3:03:01,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1292, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 285/6501 [08:15<3:03:01,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:46,535] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 286/6501 [08:17<3:03:21,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1291, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 286/6501 [08:17<3:03:21,  1.77s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 287/6501 [08:19<3:00:09,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3298, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 287/6501 [08:19<3:00:09,  1.74s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 288/6501 [08:21<3:01:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9645, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 288/6501 [08:21<3:01:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:51,597] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 289/6501 [08:22<2:57:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4048, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 289/6501 [08:22<2:57:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:53,523] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 290/6501 [08:24<3:03:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0889, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 290/6501 [08:24<3:03:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:55,519] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 291/6501 [08:26<3:10:38,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2833, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 291/6501 [08:26<3:10:38,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:57,248] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▍         | 292/6501 [08:28<3:07:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3411, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 292/6501 [08:28<3:07:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:27:58,954] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 293/6501 [08:30<3:03:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1339, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 293/6501 [08:30<3:03:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:00,786] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 294/6501 [08:31<3:05:34,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2343, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 294/6501 [08:31<3:05:34,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:02,424] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 295/6501 [08:33<3:00:43,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2362, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 295/6501 [08:33<3:00:43,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:04,488] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 296/6501 [08:35<3:10:29,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1857, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 296/6501 [08:35<3:10:29,  1.84s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 297/6501 [08:36<2:42:40,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8726, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 297/6501 [08:36<2:42:40,  1.57s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 298/6501 [08:38<2:49:58,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2318, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 298/6501 [08:38<2:49:58,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:09,084] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 299/6501 [08:40<2:56:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0863, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 299/6501 [08:40<2:56:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:11,161] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 300/6501 [08:42<3:07:35,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1088, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 300/6501 [08:42<3:07:35,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:12,374] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 301/6501 [08:43<2:48:55,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1834, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 301/6501 [08:43<2:48:55,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:14,121] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 302/6501 [08:45<2:52:22,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4531, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 302/6501 [08:45<2:52:22,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:15,770] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 303/6501 [08:46<2:51:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3278, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 303/6501 [08:46<2:51:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:17,720] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 304/6501 [08:48<3:00:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3603, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 304/6501 [08:48<3:00:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 305/6501 [08:50<2:57:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0626, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 305/6501 [08:50<2:57:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:20,898] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 306/6501 [08:52<2:51:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3724, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 306/6501 [08:52<2:51:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 307/6501 [08:53<2:34:48,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0161, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 307/6501 [08:53<2:34:48,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:23,949] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 308/6501 [08:55<2:48:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1265, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 308/6501 [08:55<2:48:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:25,706] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 309/6501 [08:56<2:52:07,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8062, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 309/6501 [08:56<2:52:07,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:27,387] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 310/6501 [08:58<2:52:30,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8918, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 310/6501 [08:58<2:52:30,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:29,141] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 311/6501 [09:00<2:55:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7954, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 311/6501 [09:00<2:55:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:30,848] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 312/6501 [09:02<2:55:19,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6787, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 312/6501 [09:02<2:55:19,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:32,303] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 313/6501 [09:03<2:47:42,  1.63s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 313/6501 [09:03<2:47:42,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7747, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 314/6501 [09:04<2:26:48,  1.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.181, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 314/6501 [09:04<2:26:48,  1.42s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:34,944] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 315/6501 [09:06<2:35:00,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.198, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 315/6501 [09:06<2:35:00,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:36,622] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 316/6501 [09:07<2:40:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8467, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 316/6501 [09:07<2:40:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 317/6501 [09:09<2:43:09,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1917, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 317/6501 [09:09<2:43:09,  1.58s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 318/6501 [09:11<2:52:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4186, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 318/6501 [09:11<2:52:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:41,950] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 319/6501 [09:13<2:56:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1262, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 319/6501 [09:13<2:56:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:43,672] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 320/6501 [09:14<2:56:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2693, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 320/6501 [09:14<2:56:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:45,350] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 321/6501 [09:16<2:55:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2057, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 321/6501 [09:16<2:55:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 322/6501 [09:18<2:58:59,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0712, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 322/6501 [09:18<2:58:59,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:49,066] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 323/6501 [09:20<3:03:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5713, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 323/6501 [09:20<3:03:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:50,820] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 324/6501 [09:22<3:02:48,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4697, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 324/6501 [09:22<3:02:48,  1.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 325/6501 [09:23<2:40:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0036, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 325/6501 [09:23<2:40:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 326/6501 [09:24<2:45:41,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5328, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 326/6501 [09:24<2:45:41,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:55,530] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 327/6501 [09:26<2:55:31,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3173, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 327/6501 [09:26<2:55:31,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:57,296] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 328/6501 [09:28<2:57:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1865, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 328/6501 [09:28<2:57:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:28:58,999] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 329/6501 [09:30<2:56:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2059, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 329/6501 [09:30<2:56:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:00,620] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 330/6501 [09:31<2:53:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3256, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 330/6501 [09:31<2:53:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:02,704] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 331/6501 [09:33<3:05:50,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0775, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 331/6501 [09:33<3:05:50,  1.81s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 332/6501 [09:35<2:59:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2886, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 332/6501 [09:35<2:59:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:06,120] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 333/6501 [09:37<3:01:24,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2542, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 333/6501 [09:37<3:01:24,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:08,074] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 334/6501 [09:39<3:07:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8839, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 334/6501 [09:39<3:07:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 335/6501 [09:41<3:10:42,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0415, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 335/6501 [09:41<3:10:42,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:11,653] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 336/6501 [09:42<3:04:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2455, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 336/6501 [09:42<3:04:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:13,640] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 337/6501 [09:44<3:10:07,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1425, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 337/6501 [09:44<3:10:07,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:15,182] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 338/6501 [09:46<3:00:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4808, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 338/6501 [09:46<3:00:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 339/6501 [09:47<2:53:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0933, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 339/6501 [09:47<2:53:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 340/6501 [09:49<2:49:03,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3872, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 340/6501 [09:49<2:49:03,  1.65s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 341/6501 [09:51<2:50:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2093, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 341/6501 [09:51<2:50:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:21,657] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 342/6501 [09:52<2:52:04,  1.68s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 342/6501 [09:52<2:52:04,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2491, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 343/6501 [09:54<2:51:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.096, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 343/6501 [09:54<2:51:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:25,132] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 344/6501 [09:56<2:55:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2227, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 344/6501 [09:56<2:55:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 345/6501 [09:57<2:36:39,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1393, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 345/6501 [09:57<2:36:39,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:28,118] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 346/6501 [09:59<2:48:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2988, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 346/6501 [09:59<2:48:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 347/6501 [10:01<2:50:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8605, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 347/6501 [10:01<2:50:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:31,922] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 348/6501 [10:03<3:03:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.96, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 348/6501 [10:03<3:03:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 349/6501 [10:04<2:56:47,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2493, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 349/6501 [10:04<2:56:47,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:35,464] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 350/6501 [10:06<3:04:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1824, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 350/6501 [10:06<3:04:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:37,240] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 351/6501 [10:08<3:03:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1786, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 351/6501 [10:08<3:03:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 352/6501 [10:10<3:13:50,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2828, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 352/6501 [10:10<3:13:50,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:41,362] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 353/6501 [10:12<3:17:03,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9526, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 353/6501 [10:12<3:17:03,  1.92s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 354/6501 [10:14<3:17:56,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3401, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 354/6501 [10:14<3:17:56,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:44,586] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▌         | 355/6501 [10:15<2:57:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2122, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 355/6501 [10:15<2:57:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 356/6501 [10:17<2:52:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0049, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 356/6501 [10:17<2:52:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 357/6501 [10:19<2:56:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2901, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 357/6501 [10:19<2:56:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:49,733] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 358/6501 [10:20<2:57:51,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1345, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 358/6501 [10:20<2:57:51,  1.74s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 359/6501 [10:22<3:00:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0433, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 359/6501 [10:22<3:00:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:53,292] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 360/6501 [10:24<2:59:41,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1695, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 360/6501 [10:24<2:59:41,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:55,063] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 361/6501 [10:26<3:00:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.743, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 361/6501 [10:26<3:00:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:29:56,909] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 362/6501 [10:28<3:02:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2521, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 362/6501 [10:28<3:02:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 363/6501 [10:29<3:04:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.194, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 363/6501 [10:29<3:04:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:00,319] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 364/6501 [10:31<2:57:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0646, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 364/6501 [10:31<2:57:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:02,033] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 365/6501 [10:33<2:56:32,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.088, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 365/6501 [10:33<2:56:32,  1.73s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 366/6501 [10:34<2:43:30,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0039, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 366/6501 [10:34<2:43:30,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:05,118] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 367/6501 [10:36<2:49:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4205, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 367/6501 [10:36<2:49:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 368/6501 [10:38<2:50:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2185, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 368/6501 [10:38<2:50:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 369/6501 [10:39<2:45:06,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1638, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 369/6501 [10:39<2:45:06,  1.62s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 370/6501 [10:41<2:50:09,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0714, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 370/6501 [10:41<2:50:09,  1.67s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 371/6501 [10:42<2:34:08,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.295, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 371/6501 [10:42<2:34:08,  1.51s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 372/6501 [10:43<2:34:41,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4331, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 372/6501 [10:43<2:34:41,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:14,624] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 373/6501 [10:45<2:45:20,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3891, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 373/6501 [10:45<2:45:20,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:16,750] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 374/6501 [10:47<3:00:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4798, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 374/6501 [10:47<3:00:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:18,489] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 375/6501 [10:49<2:59:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4435, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 375/6501 [10:49<2:59:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 376/6501 [10:51<2:54:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2693, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 376/6501 [10:51<2:54:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 377/6501 [10:53<2:55:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0902, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 377/6501 [10:53<2:55:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:23,621] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 378/6501 [10:54<2:58:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1469, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 378/6501 [10:54<2:58:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 379/6501 [10:55<2:37:29,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2972, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 379/6501 [10:55<2:37:29,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:26,285] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 380/6501 [10:57<2:38:57,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2099, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 380/6501 [10:57<2:38:57,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:28,176] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 381/6501 [10:59<2:49:06,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1804, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 381/6501 [10:59<2:49:06,  1.66s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 382/6501 [11:00<2:47:22,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2067, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 382/6501 [11:00<2:47:22,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:31,676] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 383/6501 [11:02<2:55:11,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1368, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 383/6501 [11:02<2:55:11,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:34,053] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 384/6501 [11:05<3:15:19,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6274, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 384/6501 [11:05<3:15:19,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:36,152] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 385/6501 [11:07<3:20:52,  1.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2398, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 385/6501 [11:07<3:20:52,  1.97s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 386/6501 [11:09<3:13:51,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4854, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 386/6501 [11:09<3:13:51,  1.90s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 387/6501 [11:11<3:23:13,  1.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4164, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 387/6501 [11:11<3:23:13,  1.99s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 388/6501 [11:13<3:19:43,  1.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1017, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 388/6501 [11:13<3:19:43,  1.96s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:43,780] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 389/6501 [11:14<3:14:40,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9485, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 389/6501 [11:14<3:14:40,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:45,458] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 390/6501 [11:16<3:07:28,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.149, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 390/6501 [11:16<3:07:28,  1.84s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 391/6501 [11:18<3:08:39,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3198, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 391/6501 [11:18<3:08:39,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:49,084] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 392/6501 [11:20<3:05:23,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1006, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 392/6501 [11:20<3:05:23,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:50,907] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 393/6501 [11:22<3:05:24,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1408, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 393/6501 [11:22<3:05:24,  1.82s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 394/6501 [11:23<3:03:06,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8674, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 394/6501 [11:23<3:03:06,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:54,438] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 395/6501 [11:25<3:02:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0078, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 395/6501 [11:25<3:02:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:56,185] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 396/6501 [11:27<3:01:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9944, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 396/6501 [11:27<3:01:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:30:57,841] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 397/6501 [11:29<2:57:19,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0354, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 397/6501 [11:29<2:57:19,  1.74s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 398/6501 [11:30<2:33:38,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.074, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 398/6501 [11:30<2:33:38,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:00,602] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 399/6501 [11:31<2:42:13,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9946, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 399/6501 [11:31<2:42:13,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:02,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 400/6501 [11:33<2:49:55,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5141, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 400/6501 [11:33<2:49:55,  1.67s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 401/6501 [11:34<2:31:31,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4419, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 401/6501 [11:34<2:31:31,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:05,411] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 402/6501 [11:36<2:43:45,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6931, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 402/6501 [11:36<2:43:45,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:07,060] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 403/6501 [11:38<2:44:53,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1638, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 403/6501 [11:38<2:44:53,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:08,721] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 404/6501 [11:39<2:46:01,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2849, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 404/6501 [11:39<2:46:01,  1.63s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 405/6501 [11:41<2:48:56,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3527, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 405/6501 [11:41<2:48:56,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:12,210] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 406/6501 [11:43<2:51:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.279, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 406/6501 [11:43<2:51:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:14,062] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 407/6501 [11:45<2:56:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1501, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 407/6501 [11:45<2:56:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:15,856] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 408/6501 [11:47<2:58:19,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5813, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 408/6501 [11:47<2:58:19,  1.76s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 409/6501 [11:48<2:54:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1428, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 409/6501 [11:48<2:54:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:19,472] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 410/6501 [11:50<3:02:31,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3117, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 410/6501 [11:50<3:02:31,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:21,018] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 411/6501 [11:52<2:54:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1584, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 411/6501 [11:52<2:54:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 412/6501 [11:53<2:52:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2023, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 412/6501 [11:53<2:52:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:24,062] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 413/6501 [11:55<2:43:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5358, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 413/6501 [11:55<2:43:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:25,640] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 414/6501 [11:56<2:42:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9798, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 414/6501 [11:56<2:42:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 415/6501 [11:58<2:32:45,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6564, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 415/6501 [11:58<2:32:45,  1.51s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 416/6501 [11:59<2:41:38,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1752, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 416/6501 [11:59<2:41:38,  1.59s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 417/6501 [12:01<2:44:21,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0295, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 417/6501 [12:01<2:44:21,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:32,615] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 418/6501 [12:03<3:02:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0756, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 418/6501 [12:03<3:02:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:34,293] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 419/6501 [12:05<2:58:28,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.103, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 419/6501 [12:05<2:58:28,  1.76s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 420/6501 [12:06<2:45:21,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3955, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 420/6501 [12:06<2:45:21,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:37,730] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▋         | 421/6501 [12:08<2:59:48,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2536, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 421/6501 [12:08<2:59:48,  1.77s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 422/6501 [12:10<2:59:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3208, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 422/6501 [12:10<2:59:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:41,311] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 423/6501 [12:12<3:00:40,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2486, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 423/6501 [12:12<3:00:40,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:42,897] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 424/6501 [12:14<2:54:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2261, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 424/6501 [12:14<2:54:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:44,815] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 425/6501 [12:16<3:00:28,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2239, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 425/6501 [12:16<3:00:28,  1.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 426/6501 [12:17<2:45:52,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0551, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 426/6501 [12:17<2:45:52,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:47,817] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 427/6501 [12:19<2:47:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4885, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 427/6501 [12:19<2:47:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:49,381] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 428/6501 [12:20<2:44:53,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.319, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 428/6501 [12:20<2:44:53,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:51,535] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 429/6501 [12:22<3:00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2101, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 429/6501 [12:22<3:00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 430/6501 [12:24<2:57:30,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2816, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 430/6501 [12:24<2:57:30,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:55,116] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 431/6501 [12:26<3:01:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5674, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 431/6501 [12:26<3:01:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:56,928] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 432/6501 [12:28<3:02:20,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6068, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 432/6501 [12:28<3:02:20,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:31:58,722] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 433/6501 [12:29<3:02:02,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2079, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 433/6501 [12:29<3:02:02,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:00,496] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 434/6501 [12:31<3:01:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1355, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 434/6501 [12:31<3:01:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:02,156] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 435/6501 [12:33<2:57:10,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.205, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 435/6501 [12:33<2:57:10,  1.75s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 436/6501 [12:35<3:06:41,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.914, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 436/6501 [12:35<3:06:41,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:05,936] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 437/6501 [12:37<3:02:36,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0598, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 437/6501 [12:37<3:02:36,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:07,732] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 438/6501 [12:38<3:02:15,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0699, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 438/6501 [12:38<3:02:15,  1.80s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 439/6501 [12:40<2:51:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7972, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 439/6501 [12:40<2:51:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:10,947] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 440/6501 [12:42<2:53:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.164, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 440/6501 [12:42<2:53:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:12,572] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 441/6501 [12:43<2:50:38,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3194, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 441/6501 [12:43<2:50:38,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:14,397] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 442/6501 [12:45<2:54:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1177, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 442/6501 [12:45<2:54:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 443/6501 [12:46<2:32:18,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.049, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 443/6501 [12:46<2:32:18,  1.51s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 444/6501 [12:48<2:37:55,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1264, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 444/6501 [12:48<2:37:55,  1.56s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 445/6501 [12:49<2:20:12,  1.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4852, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 445/6501 [12:49<2:20:12,  1.39s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:20,022] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 446/6501 [12:51<2:37:26,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1849, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 446/6501 [12:51<2:37:26,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:21,892] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 447/6501 [12:53<2:46:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9777, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 447/6501 [12:53<2:46:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 448/6501 [12:54<2:52:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0302, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 448/6501 [12:54<2:52:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:25,502] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 449/6501 [12:56<2:54:04,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1664, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 449/6501 [12:56<2:54:04,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:27,504] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 450/6501 [12:58<3:02:23,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9307, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 450/6501 [12:58<3:02:23,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:29,878] [WARNING] [stage3.py:1787:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 451/6501 [13:01<3:19:28,  1.98s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1768, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 451/6501 [13:01<3:19:28,  1.98s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 452/6501 [13:03<3:29:32,  2.08s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2699, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 452/6501 [13:03<3:29:32,  2.08s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:33,817] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 453/6501 [13:05<3:15:50,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3219, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 453/6501 [13:05<3:15:50,  1.94s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:35,599] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 454/6501 [13:06<3:10:57,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.976, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 454/6501 [13:06<3:10:57,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:37,365] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 455/6501 [13:08<3:07:01,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1819, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 455/6501 [13:08<3:07:01,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:39,148] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 456/6501 [13:10<3:04:47,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3057, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 456/6501 [13:10<3:04:47,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:40,729] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 457/6501 [13:11<2:57:06,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9448, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 457/6501 [13:11<2:57:06,  1.76s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 458/6501 [13:12<2:34:07,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2456, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 458/6501 [13:12<2:34:07,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:43,194] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 459/6501 [13:14<2:32:09,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1647, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 459/6501 [13:14<2:32:09,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:44,854] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 460/6501 [13:16<2:36:39,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2943, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 460/6501 [13:16<2:36:39,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:46,574] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 461/6501 [13:17<2:41:34,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.225, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 461/6501 [13:17<2:41:34,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:48,349] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 462/6501 [13:19<2:46:40,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0473, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 462/6501 [13:19<2:46:40,  1.66s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 463/6501 [13:21<2:50:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1318, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 463/6501 [13:21<2:50:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:51,782] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 464/6501 [13:22<2:49:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1635, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 464/6501 [13:22<2:49:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:53,437] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 465/6501 [13:24<2:48:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0958, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 465/6501 [13:24<2:48:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 466/6501 [13:26<2:51:39,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2211, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 466/6501 [13:26<2:51:39,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:32:57,480] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 467/6501 [13:28<3:08:15,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1888, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 467/6501 [13:28<3:08:15,  1.87s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 468/6501 [13:30<3:05:46,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.176, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 468/6501 [13:30<3:05:46,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:01,017] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 469/6501 [13:32<3:02:42,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1985, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 469/6501 [13:32<3:02:42,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:02,157] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 470/6501 [13:33<2:42:14,  1.61s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 470/6501 [13:33<2:42:14,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0353, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:04,110] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 471/6501 [13:35<2:52:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4177, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 471/6501 [13:35<2:52:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 472/6501 [13:36<2:50:04,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3731, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 472/6501 [13:36<2:50:04,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:07,250] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 473/6501 [13:38<2:44:16,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5372, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 473/6501 [13:38<2:44:16,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:08,977] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 474/6501 [13:40<2:47:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2842, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 474/6501 [13:40<2:47:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:10,629] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 475/6501 [13:41<2:46:40,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9433, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 475/6501 [13:41<2:46:40,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:12,271] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 476/6501 [13:43<2:46:07,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0055, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 476/6501 [13:43<2:46:07,  1.65s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 477/6501 [13:45<2:42:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0142, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 477/6501 [13:45<2:42:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:15,471] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 478/6501 [13:46<2:43:50,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0937, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 478/6501 [13:46<2:43:50,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:17,297] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 479/6501 [13:48<2:49:39,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5875, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 479/6501 [13:48<2:49:39,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:18,853] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 480/6501 [13:50<2:45:34,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1912, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 480/6501 [13:50<2:45:34,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:20,460] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 481/6501 [13:51<2:44:14,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2249, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 481/6501 [13:51<2:44:14,  1.64s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 482/6501 [13:53<2:44:14,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4934, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 482/6501 [13:53<2:44:14,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:24,225] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 483/6501 [13:55<2:58:57,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3835, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 483/6501 [13:55<2:58:57,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:26,044] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 484/6501 [13:57<2:59:58,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.093, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 484/6501 [13:57<2:59:58,  1.79s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 485/6501 [13:58<2:58:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2549, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 485/6501 [13:58<2:58:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 486/6501 [14:00<3:00:21,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4345, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 486/6501 [14:00<3:00:21,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:31,703] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m7%|▋         | 487/6501 [14:02<3:08:36,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1043, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 487/6501 [14:02<3:08:36,  1.88s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 488/6501 [14:04<3:03:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1338, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 488/6501 [14:04<3:03:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:35,214] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 489/6501 [14:06<3:02:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9308, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 489/6501 [14:06<3:02:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 490/6501 [14:07<2:50:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1522, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 490/6501 [14:07<2:50:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:38,184] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 491/6501 [14:09<2:45:53,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6063, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 491/6501 [14:09<2:45:53,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:39,944] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 492/6501 [14:11<2:48:58,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2255, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 492/6501 [14:11<2:48:58,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:41,723] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 493/6501 [14:12<2:51:43,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1351, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 493/6501 [14:12<2:51:43,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:43,384] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 494/6501 [14:14<2:50:04,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2598, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 494/6501 [14:14<2:50:04,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:45,194] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 495/6501 [14:16<2:53:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8652, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 495/6501 [14:16<2:53:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:46,905] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 496/6501 [14:18<2:52:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3565, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 496/6501 [14:18<2:52:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 497/6501 [14:19<2:50:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1989, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 497/6501 [14:19<2:50:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:50,285] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 498/6501 [14:21<2:51:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0697, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 498/6501 [14:21<2:51:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:51,605] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 499/6501 [14:22<2:39:21,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2404, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 499/6501 [14:22<2:39:21,  1.59s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 500/6501 [14:23<2:23:01,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1586, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 500/6501 [14:23<2:23:01,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:54,425] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 501/6501 [14:25<2:33:13,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5579, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 501/6501 [14:25<2:33:13,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:56,231] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 502/6501 [14:27<2:41:24,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2618, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 502/6501 [14:27<2:41:24,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:33:57,962] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 503/6501 [14:29<2:44:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.8789, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 503/6501 [14:29<2:44:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 504/6501 [14:30<2:39:49,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3655, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 504/6501 [14:30<2:39:49,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:01,225] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 505/6501 [14:32<2:45:15,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.906, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 505/6501 [14:32<2:45:15,  1.65s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 506/6501 [14:33<2:36:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6297, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 506/6501 [14:33<2:36:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:04,153] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 507/6501 [14:35<2:36:31,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.171, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 507/6501 [14:35<2:36:31,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:05,861] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 508/6501 [14:37<2:40:44,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2089, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 508/6501 [14:37<2:40:44,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:07,704] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 509/6501 [14:38<2:47:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2681, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 509/6501 [14:38<2:47:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:09,396] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 510/6501 [14:40<2:48:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 510/6501 [14:40<2:48:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3062, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:11,386] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 511/6501 [14:42<2:57:13,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0784, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 511/6501 [14:42<2:57:13,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:13,317] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 512/6501 [14:44<3:01:52,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2881, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 512/6501 [14:44<3:01:52,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:14,948] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 513/6501 [14:46<2:56:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8077, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 513/6501 [14:46<2:56:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:16,885] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 514/6501 [14:48<3:01:14,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9592, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 514/6501 [14:48<3:01:14,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:18,641] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 515/6501 [14:49<2:59:24,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5835, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 515/6501 [14:49<2:59:24,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:20,453] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 516/6501 [14:51<2:59:47,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2479, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 516/6501 [14:51<2:59:47,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:22,197] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 517/6501 [14:53<2:58:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3818, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 517/6501 [14:53<2:58:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:23,638] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 518/6501 [14:54<2:47:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9724, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 518/6501 [14:54<2:47:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:25,383] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 519/6501 [14:56<2:49:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2717, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 519/6501 [14:56<2:49:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:26,901] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 520/6501 [14:58<2:44:03,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4513, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 520/6501 [14:58<2:44:03,  1.65s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 521/6501 [14:59<2:23:42,  1.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3963, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 521/6501 [14:59<2:23:42,  1.44s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:29,959] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 522/6501 [15:01<2:43:06,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3463, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 522/6501 [15:01<2:43:06,  1.64s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 523/6501 [15:02<2:42:54,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4577, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 523/6501 [15:02<2:42:54,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:33,583] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 524/6501 [15:04<2:53:34,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2591, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 524/6501 [15:04<2:53:34,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:35,342] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 525/6501 [15:06<2:54:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4092, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 525/6501 [15:06<2:54:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 526/6501 [15:08<2:51:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0311, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 526/6501 [15:08<2:51:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:39,029] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 527/6501 [15:10<3:00:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8447, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 527/6501 [15:10<3:00:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 528/6501 [15:11<2:57:33,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7876, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 528/6501 [15:11<2:57:33,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:42,594] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 529/6501 [15:13<2:59:29,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2716, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 529/6501 [15:13<2:59:29,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:44,165] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 530/6501 [15:15<2:52:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2682, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 530/6501 [15:15<2:52:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:45,741] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 531/6501 [15:16<2:47:47,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2105, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 531/6501 [15:16<2:47:47,  1.69s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 532/6501 [15:18<2:35:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1935, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 532/6501 [15:18<2:35:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:48,517] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 533/6501 [15:19<2:33:40,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9654, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 533/6501 [15:19<2:33:40,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:50,265] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 534/6501 [15:21<2:39:40,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2382, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 534/6501 [15:21<2:39:40,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:52,045] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 535/6501 [15:23<2:44:51,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5183, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 535/6501 [15:23<2:44:51,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:53,692] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 536/6501 [15:24<2:44:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3853, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 536/6501 [15:24<2:44:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:55,454] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 537/6501 [15:26<2:47:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.14, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 537/6501 [15:26<2:47:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:57,094] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 538/6501 [15:28<2:46:14,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9083, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 538/6501 [15:28<2:46:14,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:34:58,688] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 539/6501 [15:29<2:43:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3239, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 539/6501 [15:29<2:43:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:00,782] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 540/6501 [15:31<2:57:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1757, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 540/6501 [15:31<2:57:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 541/6501 [15:33<2:55:02,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5682, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 541/6501 [15:33<2:55:02,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:04,207] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 542/6501 [15:35<2:53:28,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3215, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 542/6501 [15:35<2:53:28,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:06,261] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 543/6501 [15:37<3:02:36,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3768, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 543/6501 [15:37<3:02:36,  1.84s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 544/6501 [15:39<2:56:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2447, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 544/6501 [15:39<2:56:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 545/6501 [15:40<2:56:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0017, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 545/6501 [15:40<2:56:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:11,448] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 546/6501 [15:42<2:56:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1829, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 546/6501 [15:42<2:56:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 547/6501 [15:44<2:46:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3467, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 547/6501 [15:44<2:46:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 548/6501 [15:45<2:50:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0178, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 548/6501 [15:45<2:50:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:16,305] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 549/6501 [15:47<2:46:47,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1864, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 549/6501 [15:47<2:46:47,  1.68s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 550/6501 [15:49<2:48:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1775, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 550/6501 [15:49<2:48:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:19,943] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 551/6501 [15:51<2:54:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1714, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 551/6501 [15:51<2:54:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:21,471] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 552/6501 [15:52<2:47:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3757, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 552/6501 [15:52<2:47:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:23,245] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 553/6501 [15:54<2:50:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2244, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 553/6501 [15:54<2:50:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:25,033] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 554/6501 [15:56<2:52:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1347, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 554/6501 [15:56<2:52:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:26,664] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 555/6501 [15:57<2:49:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0945, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 555/6501 [15:57<2:49:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 556/6501 [15:59<2:58:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8632, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 556/6501 [15:59<2:58:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:30,656] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 557/6501 [16:01<3:03:12,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1759, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 557/6501 [16:01<3:03:12,  1.85s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 558/6501 [16:03<3:02:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0911, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 558/6501 [16:03<3:02:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:33,846] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 559/6501 [16:05<2:48:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0289, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 559/6501 [16:05<2:48:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:35,522] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 560/6501 [16:06<2:47:34,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9603, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 560/6501 [16:06<2:47:34,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:37,618] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 561/6501 [16:08<2:59:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4335, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 561/6501 [16:08<2:59:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:39,050] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 562/6501 [16:10<2:48:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2473, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 562/6501 [16:10<2:48:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:40,699] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 563/6501 [16:11<2:46:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3116, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 563/6501 [16:11<2:46:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 564/6501 [16:13<2:39:51,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3004, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 564/6501 [16:13<2:39:51,  1.62s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 565/6501 [16:14<2:37:48,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6222, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 565/6501 [16:14<2:37:48,  1.60s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 566/6501 [16:16<2:46:04,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2978, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 566/6501 [16:16<2:46:04,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:47,429] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▊         | 567/6501 [16:18<2:51:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1036, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 567/6501 [16:18<2:51:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 568/6501 [16:19<2:33:34,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3429, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 568/6501 [16:19<2:33:34,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:50,590] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 569/6501 [16:21<2:47:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3561, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 569/6501 [16:21<2:47:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:52,364] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 570/6501 [16:23<2:49:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3634, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 570/6501 [16:23<2:49:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:54,289] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 571/6501 [16:25<2:55:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 571/6501 [16:25<2:55:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4669, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:56,086] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 572/6501 [16:27<2:56:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4645, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 572/6501 [16:27<2:56:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 573/6501 [16:28<2:53:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2751, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 573/6501 [16:28<2:53:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:35:59,337] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 574/6501 [16:30<2:47:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3803, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 574/6501 [16:30<2:47:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:01,345] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 575/6501 [16:32<2:56:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0635, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 575/6501 [16:32<2:56:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 576/6501 [16:34<2:53:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2029, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 576/6501 [16:34<2:53:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:05,000] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 577/6501 [16:36<2:59:49,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.752, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 577/6501 [16:36<2:59:49,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:06,792] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 578/6501 [16:37<2:58:55,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1862, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 578/6501 [16:37<2:58:55,  1.81s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 579/6501 [16:39<2:49:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 579/6501 [16:39<2:49:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3173, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:10,112] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 580/6501 [16:41<2:52:41,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1382, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 580/6501 [16:41<2:52:41,  1.75s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 581/6501 [16:42<2:50:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1815, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 581/6501 [16:42<2:50:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:13,925] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 582/6501 [16:45<3:02:41,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1985, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 582/6501 [16:45<3:02:41,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:15,654] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 583/6501 [16:46<2:59:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2565, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 583/6501 [16:46<2:59:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 584/6501 [16:48<2:51:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3806, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 584/6501 [16:48<2:51:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:19,259] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 585/6501 [16:50<3:00:22,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1005, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 585/6501 [16:50<3:00:22,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:21,302] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 586/6501 [16:52<3:06:39,  1.89s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 586/6501 [16:52<3:06:39,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8225, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 587/6501 [16:54<3:02:28,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9378, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 587/6501 [16:54<3:02:28,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:24,608] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 588/6501 [16:55<2:53:36,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1307, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 588/6501 [16:55<2:53:36,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:26,410] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 589/6501 [16:57<2:54:46,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8661, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 589/6501 [16:57<2:54:46,  1.77s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 590/6501 [16:59<2:52:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6514, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 590/6501 [16:59<2:52:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:29,713] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 591/6501 [17:00<2:48:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4838, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 591/6501 [17:00<2:48:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:31,481] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 592/6501 [17:02<2:49:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1901, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 592/6501 [17:02<2:49:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:33,127] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 593/6501 [17:04<2:47:27,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2434, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 593/6501 [17:04<2:47:27,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:34,789] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 594/6501 [17:05<2:46:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0707, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 594/6501 [17:05<2:46:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:36,475] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 595/6501 [17:07<2:46:11,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2572, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 595/6501 [17:07<2:46:11,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:37,753] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 596/6501 [17:08<2:34:02,  1.57s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 596/6501 [17:08<2:34:02,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3774, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:39,702] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 597/6501 [17:10<2:45:20,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2254, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 597/6501 [17:10<2:45:20,  1.68s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 598/6501 [17:12<2:46:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2831, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 598/6501 [17:12<2:46:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:43,004] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 599/6501 [17:14<2:43:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8469, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 599/6501 [17:14<2:43:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:44,527] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 600/6501 [17:15<2:39:06,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2084, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 600/6501 [17:15<2:39:06,  1.62s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 601/6501 [17:17<2:38:30,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1367, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 601/6501 [17:17<2:38:30,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:47,916] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 602/6501 [17:19<2:43:44,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3269, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 602/6501 [17:19<2:43:44,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:49,696] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 603/6501 [17:20<2:47:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2007, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 603/6501 [17:20<2:47:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 604/6501 [17:22<2:46:55,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2596, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 604/6501 [17:22<2:46:55,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:52,966] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 605/6501 [17:24<2:43:15,  1.66s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 605/6501 [17:24<2:43:15,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9335, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:54,673] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 606/6501 [17:25<2:44:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3738, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 606/6501 [17:25<2:44:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:36:56,420] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 607/6501 [17:27<2:46:40,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4307, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 607/6501 [17:27<2:46:40,  1.70s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 608/6501 [17:29<2:47:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1911, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 608/6501 [17:29<2:47:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 609/6501 [17:30<2:45:12,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4348, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 609/6501 [17:30<2:45:12,  1.68s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 610/6501 [17:32<2:44:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1317, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 610/6501 [17:32<2:44:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:03,042] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 611/6501 [17:34<2:42:33,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3071, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 611/6501 [17:34<2:42:33,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:04,685] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 612/6501 [17:35<2:42:10,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3504, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 612/6501 [17:35<2:42:10,  1.65s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 613/6501 [17:37<2:42:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0823, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 613/6501 [17:37<2:42:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 614/6501 [17:38<2:21:00,  1.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1533, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 614/6501 [17:38<2:21:00,  1.44s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:09,021] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 615/6501 [17:40<2:29:52,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9603, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 615/6501 [17:40<2:29:52,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:10,949] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 616/6501 [17:42<2:41:37,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2762, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 616/6501 [17:42<2:41:37,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:12,541] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 617/6501 [17:43<2:39:56,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.186, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 617/6501 [17:43<2:39:56,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:14,397] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 618/6501 [17:45<2:46:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0847, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 618/6501 [17:45<2:46:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 619/6501 [17:47<2:46:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8861, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 619/6501 [17:47<2:46:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:17,910] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 620/6501 [17:49<2:49:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5977, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 620/6501 [17:49<2:49:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:19,900] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 621/6501 [17:51<2:57:24,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3169, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 621/6501 [17:51<2:57:24,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:22,114] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 622/6501 [17:53<3:09:14,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.174, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 622/6501 [17:53<3:09:14,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:23,986] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 623/6501 [17:55<3:07:27,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9722, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 623/6501 [17:55<3:07:27,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:25,975] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 624/6501 [17:57<3:09:40,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.163, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 624/6501 [17:57<3:09:40,  1.94s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 625/6501 [17:58<3:02:05,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1178, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 625/6501 [17:58<3:02:05,  1.86s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 626/6501 [18:00<2:59:54,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1577, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 626/6501 [18:00<2:59:54,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:31,129] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 627/6501 [18:02<2:55:28,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1177, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 627/6501 [18:02<2:55:28,  1.79s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 628/6501 [18:04<2:54:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1275, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 628/6501 [18:04<2:54:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:34,429] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 629/6501 [18:05<2:47:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2837, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 629/6501 [18:05<2:47:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:36,237] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 630/6501 [18:07<2:50:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 630/6501 [18:07<2:50:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2946, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 631/6501 [18:08<2:44:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4547, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 631/6501 [18:08<2:44:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:39,498] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 632/6501 [18:10<2:45:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.967, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 632/6501 [18:10<2:45:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 633/6501 [18:12<2:42:05,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2582, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 633/6501 [18:12<2:42:05,  1.66s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 634/6501 [18:13<2:42:07,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5264, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 634/6501 [18:13<2:42:07,  1.66s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 635/6501 [18:15<2:40:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2019, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 635/6501 [18:15<2:40:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:46,088] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 636/6501 [18:17<2:43:34,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1321, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 636/6501 [18:17<2:43:34,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:47,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 637/6501 [18:18<2:34:24,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8811, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 637/6501 [18:18<2:34:24,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:49,244] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 638/6501 [18:20<2:40:40,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0689, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 638/6501 [18:20<2:40:40,  1.64s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 639/6501 [18:22<2:43:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.111, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 639/6501 [18:22<2:43:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:52,640] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 640/6501 [18:23<2:42:58,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3413, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 640/6501 [18:23<2:42:58,  1.67s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 641/6501 [18:25<2:42:36,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6358, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 641/6501 [18:25<2:42:36,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:56,435] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 642/6501 [18:27<2:56:25,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9735, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 642/6501 [18:27<2:56:25,  1.81s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 643/6501 [18:29<2:47:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6809, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 643/6501 [18:29<2:47:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:37:59,700] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 644/6501 [18:30<2:48:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3487, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 644/6501 [18:30<2:48:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:01,350] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 645/6501 [18:32<2:46:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2088, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 645/6501 [18:32<2:46:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:03,308] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 646/6501 [18:34<2:53:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8256, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 646/6501 [18:34<2:53:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 647/6501 [18:36<2:47:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1921, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 647/6501 [18:36<2:47:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 648/6501 [18:37<2:49:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0498, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 648/6501 [18:37<2:49:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 649/6501 [18:39<2:53:07,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.187, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 649/6501 [18:39<2:53:07,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:10,388] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|▉         | 650/6501 [18:41<2:55:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0343, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 650/6501 [18:41<2:55:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:12,278] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 651/6501 [18:43<2:58:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2466, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 651/6501 [18:43<2:58:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 652/6501 [18:45<2:56:47,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.387, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 652/6501 [18:45<2:56:47,  1.81s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 653/6501 [18:46<2:49:39,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1284, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 653/6501 [18:46<2:49:39,  1.74s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 654/6501 [18:48<2:48:57,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4708, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 654/6501 [18:48<2:48:57,  1.73s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 655/6501 [18:49<2:29:28,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2141, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 655/6501 [18:49<2:29:28,  1.53s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 656/6501 [18:51<2:34:04,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2672, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 656/6501 [18:51<2:34:04,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:21,913] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 657/6501 [18:53<2:40:38,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5074, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 657/6501 [18:53<2:40:38,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:23,789] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 658/6501 [18:54<2:47:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.201, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 658/6501 [18:54<2:47:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:25,542] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 659/6501 [18:56<2:48:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1553, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 659/6501 [18:56<2:48:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 660/6501 [18:58<2:47:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3337, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 660/6501 [18:58<2:47:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 661/6501 [18:59<2:42:14,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2174, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 661/6501 [18:59<2:42:14,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:30,719] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 662/6501 [19:01<2:50:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1517, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 662/6501 [19:01<2:50:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 663/6501 [19:03<2:52:48,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1907, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 663/6501 [19:03<2:52:48,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:34,377] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 664/6501 [19:05<2:53:54,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3039, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 664/6501 [19:05<2:53:54,  1.79s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 665/6501 [19:07<2:52:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.245, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 665/6501 [19:07<2:52:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:37,857] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 666/6501 [19:09<2:51:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2507, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 666/6501 [19:09<2:51:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:39,749] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 667/6501 [19:10<2:55:12,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.159, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 667/6501 [19:10<2:55:12,  1.80s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 668/6501 [19:12<2:47:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0866, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 668/6501 [19:12<2:47:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 669/6501 [19:14<2:43:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.198, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 669/6501 [19:14<2:43:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 670/6501 [19:14<2:21:17,  1.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1093, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 670/6501 [19:14<2:21:17,  1.45s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 671/6501 [19:16<2:30:17,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2095, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 671/6501 [19:16<2:30:17,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:47,417] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 672/6501 [19:18<2:39:22,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9953, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 672/6501 [19:18<2:39:22,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:48,973] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 673/6501 [19:20<2:36:52,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0045, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 673/6501 [19:20<2:36:52,  1.62s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 674/6501 [19:21<2:34:52,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3266, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 674/6501 [19:21<2:34:52,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:52,306] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 675/6501 [19:23<2:40:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1995, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 675/6501 [19:23<2:40:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 676/6501 [19:24<2:19:53,  1.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.253, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 676/6501 [19:24<2:19:53,  1.44s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:55,153] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 677/6501 [19:26<2:33:12,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3018, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 677/6501 [19:26<2:33:12,  1.58s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 678/6501 [19:27<2:31:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2377, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 678/6501 [19:27<2:31:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:38:58,683] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 679/6501 [19:29<2:44:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8116, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 679/6501 [19:29<2:44:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 680/6501 [19:31<2:39:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0423, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 680/6501 [19:31<2:39:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:02,393] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 681/6501 [19:33<2:55:20,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9341, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 681/6501 [19:33<2:55:20,  1.81s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 682/6501 [19:35<2:52:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3901, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 682/6501 [19:35<2:52:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:05,856] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 683/6501 [19:37<2:51:36,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3963, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 683/6501 [19:37<2:51:36,  1.77s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 684/6501 [19:39<2:56:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9174, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 684/6501 [19:39<2:56:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:09,392] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 685/6501 [19:40<2:49:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1815, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 685/6501 [19:40<2:49:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:11,350] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 686/6501 [19:42<2:55:48,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1849, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 686/6501 [19:42<2:55:48,  1.81s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 687/6501 [19:44<2:55:46,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1537, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 687/6501 [19:44<2:55:46,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:14,865] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 688/6501 [19:46<2:52:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0475, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 688/6501 [19:46<2:52:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:16,676] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 689/6501 [19:47<2:53:19,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9972, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 689/6501 [19:47<2:53:19,  1.79s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 690/6501 [19:49<2:49:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3542, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 690/6501 [19:49<2:49:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 691/6501 [19:51<2:48:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9836, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 691/6501 [19:51<2:48:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 692/6501 [19:52<2:46:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1711, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 692/6501 [19:52<2:46:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:23,428] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 693/6501 [19:54<2:46:20,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.476, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 693/6501 [19:54<2:46:20,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:25,545] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 694/6501 [19:56<2:57:53,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0619, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 694/6501 [19:56<2:57:53,  1.84s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 695/6501 [19:58<2:54:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0687, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 695/6501 [19:58<2:54:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:29,046] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 696/6501 [20:00<2:53:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3459, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 696/6501 [20:00<2:53:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:31,084] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 697/6501 [20:02<3:00:40,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1893, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 697/6501 [20:02<3:00:40,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:32,901] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 698/6501 [20:04<2:59:09,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.248, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 698/6501 [20:04<2:59:09,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:34,663] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 699/6501 [20:05<2:56:30,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8284, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 699/6501 [20:05<2:56:30,  1.83s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 700/6501 [20:07<2:52:20,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3719, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 700/6501 [20:07<2:52:20,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:38,150] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 701/6501 [20:09<2:52:56,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1246, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 701/6501 [20:09<2:52:56,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:39,873] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 702/6501 [20:11<2:50:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8082, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 702/6501 [20:11<2:50:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:41,765] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 703/6501 [20:12<2:54:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4006, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 703/6501 [20:12<2:54:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 704/6501 [20:14<2:52:13,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1821, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 704/6501 [20:14<2:52:13,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:45,160] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 705/6501 [20:16<2:48:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4075, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 705/6501 [20:16<2:48:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 706/6501 [20:17<2:43:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9356, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 706/6501 [20:17<2:43:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 707/6501 [20:19<2:37:34,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0331, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 707/6501 [20:19<2:37:34,  1.63s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 708/6501 [20:20<2:34:39,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2628, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 708/6501 [20:20<2:34:39,  1.60s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 709/6501 [20:22<2:30:29,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.218, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 709/6501 [20:22<2:30:29,  1.56s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 710/6501 [20:23<2:22:25,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9992, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 710/6501 [20:23<2:22:25,  1.48s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:54,374] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 711/6501 [20:25<2:34:27,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1909, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 711/6501 [20:25<2:34:27,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:55,906] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 712/6501 [20:27<2:32:27,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9108, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 712/6501 [20:27<2:32:27,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:57,587] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 713/6501 [20:28<2:35:20,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3443, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 713/6501 [20:28<2:35:20,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:39:59,332] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 714/6501 [20:30<2:39:12,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0661, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 714/6501 [20:30<2:39:12,  1.65s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 715/6501 [20:32<2:46:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6714, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 715/6501 [20:32<2:46:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:03,098] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 716/6501 [20:34<2:50:31,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9146, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 716/6501 [20:34<2:50:31,  1.77s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 717/6501 [20:36<2:57:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2732, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 717/6501 [20:36<2:57:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:06,786] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 718/6501 [20:37<2:52:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9063, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 718/6501 [20:37<2:52:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:08,549] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 719/6501 [20:39<2:51:50,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1424, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 719/6501 [20:39<2:51:50,  1.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 720/6501 [20:41<2:39:28,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1018, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 720/6501 [20:41<2:39:28,  1.66s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 721/6501 [20:42<2:43:43,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2313, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 721/6501 [20:42<2:43:43,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:13,324] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 722/6501 [20:44<2:41:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9042, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 722/6501 [20:44<2:41:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:15,029] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 723/6501 [20:46<2:42:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 723/6501 [20:46<2:42:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8553, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:16,831] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 724/6501 [20:48<2:45:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5107, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 724/6501 [20:48<2:45:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:18,454] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 725/6501 [20:49<2:42:43,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4403, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 725/6501 [20:49<2:42:43,  1.69s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 726/6501 [20:51<2:45:44,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4149, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 726/6501 [20:51<2:45:44,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:22,003] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 727/6501 [20:53<2:46:36,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.604, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 727/6501 [20:53<2:46:36,  1.73s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 728/6501 [20:55<2:53:03,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8167, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 728/6501 [20:55<2:53:03,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:25,646] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 729/6501 [20:56<2:49:48,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7018, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 729/6501 [20:56<2:49:48,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:27,240] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 730/6501 [20:58<2:44:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9935, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 730/6501 [20:58<2:44:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:29,120] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 731/6501 [21:00<2:49:36,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5869, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 731/6501 [21:00<2:49:36,  1.76s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 732/6501 [21:02<2:47:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1722, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 732/6501 [21:02<2:47:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 733/6501 [21:03<2:48:50,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0614, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 733/6501 [21:03<2:48:50,  1.76s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 734/6501 [21:05<2:41:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4938, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 734/6501 [21:05<2:41:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:36,088] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 735/6501 [21:07<2:50:05,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0101, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 735/6501 [21:07<2:50:05,  1.77s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 736/6501 [21:08<2:43:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4782, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 736/6501 [21:08<2:43:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:39,431] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 737/6501 [21:10<2:46:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3549, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 737/6501 [21:10<2:46:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:41,182] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 738/6501 [21:12<2:46:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5095, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 738/6501 [21:12<2:46:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:42,957] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 739/6501 [21:14<2:47:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.037, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 739/6501 [21:14<2:47:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:44,677] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 740/6501 [21:15<2:47:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.191, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 740/6501 [21:15<2:47:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 741/6501 [21:17<2:40:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7657, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 741/6501 [21:17<2:40:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 742/6501 [21:19<2:39:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9935, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 742/6501 [21:19<2:39:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:49,835] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 743/6501 [21:21<2:49:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9345, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 743/6501 [21:21<2:49:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 744/6501 [21:21<2:25:33,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0706, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 744/6501 [21:21<2:25:33,  1.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 745/6501 [21:23<2:31:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1679, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 745/6501 [21:23<2:31:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:54,267] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 746/6501 [21:25<2:36:50,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2442, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 746/6501 [21:25<2:36:50,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:40:55,986] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█▏        | 747/6501 [21:27<2:39:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.138, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 747/6501 [21:27<2:39:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 748/6501 [21:29<2:51:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3077, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 748/6501 [21:29<2:51:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:00,091] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 749/6501 [21:31<2:58:02,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7078, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 749/6501 [21:31<2:58:02,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:01,941] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 750/6501 [21:33<2:57:49,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8028, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 750/6501 [21:33<2:57:49,  1.86s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 751/6501 [21:34<2:33:16,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.225, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 751/6501 [21:34<2:33:16,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:04,758] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 752/6501 [21:35<2:39:24,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3569, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 752/6501 [21:35<2:39:24,  1.66s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 753/6501 [21:37<2:42:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.399, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 753/6501 [21:37<2:42:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:08,362] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 754/6501 [21:39<2:46:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2907, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 754/6501 [21:39<2:46:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:10,118] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 755/6501 [21:41<2:46:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0795, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 755/6501 [21:41<2:46:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 756/6501 [21:43<2:51:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3664, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 756/6501 [21:43<2:51:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:13,721] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 757/6501 [21:44<2:48:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0221, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 757/6501 [21:44<2:48:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 758/6501 [21:46<2:48:39,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2359, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 758/6501 [21:46<2:48:39,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:17,616] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 759/6501 [21:48<2:59:26,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0369, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 759/6501 [21:48<2:59:26,  1.88s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 760/6501 [21:50<2:53:28,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4711, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 760/6501 [21:50<2:53:28,  1.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 761/6501 [21:52<2:50:57,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1762, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 761/6501 [21:52<2:50:57,  1.79s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 762/6501 [21:54<2:51:15,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1844, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 762/6501 [21:54<2:51:15,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:24,692] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 763/6501 [21:55<2:53:52,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3187, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 763/6501 [21:55<2:53:52,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:26,154] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 764/6501 [21:57<2:43:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9129, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 764/6501 [21:57<2:43:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 765/6501 [21:58<2:38:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8841, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 765/6501 [21:58<2:38:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 766/6501 [22:00<2:38:22,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0158, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 766/6501 [22:00<2:38:22,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:31,184] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 767/6501 [22:02<2:43:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6904, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 767/6501 [22:02<2:43:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:33,058] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 768/6501 [22:04<2:48:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3435, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 768/6501 [22:04<2:48:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 769/6501 [22:05<2:46:22,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1167, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 769/6501 [22:05<2:46:22,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:36,525] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 770/6501 [22:07<2:47:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1005, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 770/6501 [22:07<2:47:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 771/6501 [22:09<2:45:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0217, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 771/6501 [22:09<2:45:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:39,947] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 772/6501 [22:11<2:45:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7921, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 772/6501 [22:11<2:45:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:42,169] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 773/6501 [22:13<2:59:20,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3528, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 773/6501 [22:13<2:59:20,  1.88s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 774/6501 [22:14<2:50:34,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2103, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 774/6501 [22:14<2:50:34,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:45,357] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 775/6501 [22:16<2:45:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3297, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 775/6501 [22:16<2:45:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:47,225] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 776/6501 [22:18<2:49:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8404, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 776/6501 [22:18<2:49:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:48,885] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 777/6501 [22:20<2:46:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7617, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 777/6501 [22:20<2:46:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 778/6501 [22:21<2:40:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9465, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 778/6501 [22:21<2:40:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:52,219] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 779/6501 [22:23<2:43:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1561, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 779/6501 [22:23<2:43:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 780/6501 [22:25<2:57:11,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9485, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 780/6501 [22:25<2:57:11,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:56,037] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 781/6501 [22:27<2:50:27,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5271, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 781/6501 [22:27<2:50:27,  1.79s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 782/6501 [22:28<2:46:31,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2238, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 782/6501 [22:28<2:46:31,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:41:59,828] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 783/6501 [22:31<2:57:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9766, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 783/6501 [22:31<2:57:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:01,510] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 784/6501 [22:32<2:52:28,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.712, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 784/6501 [22:32<2:52:28,  1.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 785/6501 [22:34<2:51:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9556, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 785/6501 [22:34<2:51:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:05,235] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 786/6501 [22:36<2:55:37,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2675, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 786/6501 [22:36<2:55:37,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:06,884] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 787/6501 [22:38<2:50:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1731, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 787/6501 [22:38<2:50:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:08,714] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 788/6501 [22:39<2:51:16,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1487, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 788/6501 [22:39<2:51:16,  1.80s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 789/6501 [22:41<2:47:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1109, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 789/6501 [22:41<2:47:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:12,217] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 790/6501 [22:43<2:49:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2315, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 790/6501 [22:43<2:49:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:13,969] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 791/6501 [22:45<2:48:42,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0953, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 791/6501 [22:45<2:48:42,  1.77s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 792/6501 [22:46<2:47:19,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1036, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 792/6501 [22:46<2:47:19,  1.76s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 793/6501 [22:48<2:51:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1324, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 793/6501 [22:48<2:51:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 794/6501 [22:49<2:26:24,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9913, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 794/6501 [22:49<2:26:24,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:20,188] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 795/6501 [22:51<2:29:57,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1223, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 795/6501 [22:51<2:29:57,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:21,674] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 796/6501 [22:52<2:27:20,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0758, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 796/6501 [22:52<2:27:20,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:23,126] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 797/6501 [22:54<2:24:32,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3498, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 797/6501 [22:54<2:24:32,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:24,859] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 798/6501 [22:56<2:30:34,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1773, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 798/6501 [22:56<2:30:34,  1.58s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 799/6501 [22:57<2:29:34,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0966, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 799/6501 [22:57<2:29:34,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:28,131] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 800/6501 [22:59<2:33:46,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3693, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 800/6501 [22:59<2:33:46,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:29,765] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 801/6501 [23:00<2:34:10,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2555, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 801/6501 [23:00<2:34:10,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:31,341] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 802/6501 [23:02<2:32:48,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2185, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 802/6501 [23:02<2:32:48,  1.61s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 803/6501 [23:04<2:37:34,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0578, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 803/6501 [23:04<2:37:34,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:34,783] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 804/6501 [23:05<2:37:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2781, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 804/6501 [23:05<2:37:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 805/6501 [23:07<2:36:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9883, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 805/6501 [23:07<2:36:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:38,212] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 806/6501 [23:09<2:40:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1281, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 806/6501 [23:09<2:40:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:39,743] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 807/6501 [23:10<2:36:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2551, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 807/6501 [23:10<2:36:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:41,105] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 808/6501 [23:12<2:28:05,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0576, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 808/6501 [23:12<2:28:05,  1.56s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 809/6501 [23:14<2:37:22,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9557, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 809/6501 [23:14<2:37:22,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:44,489] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 810/6501 [23:15<2:32:44,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2327, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 810/6501 [23:15<2:32:44,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:45,792] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 811/6501 [23:16<2:23:57,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2953, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 811/6501 [23:16<2:23:57,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:47,495] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 812/6501 [23:18<2:29:12,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3168, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 812/6501 [23:18<2:29:12,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:49,150] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 813/6501 [23:20<2:31:27,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8911, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 813/6501 [23:20<2:31:27,  1.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 814/6501 [23:22<2:41:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.173, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 814/6501 [23:22<2:41:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:52,742] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 815/6501 [23:23<2:39:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0847, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 815/6501 [23:23<2:39:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:54,526] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 816/6501 [23:25<2:42:26,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9505, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 816/6501 [23:25<2:42:26,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:56,308] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 817/6501 [23:27<2:44:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5991, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 817/6501 [23:27<2:44:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 818/6501 [23:29<2:41:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5717, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 818/6501 [23:29<2:41:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:42:59,682] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 819/6501 [23:30<2:42:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1142, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 819/6501 [23:30<2:42:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:01,729] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 820/6501 [23:32<2:51:45,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4049, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 820/6501 [23:32<2:51:45,  1.81s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 821/6501 [23:34<2:53:31,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1798, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 821/6501 [23:34<2:53:31,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:05,405] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 822/6501 [23:36<2:52:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0524, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 822/6501 [23:36<2:52:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:07,126] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 823/6501 [23:38<2:49:35,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0066, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 823/6501 [23:38<2:49:35,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:09,066] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 824/6501 [23:40<2:53:45,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9988, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 824/6501 [23:40<2:53:45,  1.84s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 825/6501 [23:41<2:41:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1548, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 825/6501 [23:41<2:41:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:12,154] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 826/6501 [23:43<2:40:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.299, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 826/6501 [23:43<2:40:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 827/6501 [23:44<2:36:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0249, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 827/6501 [23:44<2:36:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 828/6501 [23:46<2:37:07,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.447, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 828/6501 [23:46<2:37:07,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:17,154] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 829/6501 [23:48<2:40:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5596, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 829/6501 [23:48<2:40:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:18,789] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 830/6501 [23:49<2:38:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4262, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 830/6501 [23:49<2:38:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:20,804] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 831/6501 [23:52<2:48:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.425, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 831/6501 [23:52<2:48:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 832/6501 [23:53<2:44:43,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1967, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 832/6501 [23:53<2:44:43,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:24,250] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 833/6501 [23:55<2:45:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.016, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 833/6501 [23:55<2:45:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 834/6501 [23:56<2:35:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 834/6501 [23:56<2:35:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6696, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:27,689] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 835/6501 [23:58<2:46:51,  1.77s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 835/6501 [23:58<2:46:51,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3401, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 836/6501 [24:00<2:40:36,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4053, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 836/6501 [24:00<2:40:36,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:31,042] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 837/6501 [24:02<2:43:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2576, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 837/6501 [24:02<2:43:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 838/6501 [24:03<2:43:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8028, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 838/6501 [24:03<2:43:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:34,156] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 839/6501 [24:05<2:33:36,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9639, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 839/6501 [24:05<2:33:36,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:35,881] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 840/6501 [24:07<2:36:18,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3447, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 840/6501 [24:07<2:36:18,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:37,494] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 841/6501 [24:08<2:35:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2727, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 841/6501 [24:08<2:35:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:39,190] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 842/6501 [24:10<2:36:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9775, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 842/6501 [24:10<2:36:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 843/6501 [24:12<2:43:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3441, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 843/6501 [24:12<2:43:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:42,657] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 844/6501 [24:13<2:38:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0413, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 844/6501 [24:13<2:38:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 845/6501 [24:15<2:37:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6337, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 845/6501 [24:15<2:37:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:46,205] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 846/6501 [24:17<2:44:05,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6322, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 846/6501 [24:17<2:44:05,  1.74s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 847/6501 [24:19<2:49:34,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3474, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 847/6501 [24:19<2:49:34,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:50,176] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 848/6501 [24:21<2:56:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2253, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 848/6501 [24:21<2:56:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:51,884] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 849/6501 [24:23<2:51:34,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.102, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 849/6501 [24:23<2:51:34,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:53,527] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 850/6501 [24:24<2:46:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.965, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 850/6501 [24:24<2:46:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:55,101] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 851/6501 [24:26<2:40:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2794, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 851/6501 [24:26<2:40:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 852/6501 [24:27<2:38:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3512, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 852/6501 [24:27<2:38:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:43:58,635] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 853/6501 [24:29<2:45:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.067, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 853/6501 [24:29<2:45:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:00,255] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 854/6501 [24:31<2:41:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2399, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 854/6501 [24:31<2:41:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:02,169] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 855/6501 [24:33<2:46:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1338, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 855/6501 [24:33<2:46:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 856/6501 [24:34<2:41:31,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4665, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 856/6501 [24:34<2:41:31,  1.72s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 857/6501 [24:36<2:39:19,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8175, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 857/6501 [24:36<2:39:19,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:07,123] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 858/6501 [24:38<2:40:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1381, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 858/6501 [24:38<2:40:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:08,846] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 859/6501 [24:40<2:40:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1757, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 859/6501 [24:40<2:40:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 860/6501 [24:41<2:46:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1303, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 860/6501 [24:41<2:46:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:12,625] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 861/6501 [24:43<2:49:13,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0335, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 861/6501 [24:43<2:49:13,  1.80s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 862/6501 [24:45<2:39:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2532, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 862/6501 [24:45<2:39:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:15,713] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 863/6501 [24:46<2:37:44,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1722, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 863/6501 [24:46<2:37:44,  1.68s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 864/6501 [24:47<2:19:22,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5565, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 864/6501 [24:47<2:19:22,  1.48s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:18,515] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 865/6501 [24:49<2:27:32,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1822, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 865/6501 [24:49<2:27:32,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:20,173] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 866/6501 [24:51<2:29:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2749, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 866/6501 [24:51<2:29:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:21,801] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 867/6501 [24:52<2:30:49,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3786, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 867/6501 [24:52<2:30:49,  1.61s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 868/6501 [24:54<2:28:43,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1404, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 868/6501 [24:54<2:28:43,  1.58s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 869/6501 [24:56<2:39:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.259, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 869/6501 [24:56<2:39:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:26,976] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 870/6501 [24:58<2:38:53,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4998, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 870/6501 [24:58<2:38:53,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:28,858] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 871/6501 [25:00<2:44:11,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3468, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 871/6501 [25:00<2:44:11,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:30,574] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 872/6501 [25:01<2:43:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2374, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 872/6501 [25:01<2:43:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:32,573] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 873/6501 [25:03<2:50:27,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0974, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 873/6501 [25:03<2:50:27,  1.82s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 874/6501 [25:04<2:28:11,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3364, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 874/6501 [25:04<2:28:11,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:35,395] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 875/6501 [25:06<2:34:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1046, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 875/6501 [25:06<2:34:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 876/6501 [25:08<2:33:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4435, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 876/6501 [25:08<2:33:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:38,617] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 877/6501 [25:09<2:32:32,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7577, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 877/6501 [25:09<2:32:32,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:40,466] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 878/6501 [25:11<2:38:44,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0364, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 878/6501 [25:11<2:38:44,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:42,496] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 879/6501 [25:13<2:48:11,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3292, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 879/6501 [25:13<2:48:11,  1.80s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 880/6501 [25:15<2:48:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2964, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 880/6501 [25:15<2:48:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:46,184] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 881/6501 [25:17<2:50:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0491, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 881/6501 [25:17<2:50:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 882/6501 [25:19<2:55:19,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3625, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 882/6501 [25:19<2:55:19,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:49,581] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 883/6501 [25:20<2:42:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1786, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 883/6501 [25:20<2:42:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:51,595] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 884/6501 [25:22<2:50:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 884/6501 [25:22<2:50:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4493, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 885/6501 [25:24<2:42:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5316, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 885/6501 [25:24<2:42:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:54,658] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 886/6501 [25:25<2:36:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1171, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 886/6501 [25:25<2:36:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:56,340] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 887/6501 [25:27<2:36:43,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4244, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 887/6501 [25:27<2:36:43,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:58,009] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 888/6501 [25:29<2:36:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4398, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 888/6501 [25:29<2:36:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:44:59,981] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 889/6501 [25:31<2:44:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0951, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 889/6501 [25:31<2:44:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 890/6501 [25:32<2:38:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.431, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 890/6501 [25:32<2:38:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:03,305] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 891/6501 [25:34<2:40:57,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2575, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 891/6501 [25:34<2:40:57,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:05,279] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 892/6501 [25:36<2:48:01,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0994, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 892/6501 [25:36<2:48:01,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:07,141] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▎        | 893/6501 [25:38<2:49:48,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9144, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 893/6501 [25:38<2:49:48,  1.82s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 894/6501 [25:40<2:50:11,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0845, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 894/6501 [25:40<2:50:11,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:10,664] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 895/6501 [25:41<2:46:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.337, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 895/6501 [25:41<2:46:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 896/6501 [25:43<2:43:08,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0245, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 896/6501 [25:43<2:43:08,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:14,182] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 897/6501 [25:45<2:46:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0248, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 897/6501 [25:45<2:46:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:15,894] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 898/6501 [25:47<2:44:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2798, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 898/6501 [25:47<2:44:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 899/6501 [25:48<2:41:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8986, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 899/6501 [25:48<2:41:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:19,469] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 900/6501 [25:50<2:46:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1107, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 900/6501 [25:50<2:46:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:21,313] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 901/6501 [25:52<2:48:15,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9374, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 901/6501 [25:52<2:48:15,  1.80s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 902/6501 [25:54<2:45:12,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8832, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 902/6501 [25:54<2:45:12,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:24,896] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 903/6501 [25:56<2:48:27,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0739, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 903/6501 [25:56<2:48:27,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:26,482] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 904/6501 [25:57<2:42:17,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1965, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 904/6501 [25:57<2:42:17,  1.74s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 905/6501 [25:59<2:45:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2545, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 905/6501 [25:59<2:45:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:30,114] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 906/6501 [26:01<2:45:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1349, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 906/6501 [26:01<2:45:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:32,266] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 907/6501 [26:03<2:56:03,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9763, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 907/6501 [26:03<2:56:03,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:33,958] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 908/6501 [26:05<2:50:33,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3281, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 908/6501 [26:05<2:50:33,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:35,746] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 909/6501 [26:06<2:49:20,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0309, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 909/6501 [26:06<2:49:20,  1.82s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 910/6501 [26:08<2:45:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.47, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 910/6501 [26:08<2:45:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 911/6501 [26:10<2:45:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1307, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 911/6501 [26:10<2:45:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 912/6501 [26:12<2:43:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1616, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 912/6501 [26:12<2:43:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:42,581] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 913/6501 [26:13<2:41:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5035, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 913/6501 [26:13<2:41:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 914/6501 [26:15<2:45:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2392, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 914/6501 [26:15<2:45:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:46,766] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 915/6501 [26:17<3:00:07,  1.93s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 915/6501 [26:17<3:00:07,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0639, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:48,390] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 916/6501 [26:19<2:51:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4069, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 916/6501 [26:19<2:51:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:50,422] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 917/6501 [26:21<2:56:43,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2286, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 917/6501 [26:21<2:56:43,  1.90s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 918/6501 [26:23<2:51:58,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0452, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 918/6501 [26:23<2:51:58,  1.85s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 919/6501 [26:24<2:36:51,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2032, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 919/6501 [26:24<2:36:51,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:55,223] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 920/6501 [26:26<2:38:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2101, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 920/6501 [26:26<2:38:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 921/6501 [26:28<2:46:18,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3683, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 921/6501 [26:28<2:46:18,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:45:59,101] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 922/6501 [26:30<2:49:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3434, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 922/6501 [26:30<2:49:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:00,686] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 923/6501 [26:31<2:42:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0216, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 923/6501 [26:31<2:42:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:02,600] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 924/6501 [26:33<2:47:21,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0837, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 924/6501 [26:33<2:47:21,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:04,346] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 925/6501 [26:35<2:45:48,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1292, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 925/6501 [26:35<2:45:48,  1.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 926/6501 [26:37<2:40:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.397, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 926/6501 [26:37<2:40:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 927/6501 [26:38<2:22:06,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3122, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 927/6501 [26:38<2:22:06,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:08,727] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 928/6501 [26:39<2:27:13,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2772, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 928/6501 [26:39<2:27:13,  1.59s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 929/6501 [26:41<2:31:04,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9629, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 929/6501 [26:41<2:31:04,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:12,589] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 930/6501 [26:43<2:45:17,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7694, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 930/6501 [26:43<2:45:17,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:14,154] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 931/6501 [26:45<2:39:16,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0312, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 931/6501 [26:45<2:39:16,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:15,765] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 932/6501 [26:46<2:36:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3005, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 932/6501 [26:46<2:36:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:17,541] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 933/6501 [26:48<2:38:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1664, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 933/6501 [26:48<2:38:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:19,189] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 934/6501 [26:50<2:37:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0251, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 934/6501 [26:50<2:37:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:20,600] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 935/6501 [26:51<2:29:09,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4019, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 935/6501 [26:51<2:29:09,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:22,584] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 936/6501 [26:53<2:39:36,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8874, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 936/6501 [26:53<2:39:36,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:24,394] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 937/6501 [26:55<2:42:03,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0889, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 937/6501 [26:55<2:42:03,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:26,109] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 938/6501 [26:57<2:41:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2456, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 938/6501 [26:57<2:41:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 939/6501 [26:59<2:41:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.747, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 939/6501 [26:59<2:41:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 940/6501 [27:00<2:44:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8342, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 940/6501 [27:00<2:44:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:31,393] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 941/6501 [27:02<2:41:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3937, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 941/6501 [27:02<2:41:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:33,094] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 942/6501 [27:04<2:40:32,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2421, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 942/6501 [27:04<2:40:32,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:34,686] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 943/6501 [27:05<2:36:36,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1181, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 943/6501 [27:05<2:36:36,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:36,181] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 944/6501 [27:07<2:31:08,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2903, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 944/6501 [27:07<2:31:08,  1.63s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 945/6501 [27:09<2:36:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4007, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 945/6501 [27:09<2:36:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:40,005] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 946/6501 [27:11<2:45:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0602, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 946/6501 [27:11<2:45:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 947/6501 [27:13<2:49:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3268, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 947/6501 [27:13<2:49:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:43,441] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 948/6501 [27:14<2:40:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3458, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 948/6501 [27:14<2:40:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:45,206] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 949/6501 [27:16<2:41:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2298, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 949/6501 [27:16<2:41:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:46,878] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 950/6501 [27:18<2:39:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9086, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 950/6501 [27:18<2:39:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:48,812] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 951/6501 [27:20<2:45:02,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2068, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 951/6501 [27:20<2:45:02,  1.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 952/6501 [27:21<2:41:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2542, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 952/6501 [27:21<2:41:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:52,674] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 953/6501 [27:23<2:54:08,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6062, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 953/6501 [27:23<2:54:08,  1.88s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 954/6501 [27:25<2:54:19,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4498, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 954/6501 [27:25<2:54:19,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:46:56,298] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 955/6501 [27:27<2:50:02,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4411, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 955/6501 [27:27<2:50:02,  1.84s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 956/6501 [27:29<2:54:45,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1561, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 956/6501 [27:29<2:54:45,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:00,090] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 957/6501 [27:31<2:51:41,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0142, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 957/6501 [27:31<2:51:41,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:01,568] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 958/6501 [27:32<2:41:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5715, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 958/6501 [27:32<2:41:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:03,533] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 959/6501 [27:34<2:47:14,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1785, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 959/6501 [27:34<2:47:14,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:05,423] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 960/6501 [27:36<2:49:23,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0801, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 960/6501 [27:36<2:49:23,  1.83s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 961/6501 [27:37<2:26:30,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4965, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 961/6501 [27:37<2:26:30,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:07,994] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 962/6501 [27:39<2:25:48,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3354, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 962/6501 [27:39<2:25:48,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:09,873] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 963/6501 [27:41<2:34:04,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.265, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 963/6501 [27:41<2:34:04,  1.67s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 964/6501 [27:42<2:37:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1086, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 964/6501 [27:42<2:37:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:13,391] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 965/6501 [27:44<2:38:02,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0212, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 965/6501 [27:44<2:38:02,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:15,060] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 966/6501 [27:46<2:36:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1765, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 966/6501 [27:46<2:36:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 967/6501 [27:47<2:37:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1245, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 967/6501 [27:47<2:37:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:18,447] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 968/6501 [27:49<2:36:14,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1234, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 968/6501 [27:49<2:36:14,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:20,421] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 969/6501 [27:51<2:43:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.588, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 969/6501 [27:51<2:43:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 970/6501 [27:53<2:41:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2105, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 970/6501 [27:53<2:41:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:23,994] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 971/6501 [27:55<2:45:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 971/6501 [27:55<2:45:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2261, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:25,805] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 972/6501 [27:57<2:45:33,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5257, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 972/6501 [27:57<2:45:33,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:27,449] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 973/6501 [27:58<2:41:19,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2847, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 973/6501 [27:58<2:41:19,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:29,533] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 974/6501 [28:00<2:50:30,  1.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 974/6501 [28:00<2:50:30,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3977, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:31,030] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 975/6501 [28:02<2:40:41,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.821, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 975/6501 [28:02<2:40:41,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:33,042] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 976/6501 [28:04<2:48:01,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2158, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 976/6501 [28:04<2:48:01,  1.82s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 977/6501 [28:05<2:39:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1866, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 977/6501 [28:05<2:39:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:35,973] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 978/6501 [28:07<2:30:36,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3638, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 978/6501 [28:07<2:30:36,  1.64s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 979/6501 [28:09<2:40:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3241, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 979/6501 [28:09<2:40:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:39,829] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 980/6501 [28:11<2:43:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2028, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 980/6501 [28:11<2:43:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:41,720] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 981/6501 [28:12<2:46:46,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3461, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 981/6501 [28:12<2:46:46,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:43,230] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 982/6501 [28:14<2:38:22,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5765, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 982/6501 [28:14<2:38:22,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:45,065] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 983/6501 [28:16<2:41:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0856, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 983/6501 [28:16<2:41:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 984/6501 [28:18<2:43:55,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1872, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 984/6501 [28:18<2:43:55,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:48,793] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 985/6501 [28:19<2:46:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3675, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 985/6501 [28:19<2:46:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:50,372] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 986/6501 [28:21<2:40:10,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9315, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 986/6501 [28:21<2:40:10,  1.74s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 987/6501 [28:23<2:37:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3565, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 987/6501 [28:23<2:37:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 988/6501 [28:24<2:38:04,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2802, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 988/6501 [28:24<2:38:04,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:55,540] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 989/6501 [28:26<2:39:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3372, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 989/6501 [28:26<2:39:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 990/6501 [28:28<2:37:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.212, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 990/6501 [28:28<2:37:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:47:59,201] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 991/6501 [28:30<2:45:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2225, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 991/6501 [28:30<2:45:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 992/6501 [28:32<2:47:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2752, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 992/6501 [28:32<2:47:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:02,841] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 993/6501 [28:34<2:45:39,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0078, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 993/6501 [28:34<2:45:39,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:04,629] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 994/6501 [28:35<2:45:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1369, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 994/6501 [28:35<2:45:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 995/6501 [28:37<2:41:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3306, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 995/6501 [28:37<2:41:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:08,069] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 996/6501 [28:39<2:41:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1369, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 996/6501 [28:39<2:41:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:09,800] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 997/6501 [28:40<2:40:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7937, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 997/6501 [28:40<2:40:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 998/6501 [28:42<2:38:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9538, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 998/6501 [28:42<2:38:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:13,175] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 999/6501 [28:44<2:37:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.04, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 999/6501 [28:44<2:37:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:14,860] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1000/6501 [28:46<2:36:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3698, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1000/6501 [28:46<2:36:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:16,417] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1001/6501 [28:47<2:32:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.24, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1001/6501 [28:47<2:32:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:17,932] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1002/6501 [28:49<2:28:25,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0748, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1002/6501 [28:49<2:28:25,  1.62s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1003/6501 [28:50<2:32:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2354, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1003/6501 [28:50<2:32:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:21,420] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1004/6501 [28:52<2:34:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2095, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1004/6501 [28:52<2:34:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1005/6501 [28:54<2:29:03,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2137, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1005/6501 [28:54<2:29:03,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:24,382] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1006/6501 [28:55<2:24:25,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9933, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1006/6501 [28:55<2:24:25,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:26,404] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1007/6501 [28:57<2:36:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4867, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 1007/6501 [28:57<2:36:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:27,994] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1008/6501 [28:59<2:33:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.634, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1008/6501 [28:59<2:33:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:29,835] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1009/6501 [29:01<2:37:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7339, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1009/6501 [29:01<2:37:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:31,620] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1010/6501 [29:02<2:39:28,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0413, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1010/6501 [29:02<2:39:28,  1.74s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1011/6501 [29:04<2:44:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0023, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1011/6501 [29:04<2:44:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:35,382] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1012/6501 [29:06<2:45:21,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2216, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1012/6501 [29:06<2:45:21,  1.81s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1013/6501 [29:07<2:22:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3912, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1013/6501 [29:07<2:22:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:38,355] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1014/6501 [29:09<2:34:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.325, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1014/6501 [29:09<2:34:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:39,966] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1015/6501 [29:11<2:32:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2569, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1015/6501 [29:11<2:32:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:41,878] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1016/6501 [29:13<2:39:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1908, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1016/6501 [29:13<2:39:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:43,289] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1017/6501 [29:14<2:30:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.041, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1017/6501 [29:14<2:30:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:45,240] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1018/6501 [29:16<2:38:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4102, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1018/6501 [29:16<2:38:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1019/6501 [29:18<2:38:27,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2404, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1019/6501 [29:18<2:38:27,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:48,968] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1020/6501 [29:20<2:45:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1064, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1020/6501 [29:20<2:45:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:50,373] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1021/6501 [29:21<2:34:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2369, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1021/6501 [29:21<2:34:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:52,081] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1022/6501 [29:23<2:34:49,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1022/6501 [29:23<2:34:49,  1.70s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1023/6501 [29:25<2:36:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1793, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1023/6501 [29:25<2:36:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:55,866] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1024/6501 [29:27<2:44:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1024/6501 [29:27<2:44:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3779, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1025/6501 [29:28<2:38:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2132, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1025/6501 [29:28<2:38:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:48:59,005] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1026/6501 [29:30<2:33:52,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3442, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1026/6501 [29:30<2:33:52,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:00,581] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1027/6501 [29:31<2:30:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3288, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1027/6501 [29:31<2:30:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:02,279] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1028/6501 [29:33<2:32:01,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3264, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1028/6501 [29:33<2:32:01,  1.67s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1029/6501 [29:35<2:33:35,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0596, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1029/6501 [29:35<2:33:35,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:05,758] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1030/6501 [29:36<2:35:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4216, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1030/6501 [29:36<2:35:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1031/6501 [29:38<2:34:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2697, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1031/6501 [29:38<2:34:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:09,268] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1032/6501 [29:40<2:38:23,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9369, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1032/6501 [29:40<2:38:23,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:10,989] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1033/6501 [29:42<2:37:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5784, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1033/6501 [29:42<2:37:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:12,691] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1034/6501 [29:43<2:37:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.416, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1034/6501 [29:43<2:37:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1035/6501 [29:45<2:24:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1302, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1035/6501 [29:45<2:24:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:15,632] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1036/6501 [29:46<2:26:56,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0722, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1036/6501 [29:46<2:26:56,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:17,234] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1037/6501 [29:48<2:26:35,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4841, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1037/6501 [29:48<2:26:35,  1.61s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1038/6501 [29:50<2:36:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5082, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1038/6501 [29:50<2:36:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1039/6501 [29:51<2:23:53,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7879, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1039/6501 [29:51<2:23:53,  1.58s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1040/6501 [29:53<2:27:13,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7406, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1040/6501 [29:53<2:27:13,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:23,941] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1041/6501 [29:55<2:31:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3502, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1041/6501 [29:55<2:31:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:25,622] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1042/6501 [29:56<2:31:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0903, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1042/6501 [29:56<2:31:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:27,665] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1043/6501 [29:58<2:42:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3562, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1043/6501 [29:58<2:42:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1044/6501 [30:00<2:42:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1592, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1044/6501 [30:00<2:42:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:31,379] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1045/6501 [30:02<2:46:02,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4539, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1045/6501 [30:02<2:46:02,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:33,473] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1046/6501 [30:04<2:53:19,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2694, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1046/6501 [30:04<2:53:19,  1.91s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1047/6501 [30:06<2:43:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1125, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1047/6501 [30:06<2:43:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:36,599] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1048/6501 [30:07<2:37:36,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1225, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1048/6501 [30:07<2:37:36,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:38,402] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1049/6501 [30:09<2:39:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1049/6501 [30:09<2:39:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1076, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1050/6501 [30:10<2:20:25,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2305, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1050/6501 [30:10<2:20:25,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:41,130] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1051/6501 [30:12<2:23:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1728, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1051/6501 [30:12<2:23:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:42,879] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1052/6501 [30:14<2:28:17,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2547, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1052/6501 [30:14<2:28:17,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:44,523] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1053/6501 [30:15<2:28:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.463, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1053/6501 [30:15<2:28:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1054/6501 [30:17<2:35:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9991, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1054/6501 [30:17<2:35:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1055/6501 [30:19<2:40:38,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4091, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1055/6501 [30:19<2:40:38,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:50,204] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1056/6501 [30:21<2:43:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.123, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1056/6501 [30:21<2:43:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1057/6501 [30:23<2:42:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.168, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1057/6501 [30:23<2:42:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:53,497] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1058/6501 [30:24<2:35:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3053, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1058/6501 [30:24<2:35:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:55,075] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1059/6501 [30:26<2:31:40,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.248, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1059/6501 [30:26<2:31:40,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:49:56,912] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1060/6501 [30:28<2:36:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5482, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1060/6501 [30:28<2:36:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1061/6501 [30:29<2:36:28,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.181, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1061/6501 [30:29<2:36:28,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:00,342] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1062/6501 [30:31<2:35:35,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1728, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1062/6501 [30:31<2:35:35,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1063/6501 [30:32<2:22:11,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4615, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1063/6501 [30:32<2:22:11,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:03,277] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1064/6501 [30:34<2:26:01,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9875, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1064/6501 [30:34<2:26:01,  1.61s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1065/6501 [30:36<2:31:48,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5823, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1065/6501 [30:36<2:31:48,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:06,834] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1066/6501 [30:38<2:33:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8239, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1066/6501 [30:38<2:33:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:08,126] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1067/6501 [30:39<2:22:22,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4528, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1067/6501 [30:39<2:22:22,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:09,937] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1068/6501 [30:41<2:28:51,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1081, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1068/6501 [30:41<2:28:51,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:11,553] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1069/6501 [30:42<2:28:04,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1741, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1069/6501 [30:42<2:28:04,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:13,164] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1070/6501 [30:44<2:27:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1551, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1070/6501 [30:44<2:27:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1071/6501 [30:46<2:29:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9982, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1071/6501 [30:46<2:29:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:16,489] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1072/6501 [30:47<2:28:35,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4186, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 1072/6501 [30:47<2:28:35,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:18,543] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1073/6501 [30:49<2:39:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3858, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1073/6501 [30:49<2:39:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1074/6501 [30:51<2:47:50,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2349, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1074/6501 [30:51<2:47:50,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:22,368] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1075/6501 [30:53<2:45:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3017, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1075/6501 [30:53<2:45:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:24,220] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1076/6501 [30:55<2:45:50,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8108, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1076/6501 [30:55<2:45:50,  1.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1077/6501 [30:57<2:50:32,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9887, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1077/6501 [30:57<2:50:32,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:27,934] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1078/6501 [30:59<2:45:35,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0499, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1078/6501 [30:59<2:45:35,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:29,163] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1079/6501 [31:00<2:29:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0836, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1079/6501 [31:00<2:29:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1080/6501 [31:02<2:34:03,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0084, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1080/6501 [31:02<2:34:03,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:32,810] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1081/6501 [31:04<2:37:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0244, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1081/6501 [31:04<2:37:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:34,470] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1082/6501 [31:05<2:34:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4379, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1082/6501 [31:05<2:34:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:36,054] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1083/6501 [31:07<2:31:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7807, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1083/6501 [31:07<2:31:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:37,934] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1084/6501 [31:09<2:36:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1846, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1084/6501 [31:09<2:36:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:39,701] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1085/6501 [31:10<2:37:36,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2798, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1085/6501 [31:10<2:37:36,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:41,653] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1086/6501 [31:12<2:43:08,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1686, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1086/6501 [31:12<2:43:08,  1.81s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1087/6501 [31:14<2:35:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2135, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1087/6501 [31:14<2:35:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:45,046] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1088/6501 [31:16<2:39:14,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1248, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1088/6501 [31:16<2:39:14,  1.77s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1089/6501 [31:18<2:47:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2312, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1089/6501 [31:18<2:47:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:49,120] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1090/6501 [31:20<2:51:33,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2204, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1090/6501 [31:20<2:51:33,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:50,817] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1091/6501 [31:22<2:45:57,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5018, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1091/6501 [31:22<2:45:57,  1.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1092/6501 [31:23<2:45:47,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1693, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1092/6501 [31:23<2:45:47,  1.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1093/6501 [31:25<2:39:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1722, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1093/6501 [31:25<2:39:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1094/6501 [31:27<2:38:43,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2831, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1094/6501 [31:27<2:38:43,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:50:58,007] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1095/6501 [31:29<2:45:10,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2894, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1095/6501 [31:29<2:45:10,  1.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1096/6501 [31:30<2:40:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1993, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1096/6501 [31:30<2:40:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1097/6501 [31:32<2:37:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2968, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1097/6501 [31:32<2:37:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:02,996] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1098/6501 [31:34<2:34:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0995, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1098/6501 [31:34<2:34:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:04,718] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1099/6501 [31:35<2:34:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9947, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1099/6501 [31:35<2:34:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:06,830] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1100/6501 [31:38<2:45:22,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0449, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1100/6501 [31:38<2:45:22,  1.84s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1101/6501 [31:39<2:39:20,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9214, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1101/6501 [31:39<2:39:20,  1.77s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1102/6501 [31:40<2:20:17,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1172, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1102/6501 [31:40<2:20:17,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:11,518] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1103/6501 [31:42<2:32:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9503, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1103/6501 [31:42<2:32:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1104/6501 [31:44<2:33:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1808, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1104/6501 [31:44<2:33:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:15,032] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1105/6501 [31:46<2:35:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0305, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1105/6501 [31:46<2:35:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1106/6501 [31:47<2:32:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1185, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1106/6501 [31:47<2:32:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:18,039] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1107/6501 [31:49<2:24:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1666, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1107/6501 [31:49<2:24:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:20,141] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1108/6501 [31:51<2:37:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4555, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1108/6501 [31:51<2:37:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1109/6501 [31:52<2:25:30,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.151, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1109/6501 [31:52<2:25:30,  1.62s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1110/6501 [31:54<2:27:49,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9329, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1110/6501 [31:54<2:27:49,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:25,222] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1111/6501 [31:56<2:39:07,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9106, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1111/6501 [31:56<2:39:07,  1.77s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1112/6501 [31:57<2:33:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3231, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1112/6501 [31:57<2:33:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1113/6501 [31:59<2:39:53,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8474, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1113/6501 [31:59<2:39:53,  1.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1114/6501 [32:01<2:41:13,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4077, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1114/6501 [32:01<2:41:13,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:32,510] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1115/6501 [32:03<2:45:12,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8717, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1115/6501 [32:03<2:45:12,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:34,150] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1116/6501 [32:05<2:39:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1144, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1116/6501 [32:05<2:39:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1117/6501 [32:07<2:37:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3498, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1117/6501 [32:07<2:37:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:37,419] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1118/6501 [32:08<2:32:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8906, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1118/6501 [32:08<2:32:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:39,473] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1119/6501 [32:10<2:42:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0886, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1119/6501 [32:10<2:42:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1120/6501 [32:12<2:40:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9623, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1120/6501 [32:12<2:40:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1121/6501 [32:13<2:20:21,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2891, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1121/6501 [32:13<2:20:21,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:43,930] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1122/6501 [32:15<2:22:58,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2087, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1122/6501 [32:15<2:22:58,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:45,802] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1123/6501 [32:16<2:30:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5868, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1123/6501 [32:16<2:30:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:47,633] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1124/6501 [32:18<2:34:29,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2527, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1124/6501 [32:18<2:34:29,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:49,550] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1125/6501 [32:20<2:39:39,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1098, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1125/6501 [32:20<2:39:39,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:51,496] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1126/6501 [32:22<2:44:02,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9114, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1126/6501 [32:22<2:44:02,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:53,376] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1127/6501 [32:24<2:45:19,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1345, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1127/6501 [32:24<2:45:19,  1.85s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1128/6501 [32:26<2:40:31,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0692, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1128/6501 [32:26<2:40:31,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:56,555] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1129/6501 [32:27<2:32:55,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4562, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1129/6501 [32:27<2:32:55,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:51:58,817] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1130/6501 [32:30<2:47:45,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7072, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1130/6501 [32:30<2:47:45,  1.87s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1131/6501 [32:31<2:47:47,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9021, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1131/6501 [32:31<2:47:47,  1.87s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1132/6501 [32:33<2:27:48,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6128, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1132/6501 [32:33<2:27:48,  1.65s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1133/6501 [32:34<2:30:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0144, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1133/6501 [32:34<2:30:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:05,412] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1134/6501 [32:36<2:34:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8416, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1134/6501 [32:36<2:34:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:06,537] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1135/6501 [32:37<2:18:24,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0288, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1135/6501 [32:37<2:18:24,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:08,471] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1136/6501 [32:39<2:28:45,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9332, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1136/6501 [32:39<2:28:45,  1.66s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1137/6501 [32:41<2:29:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1793, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 1137/6501 [32:41<2:29:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:11,802] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1138/6501 [32:42<2:28:28,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2684, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1138/6501 [32:42<2:28:28,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:13,449] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1139/6501 [32:44<2:28:04,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3078, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1139/6501 [32:44<2:28:04,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:15,344] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1140/6501 [32:46<2:34:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8754, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1140/6501 [32:46<2:34:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:17,174] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1141/6501 [32:48<2:37:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0166, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1141/6501 [32:48<2:37:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:18,960] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1142/6501 [32:50<2:37:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4166, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1142/6501 [32:50<2:37:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1143/6501 [32:52<2:40:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4823, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1143/6501 [32:52<2:40:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:22,652] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1144/6501 [32:53<2:41:04,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1514, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1144/6501 [32:53<2:41:04,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:24,738] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1145/6501 [32:55<2:48:34,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0928, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1145/6501 [32:55<2:48:34,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:26,436] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1146/6501 [32:57<2:43:28,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.282, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1146/6501 [32:57<2:43:28,  1.83s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1147/6501 [32:58<2:24:39,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1099, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1147/6501 [32:58<2:24:39,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:29,672] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1148/6501 [33:00<2:37:35,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1963, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1148/6501 [33:00<2:37:35,  1.77s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1149/6501 [33:02<2:31:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2036, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1149/6501 [33:02<2:31:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:33,223] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1150/6501 [33:04<2:39:59,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0777, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1150/6501 [33:04<2:39:59,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:35,092] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1151/6501 [33:06<2:41:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2878, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1151/6501 [33:06<2:41:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1152/6501 [33:08<2:41:38,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0679, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1152/6501 [33:08<2:41:38,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:38,529] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1153/6501 [33:09<2:36:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0422, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1153/6501 [33:09<2:36:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:40,335] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1154/6501 [33:11<2:37:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8375, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1154/6501 [33:11<2:37:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1155/6501 [33:13<2:38:45,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2696, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1155/6501 [33:13<2:38:45,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:43,919] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1156/6501 [33:15<2:38:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2974, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1156/6501 [33:15<2:38:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:45,516] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1157/6501 [33:16<2:33:45,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1869, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1157/6501 [33:16<2:33:45,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:47,157] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1158/6501 [33:18<2:31:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.293, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1158/6501 [33:18<2:31:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1159/6501 [33:19<2:14:18,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.225, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1159/6501 [33:19<2:14:18,  1.51s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1160/6501 [33:21<2:26:24,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2822, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1160/6501 [33:21<2:26:24,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:51,945] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1161/6501 [33:23<2:29:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0833, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1161/6501 [33:23<2:29:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1162/6501 [33:24<2:32:27,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.337, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1162/6501 [33:24<2:32:27,  1.71s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1163/6501 [33:25<2:13:55,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1825, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1163/6501 [33:25<2:13:55,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:56,509] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1164/6501 [33:27<2:20:33,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7542, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1164/6501 [33:27<2:20:33,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:52:58,145] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1165/6501 [33:29<2:22:01,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9573, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1165/6501 [33:29<2:22:01,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:00,186] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1166/6501 [33:31<2:33:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7979, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1166/6501 [33:31<2:33:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:02,005] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1167/6501 [33:33<2:36:10,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9352, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1167/6501 [33:33<2:36:10,  1.76s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1168/6501 [33:34<2:25:25,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3021, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1168/6501 [33:34<2:25:25,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:05,125] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1169/6501 [33:36<2:28:50,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2842, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1169/6501 [33:36<2:28:50,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:07,115] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1170/6501 [33:38<2:37:12,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1368, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1170/6501 [33:38<2:37:12,  1.77s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1171/6501 [33:39<2:19:13,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1065, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1171/6501 [33:39<2:19:13,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:09,820] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1172/6501 [33:41<2:20:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2969, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1172/6501 [33:41<2:20:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:11,598] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1173/6501 [33:42<2:25:34,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1663, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1173/6501 [33:42<2:25:34,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:13,339] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1174/6501 [33:44<2:28:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3256, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1174/6501 [33:44<2:28:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1175/6501 [33:46<2:25:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.011, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1175/6501 [33:46<2:25:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:16,750] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1176/6501 [33:47<2:31:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5314, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1176/6501 [33:47<2:31:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1177/6501 [33:49<2:31:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7869, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1177/6501 [33:49<2:31:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:20,385] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1178/6501 [33:51<2:37:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4854, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1178/6501 [33:51<2:37:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:22,494] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1179/6501 [33:53<2:46:00,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.198, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1179/6501 [33:53<2:46:00,  1.87s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1180/6501 [33:55<2:31:42,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9968, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1180/6501 [33:55<2:31:42,  1.71s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1181/6501 [33:56<2:37:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1476, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1181/6501 [33:56<2:37:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1182/6501 [33:58<2:35:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2868, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1182/6501 [33:58<2:35:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:29,325] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1183/6501 [34:00<2:38:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2775, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1183/6501 [34:00<2:38:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:30,922] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1184/6501 [34:02<2:33:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3307, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1184/6501 [34:02<2:33:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1185/6501 [34:04<2:37:51,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3177, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1185/6501 [34:04<2:37:51,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:34,679] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1186/6501 [34:05<2:39:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7782, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1186/6501 [34:05<2:39:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1187/6501 [34:07<2:36:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4136, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1187/6501 [34:07<2:36:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:38,113] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1188/6501 [34:09<2:35:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2258, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1188/6501 [34:09<2:35:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1189/6501 [34:11<2:35:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7008, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1189/6501 [34:11<2:35:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1190/6501 [34:12<2:16:02,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.22, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1190/6501 [34:12<2:16:02,  1.54s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1191/6501 [34:13<2:01:33,  1.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3312, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1191/6501 [34:13<2:01:33,  1.37s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1192/6501 [34:14<2:13:16,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1403, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1192/6501 [34:14<2:13:16,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:45,541] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1193/6501 [34:16<2:22:27,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3062, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1193/6501 [34:16<2:22:27,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:47,369] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1194/6501 [34:18<2:28:11,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3448, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1194/6501 [34:18<2:28:11,  1.68s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1195/6501 [34:19<2:07:57,  1.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1996, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1195/6501 [34:19<2:07:57,  1.45s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:50,206] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1196/6501 [34:21<2:20:33,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4988, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1196/6501 [34:21<2:20:33,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:52,119] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1197/6501 [34:23<2:29:06,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3797, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1197/6501 [34:23<2:29:06,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:53,769] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1198/6501 [34:24<2:28:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0154, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1198/6501 [34:24<2:28:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:55,727] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1199/6501 [34:26<2:35:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3065, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1199/6501 [34:26<2:35:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:57,213] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1200/6501 [34:28<2:28:15,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0306, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1200/6501 [34:28<2:28:15,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:53:59,019] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1201/6501 [34:30<2:31:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1201/6501 [34:30<2:31:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4703, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1202/6501 [34:31<2:32:28,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2245, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 1202/6501 [34:31<2:32:28,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:02,441] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1203/6501 [34:33<2:31:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3774, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1203/6501 [34:33<2:31:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:04,073] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1204/6501 [34:35<2:28:55,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.032, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1204/6501 [34:35<2:28:55,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:05,829] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1205/6501 [34:37<2:30:42,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.17, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1205/6501 [34:37<2:30:42,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:07,827] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1206/6501 [34:39<2:38:21,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.98, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1206/6501 [34:39<2:38:21,  1.79s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1207/6501 [34:40<2:38:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2656, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1207/6501 [34:40<2:38:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:10,850] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1208/6501 [34:42<2:23:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3552, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1208/6501 [34:42<2:23:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:12,858] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1209/6501 [34:44<2:33:21,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2933, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1209/6501 [34:44<2:33:21,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:14,728] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1210/6501 [34:45<2:36:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1848, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1210/6501 [34:45<2:36:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1211/6501 [34:47<2:34:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8942, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1211/6501 [34:47<2:34:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1212/6501 [34:49<2:37:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3005, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1212/6501 [34:49<2:37:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:20,150] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1213/6501 [34:51<2:39:27,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.358, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1213/6501 [34:51<2:39:27,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:21,694] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1214/6501 [34:52<2:32:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3013, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1214/6501 [34:52<2:32:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:23,393] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1215/6501 [34:54<2:31:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1018, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1215/6501 [34:54<2:31:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1216/6501 [34:56<2:34:07,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4408, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1216/6501 [34:56<2:34:07,  1.75s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1217/6501 [34:58<2:31:08,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0732, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1217/6501 [34:58<2:31:08,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:28,763] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1218/6501 [34:59<2:36:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0528, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 1218/6501 [34:59<2:36:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:30,277] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1219/6501 [35:01<2:29:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3501, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1219/6501 [35:01<2:29:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:32,239] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1220/6501 [35:03<2:36:21,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6643, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1220/6501 [35:03<2:36:21,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:33,964] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1221/6501 [35:05<2:34:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.135, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1221/6501 [35:05<2:34:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1222/6501 [35:06<2:27:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1736, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1222/6501 [35:06<2:27:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:37,575] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1223/6501 [35:08<2:39:33,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2909, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1223/6501 [35:08<2:39:33,  1.81s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1224/6501 [35:10<2:34:40,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2756, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1224/6501 [35:10<2:34:40,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:40,943] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1225/6501 [35:12<2:34:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1101, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1225/6501 [35:12<2:34:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:42,653] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1226/6501 [35:13<2:32:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0606, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1226/6501 [35:13<2:32:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1227/6501 [35:15<2:28:57,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2132, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1227/6501 [35:15<2:28:57,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:45,996] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1228/6501 [35:17<2:30:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6392, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1228/6501 [35:17<2:30:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:47,996] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1229/6501 [35:19<2:38:02,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1566, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1229/6501 [35:19<2:38:02,  1.80s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1230/6501 [35:20<2:34:50,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4344, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1230/6501 [35:20<2:34:50,  1.76s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1231/6501 [35:22<2:34:35,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3239, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1231/6501 [35:22<2:34:35,  1.76s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1232/6501 [35:24<2:35:35,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9732, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1232/6501 [35:24<2:35:35,  1.77s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1233/6501 [35:26<2:35:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3364, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1233/6501 [35:26<2:35:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:56,698] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1234/6501 [35:27<2:33:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3581, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1234/6501 [35:27<2:33:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1235/6501 [35:28<2:15:35,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5797, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1235/6501 [35:28<2:15:35,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:54:59,491] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1236/6501 [35:30<2:20:14,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.142, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1236/6501 [35:30<2:20:14,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:00,998] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1237/6501 [35:32<2:17:48,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.944, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1237/6501 [35:32<2:17:48,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:02,688] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1238/6501 [35:33<2:20:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0931, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1238/6501 [35:33<2:20:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1239/6501 [35:35<2:24:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2443, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1239/6501 [35:35<2:24:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:06,164] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1240/6501 [35:37<2:26:43,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3643, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1240/6501 [35:37<2:26:43,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:07,913] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1241/6501 [35:39<2:28:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1774, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1241/6501 [35:39<2:28:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:09,747] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1242/6501 [35:40<2:32:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1134, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1242/6501 [35:40<2:32:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1243/6501 [35:42<2:25:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3669, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1243/6501 [35:42<2:25:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:13,120] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1244/6501 [35:44<2:31:28,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8789, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1244/6501 [35:44<2:31:28,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:14,758] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1245/6501 [35:45<2:29:03,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0368, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1245/6501 [35:45<2:29:03,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:16,632] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1246/6501 [35:47<2:33:34,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4211, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1246/6501 [35:47<2:33:34,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:18,270] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1247/6501 [35:49<2:30:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1615, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1247/6501 [35:49<2:30:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:19,927] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1248/6501 [35:51<2:28:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9431, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1248/6501 [35:51<2:28:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1249/6501 [35:52<2:29:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9039, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1249/6501 [35:52<2:29:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:23,340] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1250/6501 [35:54<2:28:56,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1099, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1250/6501 [35:54<2:28:56,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:25,229] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1251/6501 [35:56<2:33:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4316, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1251/6501 [35:56<2:33:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:27,070] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1252/6501 [35:58<2:35:58,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3964, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1252/6501 [35:58<2:35:58,  1.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1253/6501 [36:00<2:38:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2524, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1253/6501 [36:00<2:38:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:30,765] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1254/6501 [36:01<2:38:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4702, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1254/6501 [36:01<2:38:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1255/6501 [36:03<2:24:24,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1424, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1255/6501 [36:03<2:24:24,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:33,737] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1256/6501 [36:04<2:25:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9413, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1256/6501 [36:04<2:25:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:35,482] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1257/6501 [36:06<2:27:36,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9723, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1257/6501 [36:06<2:27:36,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:37,342] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1258/6501 [36:08<2:32:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1249, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1258/6501 [36:08<2:32:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1259/6501 [36:10<2:29:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1029, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1259/6501 [36:10<2:29:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1260/6501 [36:11<2:08:33,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3749, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1260/6501 [36:11<2:08:33,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:41,758] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1261/6501 [36:12<2:18:47,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0705, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1261/6501 [36:12<2:18:47,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:43,690] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1262/6501 [36:14<2:27:45,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1956, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1262/6501 [36:14<2:27:45,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:45,341] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1263/6501 [36:16<2:26:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.11, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1263/6501 [36:16<2:26:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1264/6501 [36:18<2:35:20,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0562, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1264/6501 [36:18<2:35:20,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:49,329] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1265/6501 [36:20<2:40:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.503, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1265/6501 [36:20<2:40:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:51,294] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1266/6501 [36:22<2:43:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1055, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1266/6501 [36:22<2:43:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:52,912] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1267/6501 [36:24<2:36:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3734, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 1267/6501 [36:24<2:36:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1268/6501 [36:25<2:29:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1435, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1268/6501 [36:25<2:29:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1269/6501 [36:27<2:32:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1005, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1269/6501 [36:27<2:32:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:58,206] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1270/6501 [36:29<2:37:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4008, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1270/6501 [36:29<2:37:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:55:59,400] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1271/6501 [36:30<2:21:29,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1205, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1271/6501 [36:30<2:21:29,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:01,144] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1272/6501 [36:32<2:24:38,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0009, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1272/6501 [36:32<2:24:38,  1.66s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1273/6501 [36:34<2:31:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1559, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1273/6501 [36:34<2:31:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:04,149] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1274/6501 [36:35<2:14:11,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9234, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1274/6501 [36:35<2:14:11,  1.54s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1275/6501 [36:37<2:29:24,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1931, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1275/6501 [36:37<2:29:24,  1.72s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1276/6501 [36:39<2:25:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7329, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1276/6501 [36:39<2:25:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:09,733] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1277/6501 [36:40<2:31:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6847, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1277/6501 [36:40<2:31:16,  1.74s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1278/6501 [36:42<2:32:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1594, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1278/6501 [36:42<2:32:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1279/6501 [36:43<2:13:31,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2883, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1279/6501 [36:43<2:13:31,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:14,239] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1280/6501 [36:45<2:17:43,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.235, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1280/6501 [36:45<2:17:43,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:15,953] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1281/6501 [36:47<2:21:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9708, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1281/6501 [36:47<2:21:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1282/6501 [36:48<2:11:32,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2548, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1282/6501 [36:48<2:11:32,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:18,865] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1283/6501 [36:50<2:15:15,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6149, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1283/6501 [36:50<2:15:15,  1.56s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1284/6501 [36:51<2:14:20,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5892, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1284/6501 [36:51<2:14:20,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:22,278] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1285/6501 [36:53<2:23:21,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2216, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1285/6501 [36:53<2:23:21,  1.65s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1286/6501 [36:54<2:05:28,  1.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3644, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1286/6501 [36:54<2:05:28,  1.44s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:25,270] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1287/6501 [36:56<2:20:40,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1527, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1287/6501 [36:56<2:20:40,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:27,046] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1288/6501 [36:58<2:24:43,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5341, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1288/6501 [36:58<2:24:43,  1.67s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1289/6501 [36:59<2:17:03,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4157, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1289/6501 [36:59<2:17:03,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:30,166] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1290/6501 [37:01<2:21:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1663, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1290/6501 [37:01<2:21:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:32,029] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1291/6501 [37:03<2:27:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5856, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1291/6501 [37:03<2:27:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1292/6501 [37:05<2:32:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1344, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1292/6501 [37:05<2:32:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1293/6501 [37:06<2:12:56,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4122, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1293/6501 [37:06<2:12:56,  1.53s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1294/6501 [37:07<1:57:10,  1.35s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3097, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1294/6501 [37:07<1:57:10,  1.35s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:36,885] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1295/6501 [37:08<1:48:57,  1.26s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5212, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1295/6501 [37:08<1:48:57,  1.26s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1296/6501 [37:10<2:09:27,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0725, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1296/6501 [37:10<2:09:27,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:40,500] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1297/6501 [37:11<2:11:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.424, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1297/6501 [37:11<2:11:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1298/6501 [37:13<2:09:45,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0344, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1298/6501 [37:13<2:09:45,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:43,590] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1299/6501 [37:14<2:13:27,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2525, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1299/6501 [37:14<2:13:27,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:45,435] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1300/6501 [37:16<2:21:22,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.457, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1300/6501 [37:16<2:21:22,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:46,954] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1301/6501 [37:18<2:18:26,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1409, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1301/6501 [37:18<2:18:26,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:48,619] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1302/6501 [37:19<2:20:10,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9138, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1302/6501 [37:19<2:20:10,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:50,387] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1303/6501 [37:21<2:24:03,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4376, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1303/6501 [37:21<2:24:03,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:52,034] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1304/6501 [37:23<2:23:37,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6672, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1304/6501 [37:23<2:23:37,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:53,955] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1305/6501 [37:25<2:30:24,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2166, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1305/6501 [37:25<2:30:24,  1.74s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1306/6501 [37:26<2:25:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4079, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1306/6501 [37:26<2:25:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:57,174] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1307/6501 [37:28<2:25:13,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4176, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1307/6501 [37:28<2:25:13,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:56:59,033] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1308/6501 [37:30<2:29:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.392, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1308/6501 [37:30<2:29:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:00,945] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1309/6501 [37:32<2:34:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3662, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1309/6501 [37:32<2:34:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:02,820] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1310/6501 [37:34<2:36:49,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3397, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1310/6501 [37:34<2:36:49,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:04,748] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1311/6501 [37:35<2:39:47,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3869, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1311/6501 [37:35<2:39:47,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:06,323] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1312/6501 [37:37<2:32:42,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2574, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1312/6501 [37:37<2:32:42,  1.77s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1313/6501 [37:39<2:37:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5757, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1313/6501 [37:39<2:37:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:09,975] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1314/6501 [37:41<2:34:12,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.113, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1314/6501 [37:41<2:34:12,  1.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1315/6501 [37:42<2:28:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1409, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1315/6501 [37:42<2:28:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:13,272] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1316/6501 [37:44<2:28:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4174, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1316/6501 [37:44<2:28:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:14,927] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1317/6501 [37:46<2:27:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9753, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1317/6501 [37:46<2:27:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1318/6501 [37:47<2:26:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3135, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1318/6501 [37:47<2:26:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:18,311] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1319/6501 [37:49<2:26:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1504, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1319/6501 [37:49<2:26:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1320/6501 [37:51<2:21:57,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2851, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1320/6501 [37:51<2:21:57,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:21,755] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1321/6501 [37:52<2:29:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0273, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1321/6501 [37:52<2:29:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1322/6501 [37:54<2:12:59,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1951, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1322/6501 [37:54<2:12:59,  1.54s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1323/6501 [37:55<2:14:30,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3242, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1323/6501 [37:55<2:14:30,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:25,981] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1324/6501 [37:57<2:13:36,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0014, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1324/6501 [37:57<2:13:36,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:28,030] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1325/6501 [37:59<2:26:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2285, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1325/6501 [37:59<2:26:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:29,538] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1326/6501 [38:00<2:21:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3345, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1326/6501 [38:00<2:21:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1327/6501 [38:02<2:23:41,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3433, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1327/6501 [38:02<2:23:41,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:32,786] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1328/6501 [38:03<2:19:57,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.566, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1328/6501 [38:03<2:19:57,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:34,997] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1329/6501 [38:06<2:35:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2285, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1329/6501 [38:06<2:35:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:36,650] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1330/6501 [38:07<2:31:18,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3183, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1330/6501 [38:07<2:31:18,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:38,342] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1331/6501 [38:09<2:29:38,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0828, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 1331/6501 [38:09<2:29:38,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:40,085] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|██        | 1332/6501 [38:11<2:29:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1332/6501 [38:11<2:29:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.453, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m21%|██        | 1333/6501 [38:12<2:20:19,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2868, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1333/6501 [38:12<2:20:19,  1.63s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1334/6501 [38:14<2:21:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0491, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1334/6501 [38:14<2:21:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:44,908] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1335/6501 [38:16<2:24:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9557, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1335/6501 [38:16<2:24:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:46,599] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1336/6501 [38:17<2:25:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2375, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1336/6501 [38:17<2:25:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1337/6501 [38:19<2:24:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.541, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1337/6501 [38:19<2:24:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:50,205] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1338/6501 [38:21<2:31:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1648, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1338/6501 [38:21<2:31:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1339/6501 [38:22<2:25:46,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4255, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1339/6501 [38:22<2:25:46,  1.69s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1340/6501 [38:24<2:26:26,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5893, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1340/6501 [38:24<2:26:26,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:55,171] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1341/6501 [38:26<2:26:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.04, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1341/6501 [38:26<2:26:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:57:57,187] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1342/6501 [38:28<2:34:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2419, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1342/6501 [38:28<2:34:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1343/6501 [38:30<2:32:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0135, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1343/6501 [38:30<2:32:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:00,953] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1344/6501 [38:32<2:39:26,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2848, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1344/6501 [38:32<2:39:26,  1.86s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1345/6501 [38:34<2:40:45,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0419, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1345/6501 [38:34<2:40:45,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:04,636] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1346/6501 [38:35<2:38:16,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3051, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1346/6501 [38:35<2:38:16,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:06,373] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1347/6501 [38:37<2:35:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1282, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1347/6501 [38:37<2:35:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1348/6501 [38:39<2:34:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2131, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1348/6501 [38:39<2:34:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:09,901] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1349/6501 [38:41<2:33:21,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0611, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1349/6501 [38:41<2:33:21,  1.79s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1350/6501 [38:42<2:31:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3706, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1350/6501 [38:42<2:31:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:13,548] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1351/6501 [38:44<2:35:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0453, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1351/6501 [38:44<2:35:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:15,390] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1352/6501 [38:46<2:36:22,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1907, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1352/6501 [38:46<2:36:22,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:17,418] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1353/6501 [38:48<2:41:38,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7163, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1353/6501 [38:48<2:41:38,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:19,072] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1354/6501 [38:50<2:35:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2737, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1354/6501 [38:50<2:35:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:20,924] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1355/6501 [38:52<2:36:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4065, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1355/6501 [38:52<2:36:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1356/6501 [38:54<2:39:54,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6048, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1356/6501 [38:54<2:39:54,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:24,629] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1357/6501 [38:55<2:36:54,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6846, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1357/6501 [38:55<2:36:54,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:26,223] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1358/6501 [38:57<2:30:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1678, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1358/6501 [38:57<2:30:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1359/6501 [38:59<2:28:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0218, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1359/6501 [38:59<2:28:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:29,618] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1360/6501 [39:00<2:28:06,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1874, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1360/6501 [39:00<2:28:06,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:31,362] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1361/6501 [39:02<2:28:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6658, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1361/6501 [39:02<2:28:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:33,448] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1362/6501 [39:04<2:37:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2631, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1362/6501 [39:04<2:37:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:35,310] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1363/6501 [39:06<2:38:03,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3664, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1363/6501 [39:06<2:38:03,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:37,017] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1364/6501 [39:08<2:34:28,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1851, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1364/6501 [39:08<2:34:28,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:38,842] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1365/6501 [39:10<2:34:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2298, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1365/6501 [39:10<2:34:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1366/6501 [39:12<2:39:30,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2955, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1366/6501 [39:12<2:39:30,  1.86s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1367/6501 [39:13<2:31:57,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9847, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1367/6501 [39:13<2:31:57,  1.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1368/6501 [39:15<2:25:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2113, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1368/6501 [39:15<2:25:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1369/6501 [39:16<2:27:39,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4199, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1369/6501 [39:16<2:27:39,  1.73s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1370/6501 [39:18<2:31:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0868, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1370/6501 [39:18<2:31:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1371/6501 [39:20<2:27:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1384, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1371/6501 [39:20<2:27:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1372/6501 [39:22<2:26:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.449, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1372/6501 [39:22<2:26:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:52,725] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1373/6501 [39:23<2:29:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8641, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1373/6501 [39:23<2:29:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:54,652] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1374/6501 [39:25<2:33:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4848, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1374/6501 [39:25<2:33:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:56,637] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1375/6501 [39:27<2:38:35,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1134, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1375/6501 [39:27<2:38:35,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:58:58,507] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1376/6501 [39:29<2:38:55,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9662, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1376/6501 [39:29<2:38:55,  1.86s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1377/6501 [39:31<2:36:55,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2349, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1377/6501 [39:31<2:36:55,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:02,102] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1378/6501 [39:33<2:36:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8992, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1378/6501 [39:33<2:36:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:03,700] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1379/6501 [39:34<2:30:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5991, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1379/6501 [39:34<2:30:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 1380/6501 [39:35<2:09:59,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4416, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1380/6501 [39:35<2:09:59,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:06,206] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 1381/6501 [39:37<2:10:17,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3221, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 1381/6501 [39:37<2:10:17,  1.53s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1382/6501 [39:38<2:06:35,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3981, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1382/6501 [39:38<2:06:35,  1.48s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1383/6501 [39:40<2:07:03,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0486, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1383/6501 [39:40<2:07:03,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:11,057] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1384/6501 [39:42<2:19:11,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2151, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1384/6501 [39:42<2:19:11,  1.63s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1385/6501 [39:43<2:02:18,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2752, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1385/6501 [39:43<2:02:18,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:14,127] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1386/6501 [39:45<2:19:14,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.159, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1386/6501 [39:45<2:19:14,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:15,864] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1387/6501 [39:47<2:21:51,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3378, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1387/6501 [39:47<2:21:51,  1.66s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1388/6501 [39:48<2:25:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8127, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1388/6501 [39:48<2:25:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1389/6501 [39:50<2:24:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9868, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1389/6501 [39:50<2:24:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1390/6501 [39:52<2:25:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0799, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1390/6501 [39:52<2:25:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1391/6501 [39:53<2:24:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9603, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1391/6501 [39:53<2:24:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:24,451] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1392/6501 [39:55<2:25:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.935, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1392/6501 [39:55<2:25:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1393/6501 [39:57<2:23:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2328, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1393/6501 [39:57<2:23:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1394/6501 [39:58<2:22:20,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3227, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1394/6501 [39:58<2:22:20,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:29,593] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1395/6501 [40:00<2:27:06,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0798, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1395/6501 [40:00<2:27:06,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:30,682] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1396/6501 [40:01<2:10:45,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3066, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1396/6501 [40:01<2:10:45,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:32,617] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1397/6501 [40:03<2:20:54,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7959, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 1397/6501 [40:03<2:20:54,  1.66s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1398/6501 [40:05<2:20:27,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6711, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1398/6501 [40:05<2:20:27,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:35,893] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1399/6501 [40:07<2:20:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8187, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1399/6501 [40:07<2:20:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:37,762] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1400/6501 [40:08<2:25:40,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1865, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1400/6501 [40:08<2:25:40,  1.71s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1401/6501 [40:10<2:26:23,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.662, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1401/6501 [40:10<2:26:23,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:41,315] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1402/6501 [40:12<2:28:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3411, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1402/6501 [40:12<2:28:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:43,148] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1403/6501 [40:14<2:30:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0168, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1403/6501 [40:14<2:30:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:45,088] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1404/6501 [40:16<2:34:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.138, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1404/6501 [40:16<2:34:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:47,075] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1405/6501 [40:18<2:39:03,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8227, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1405/6501 [40:18<2:39:03,  1.87s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1406/6501 [40:20<2:39:17,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3319, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1406/6501 [40:20<2:39:17,  1.88s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1407/6501 [40:21<2:34:57,  1.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1407/6501 [40:21<2:34:57,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9588, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1408/6501 [40:24<2:45:22,  1.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0176, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1408/6501 [40:24<2:45:22,  1.95s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:54,579] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1409/6501 [40:25<2:38:28,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2731, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1409/6501 [40:25<2:38:28,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:56,348] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1410/6501 [40:27<2:35:56,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.015, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1410/6501 [40:27<2:35:56,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 08:59:58,217] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1411/6501 [40:29<2:36:42,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3763, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1411/6501 [40:29<2:36:42,  1.85s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1412/6501 [40:31<2:34:50,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3702, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1412/6501 [40:31<2:34:50,  1.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1413/6501 [40:33<2:34:45,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2003, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1413/6501 [40:33<2:34:45,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:03,851] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1414/6501 [40:35<2:40:04,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7183, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1414/6501 [40:35<2:40:04,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:05,532] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1415/6501 [40:36<2:34:47,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6337, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1415/6501 [40:36<2:34:47,  1.83s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1416/6501 [40:38<2:27:37,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2298, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1416/6501 [40:38<2:27:37,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:08,809] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1417/6501 [40:40<2:27:19,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6316, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1417/6501 [40:40<2:27:19,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:10,454] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1418/6501 [40:41<2:24:55,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1947, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1418/6501 [40:41<2:24:55,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:12,091] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1419/6501 [40:43<2:23:00,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3154, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1419/6501 [40:43<2:23:00,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:13,842] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1420/6501 [40:45<2:24:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1420/6501 [40:45<2:24:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8658, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:15,832] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1421/6501 [40:47<2:31:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6652, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1421/6501 [40:47<2:31:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:17,653] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1422/6501 [40:48<2:32:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5067, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1422/6501 [40:48<2:32:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1423/6501 [40:49<2:14:08,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9635, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1423/6501 [40:49<2:14:08,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:20,695] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1424/6501 [40:51<2:23:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2293, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1424/6501 [40:51<2:23:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:22,522] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1425/6501 [40:53<2:26:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2441, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1425/6501 [40:53<2:26:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1426/6501 [40:55<2:26:59,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4506, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1426/6501 [40:55<2:26:59,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:26,153] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1427/6501 [40:57<2:30:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9872, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1427/6501 [40:57<2:30:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1428/6501 [40:58<2:09:47,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3864, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1428/6501 [40:58<2:09:47,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:28,936] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1429/6501 [41:00<2:17:11,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1952, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1429/6501 [41:00<2:17:11,  1.62s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1430/6501 [41:01<2:17:38,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1172, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1430/6501 [41:01<2:17:38,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:32,699] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1431/6501 [41:03<2:30:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3621, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1431/6501 [41:03<2:30:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1432/6501 [41:04<2:12:07,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2584, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1432/6501 [41:04<2:12:07,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:35,725] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1433/6501 [41:06<2:22:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1865, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1433/6501 [41:06<2:22:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:37,634] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1434/6501 [41:08<2:27:47,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5828, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1434/6501 [41:08<2:27:47,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:39,532] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1435/6501 [41:10<2:31:31,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1264, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1435/6501 [41:10<2:31:31,  1.79s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1436/6501 [41:12<2:23:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0983, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1436/6501 [41:12<2:23:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1437/6501 [41:13<2:19:49,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2112, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1437/6501 [41:13<2:19:49,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:44,314] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1438/6501 [41:15<2:22:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0813, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1438/6501 [41:15<2:22:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:46,025] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1439/6501 [41:17<2:22:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.271, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1439/6501 [41:17<2:22:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:47,379] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1440/6501 [41:18<2:14:09,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6265, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1440/6501 [41:18<2:14:09,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:49,462] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1441/6501 [41:20<2:26:34,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5209, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1441/6501 [41:20<2:26:34,  1.74s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1442/6501 [41:22<2:23:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2133, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1442/6501 [41:22<2:23:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:52,971] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1443/6501 [41:24<2:28:14,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2927, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1443/6501 [41:24<2:28:14,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:54,540] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1444/6501 [41:25<2:23:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4174, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1444/6501 [41:25<2:23:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1445/6501 [41:27<2:22:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2106, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1445/6501 [41:27<2:22:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:57,816] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1446/6501 [41:29<2:20:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4613, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1446/6501 [41:29<2:20:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:00:59,525] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1447/6501 [41:30<2:21:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1447/6501 [41:30<2:21:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2805, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:01,227] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1448/6501 [41:32<2:22:00,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9522, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1448/6501 [41:32<2:22:00,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:03,032] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1449/6501 [41:34<2:24:59,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0462, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1449/6501 [41:34<2:24:59,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:04,838] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1450/6501 [41:36<2:27:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2165, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1450/6501 [41:36<2:27:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1451/6501 [41:37<2:25:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0203, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1451/6501 [41:37<2:25:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:08,140] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1452/6501 [41:39<2:22:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3067, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1452/6501 [41:39<2:22:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1453/6501 [41:40<2:19:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0433, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1453/6501 [41:40<2:19:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:11,461] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1454/6501 [41:42<2:21:37,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8911, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1454/6501 [41:42<2:21:37,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:13,227] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1455/6501 [41:44<2:23:40,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3011, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1455/6501 [41:44<2:23:40,  1.71s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1456/6501 [41:45<2:18:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5093, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1456/6501 [41:45<2:18:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:16,547] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1457/6501 [41:47<2:22:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0578, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1457/6501 [41:47<2:22:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1458/6501 [41:49<2:20:20,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4232, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1458/6501 [41:49<2:20:20,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:20,079] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1459/6501 [41:51<2:26:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.118, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1459/6501 [41:51<2:26:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1460/6501 [41:52<2:25:22,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4968, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1460/6501 [41:52<2:25:22,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:23,424] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1461/6501 [41:54<2:23:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9966, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1461/6501 [41:54<2:23:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1462/6501 [41:56<2:24:29,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1636, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 1462/6501 [41:56<2:24:29,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:27,084] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1463/6501 [41:58<2:29:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2974, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1463/6501 [41:58<2:29:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:28,996] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1464/6501 [42:00<2:32:35,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0532, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1464/6501 [42:00<2:32:35,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:30,944] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1465/6501 [42:02<2:35:50,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2343, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1465/6501 [42:02<2:35:50,  1.86s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1466/6501 [42:03<2:33:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0216, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1466/6501 [42:03<2:33:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:34,419] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1467/6501 [42:05<2:30:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9714, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1467/6501 [42:05<2:30:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:36,322] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1468/6501 [42:07<2:33:07,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2575, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1468/6501 [42:07<2:33:07,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:37,904] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1469/6501 [42:09<2:26:57,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6569, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1469/6501 [42:09<2:26:57,  1.75s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1470/6501 [42:10<2:26:54,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0127, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1470/6501 [42:10<2:26:54,  1.75s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1471/6501 [42:12<2:24:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1274, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1471/6501 [42:12<2:24:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:43,342] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1472/6501 [42:14<2:32:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0601, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1472/6501 [42:14<2:32:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:45,231] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1473/6501 [42:16<2:33:58,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3942, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1473/6501 [42:16<2:33:58,  1.84s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1474/6501 [42:18<2:35:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1671, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1474/6501 [42:18<2:35:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:48,674] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1475/6501 [42:19<2:27:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7617, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1475/6501 [42:19<2:27:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1476/6501 [42:21<2:24:46,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5892, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1476/6501 [42:21<2:24:46,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:51,738] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1477/6501 [42:22<2:16:56,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0437, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1477/6501 [42:22<2:16:56,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:53,756] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1478/6501 [42:24<2:26:31,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3742, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1478/6501 [42:24<2:26:31,  1.75s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1479/6501 [42:26<2:20:14,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2187, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1479/6501 [42:26<2:20:14,  1.68s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1480/6501 [42:27<2:05:24,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9391, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1480/6501 [42:27<2:05:24,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:01:58,258] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1481/6501 [42:29<2:15:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4016, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1481/6501 [42:29<2:15:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:00,033] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1482/6501 [42:31<2:19:35,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0651, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1482/6501 [42:31<2:19:35,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:01,797] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1483/6501 [42:32<2:21:57,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3859, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1483/6501 [42:32<2:21:57,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:03,758] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1484/6501 [42:34<2:28:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3145, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1484/6501 [42:34<2:28:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:05,523] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1485/6501 [42:36<2:28:14,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2332, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1485/6501 [42:36<2:28:14,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:07,237] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1486/6501 [42:38<2:26:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9048, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1486/6501 [42:38<2:26:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:08,986] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1487/6501 [42:40<2:26:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2527, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1487/6501 [42:40<2:26:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:10,941] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1488/6501 [42:42<2:31:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1033, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1488/6501 [42:42<2:31:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:12,780] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1489/6501 [42:43<2:32:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.363, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1489/6501 [42:43<2:32:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:14,488] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1490/6501 [42:45<2:29:17,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3098, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1490/6501 [42:45<2:29:17,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:16,322] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1491/6501 [42:47<2:30:24,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3755, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1491/6501 [42:47<2:30:24,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:18,214] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1492/6501 [42:49<2:32:39,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9814, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1492/6501 [42:49<2:32:39,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:19,962] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1493/6501 [42:51<2:30:37,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6534, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1493/6501 [42:51<2:30:37,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:21,732] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1494/6501 [42:52<2:29:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5376, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1494/6501 [42:52<2:29:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:23,763] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1495/6501 [42:54<2:35:37,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1135, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1495/6501 [42:54<2:35:37,  1.87s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1496/6501 [42:56<2:35:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4624, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1496/6501 [42:56<2:35:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:27,376] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1497/6501 [42:58<2:32:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4015, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1497/6501 [42:58<2:32:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:29,189] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1498/6501 [43:00<2:32:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9711, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1498/6501 [43:00<2:32:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1499/6501 [43:01<2:26:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1836, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1499/6501 [43:01<2:26:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:32,443] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1500/6501 [43:03<2:24:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.256, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1500/6501 [43:03<2:24:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:33,946] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1501/6501 [43:05<2:18:22,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3871, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1501/6501 [43:05<2:18:22,  1.66s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1502/6501 [43:06<2:02:16,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0688, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1502/6501 [43:06<2:02:16,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:36,770] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1503/6501 [43:07<2:10:44,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0596, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1503/6501 [43:07<2:10:44,  1.57s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1504/6501 [43:09<2:08:57,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2757, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1504/6501 [43:09<2:08:57,  1.55s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1505/6501 [43:11<2:14:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2864, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1505/6501 [43:11<2:14:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:41,794] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1506/6501 [43:12<2:17:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5383, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1506/6501 [43:12<2:17:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:43,955] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1507/6501 [43:15<2:30:24,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3029, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1507/6501 [43:15<2:30:24,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:45,674] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1508/6501 [43:16<2:28:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5592, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1508/6501 [43:16<2:28:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:47,735] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1509/6501 [43:18<2:35:08,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.229, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1509/6501 [43:18<2:35:08,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:49,849] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1510/6501 [43:21<2:41:19,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9838, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1510/6501 [43:21<2:41:19,  1.94s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1511/6501 [43:22<2:35:26,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3014, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1511/6501 [43:22<2:35:26,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:53,377] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1512/6501 [43:24<2:34:15,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1904, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1512/6501 [43:24<2:34:15,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:55,142] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1513/6501 [43:26<2:31:58,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0662, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1513/6501 [43:26<2:31:58,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:56,791] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1514/6501 [43:27<2:27:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.268, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1514/6501 [43:27<2:27:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:58,329] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1515/6501 [43:29<2:21:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3181, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1515/6501 [43:29<2:21:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:02:59,871] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1516/6501 [43:31<2:17:29,  1.65s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1516/6501 [43:31<2:17:29,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7487, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1517/6501 [43:31<1:58:47,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.577, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1517/6501 [43:31<1:58:47,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:02,683] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1518/6501 [43:33<2:10:39,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1286, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1518/6501 [43:33<2:10:39,  1.57s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1519/6501 [43:35<2:11:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1253, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1519/6501 [43:35<2:11:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:06,374] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1520/6501 [43:37<2:23:52,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1897, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1520/6501 [43:37<2:23:52,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:08,104] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1521/6501 [43:39<2:23:45,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1202, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1521/6501 [43:39<2:23:45,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:09,714] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1522/6501 [43:40<2:20:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5778, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1522/6501 [43:40<2:20:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:11,531] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1523/6501 [43:42<2:23:41,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3037, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1523/6501 [43:42<2:23:41,  1.73s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1524/6501 [43:44<2:26:57,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.188, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1524/6501 [43:44<2:26:57,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:15,240] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1525/6501 [43:46<2:28:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3389, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1525/6501 [43:46<2:28:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1526/6501 [43:47<2:20:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1364, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1526/6501 [43:47<2:20:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1527/6501 [43:49<2:20:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1309, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 1527/6501 [43:49<2:20:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1528/6501 [43:51<2:19:29,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.968, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1528/6501 [43:51<2:19:29,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:21,789] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1529/6501 [43:52<2:20:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3265, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1529/6501 [43:52<2:20:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:23,109] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1530/6501 [43:54<2:11:10,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.362, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1530/6501 [43:54<2:11:10,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:25,282] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1531/6501 [43:56<2:25:47,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0119, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1531/6501 [43:56<2:25:47,  1.76s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1532/6501 [43:58<2:24:28,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1829, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1532/6501 [43:58<2:24:28,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:28,631] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1533/6501 [43:59<2:21:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3005, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1533/6501 [43:59<2:21:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1534/6501 [44:01<2:17:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1222, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1534/6501 [44:01<2:17:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:31,764] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1535/6501 [44:02<2:15:54,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.198, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1535/6501 [44:02<2:15:54,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:33,675] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1536/6501 [44:04<2:22:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0463, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1536/6501 [44:04<2:22:33,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:35,588] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1537/6501 [44:06<2:27:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0193, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1537/6501 [44:06<2:27:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:37,468] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1538/6501 [44:08<2:29:42,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3054, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1538/6501 [44:08<2:29:42,  1.81s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1539/6501 [44:09<2:17:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2191, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1539/6501 [44:09<2:17:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:40,853] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1540/6501 [44:12<2:27:28,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.317, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1540/6501 [44:12<2:27:28,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:42,834] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1541/6501 [44:14<2:32:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3562, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1541/6501 [44:14<2:32:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:44,514] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1542/6501 [44:15<2:28:16,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2903, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1542/6501 [44:15<2:28:16,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:46,264] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1543/6501 [44:17<2:27:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2719, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 1543/6501 [44:17<2:27:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:48,109] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1544/6501 [44:19<2:28:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2464, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1544/6501 [44:19<2:28:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1545/6501 [44:20<2:23:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2679, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1545/6501 [44:20<2:23:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1546/6501 [44:22<2:19:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2611, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1546/6501 [44:22<2:19:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:53,014] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1547/6501 [44:24<2:20:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1389, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1547/6501 [44:24<2:20:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1548/6501 [44:25<2:19:24,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3129, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1548/6501 [44:25<2:19:24,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:56,455] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1549/6501 [44:27<2:21:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2384, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1549/6501 [44:27<2:21:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:03:58,337] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1550/6501 [44:29<2:25:51,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9856, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1550/6501 [44:29<2:25:51,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:00,099] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1551/6501 [44:31<2:25:40,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1791, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1551/6501 [44:31<2:25:40,  1.77s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1552/6501 [44:33<2:28:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9598, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1552/6501 [44:33<2:28:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:03,793] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1553/6501 [44:34<2:28:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2415, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1553/6501 [44:34<2:28:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:05,446] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1554/6501 [44:36<2:24:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1545, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1554/6501 [44:36<2:24:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1555/6501 [44:38<2:33:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3499, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1555/6501 [44:38<2:33:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:09,232] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1556/6501 [44:40<2:29:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7356, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1556/6501 [44:40<2:29:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:11,045] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1557/6501 [44:42<2:29:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2085, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1557/6501 [44:42<2:29:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1558/6501 [44:43<2:21:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.959, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1558/6501 [44:43<2:21:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:14,241] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1559/6501 [44:45<2:20:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4581, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1559/6501 [44:45<2:20:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:15,880] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1560/6501 [44:47<2:19:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0649, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1560/6501 [44:47<2:19:03,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:17,610] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1561/6501 [44:48<2:20:03,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.789, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1561/6501 [44:48<2:20:03,  1.70s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1562/6501 [44:50<2:19:37,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.522, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1562/6501 [44:50<2:19:37,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:21,134] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1563/6501 [44:52<2:23:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2394, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1563/6501 [44:52<2:23:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:22,930] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1564/6501 [44:54<2:24:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1869, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1564/6501 [44:54<2:24:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:24,610] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1565/6501 [44:55<2:22:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1565/6501 [44:55<2:22:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1587, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1566/6501 [44:57<2:18:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4078, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1566/6501 [44:57<2:18:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:27,993] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1567/6501 [44:59<2:21:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3464, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1567/6501 [44:59<2:21:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:29,714] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1568/6501 [45:00<2:21:35,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2206, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1568/6501 [45:00<2:21:35,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:31,394] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1569/6501 [45:02<2:20:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.233, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1569/6501 [45:02<2:20:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:33,510] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1570/6501 [45:04<2:30:32,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.071, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1570/6501 [45:04<2:30:32,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:35,188] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1571/6501 [45:06<2:26:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.149, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1571/6501 [45:06<2:26:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:37,485] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1572/6501 [45:08<2:39:16,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3315, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1572/6501 [45:08<2:39:16,  1.94s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:39,149] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1573/6501 [45:10<2:32:28,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5787, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1573/6501 [45:10<2:32:28,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:40,632] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1574/6501 [45:11<2:23:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1089, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1574/6501 [45:11<2:23:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:42,655] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1575/6501 [45:13<2:30:04,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4781, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1575/6501 [45:13<2:30:04,  1.83s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1576/6501 [45:15<2:25:58,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.577, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1576/6501 [45:15<2:25:58,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:46,058] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1577/6501 [45:17<2:25:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4347, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1577/6501 [45:17<2:25:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:47,210] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1578/6501 [45:18<2:09:50,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2881, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1578/6501 [45:18<2:09:50,  1.58s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1579/6501 [45:19<2:02:32,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4487, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1579/6501 [45:19<2:02:32,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:50,148] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1580/6501 [45:21<2:06:23,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0524, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1580/6501 [45:21<2:06:23,  1.54s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1581/6501 [45:22<2:03:37,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4459, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1581/6501 [45:22<2:03:37,  1.51s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1582/6501 [45:24<2:08:59,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2734, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1582/6501 [45:24<2:08:59,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:54,913] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1583/6501 [45:26<2:09:50,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3469, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1583/6501 [45:26<2:09:50,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:56,647] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1584/6501 [45:27<2:13:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3899, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1584/6501 [45:27<2:13:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:04:58,226] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1585/6501 [45:29<2:12:14,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0411, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1585/6501 [45:29<2:12:14,  1.61s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1586/6501 [45:30<2:04:43,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9923, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1586/6501 [45:30<2:04:43,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:01,102] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1587/6501 [45:32<2:05:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.966, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1587/6501 [45:32<2:05:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1588/6501 [45:33<1:59:42,  1.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2405, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1588/6501 [45:33<1:59:42,  1.46s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:04,113] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1589/6501 [45:35<2:06:02,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2461, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1589/6501 [45:35<2:06:02,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:05,655] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1590/6501 [45:36<2:06:04,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1428, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1590/6501 [45:36<2:06:04,  1.54s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1591/6501 [45:37<1:51:12,  1.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5312, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1591/6501 [45:37<1:51:12,  1.36s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1592/6501 [45:39<1:48:47,  1.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0875, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 1592/6501 [45:39<1:48:47,  1.33s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:09,837] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1593/6501 [45:41<2:04:51,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7919, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1593/6501 [45:41<2:04:51,  1.53s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1594/6501 [45:42<2:06:22,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9561, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1594/6501 [45:42<2:06:22,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:13,106] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1595/6501 [45:44<2:09:38,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1531, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1595/6501 [45:44<2:09:38,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:14,650] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1596/6501 [45:45<2:08:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1712, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1596/6501 [45:45<2:08:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:16,335] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1597/6501 [45:47<2:11:19,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.534, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1597/6501 [45:47<2:11:19,  1.61s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1598/6501 [45:49<2:12:32,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9722, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1598/6501 [45:49<2:12:32,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:19,707] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1599/6501 [45:50<2:14:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.101, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1599/6501 [45:50<2:14:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1600/6501 [45:52<2:21:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5522, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1600/6501 [45:52<2:21:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:23,401] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1601/6501 [45:54<2:22:24,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.116, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1601/6501 [45:54<2:22:24,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:25,210] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1602/6501 [45:56<2:23:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0902, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1602/6501 [45:56<2:23:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:27,113] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1603/6501 [45:58<2:27:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4565, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1603/6501 [45:58<2:27:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:28,974] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1604/6501 [46:00<2:28:41,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.676, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1604/6501 [46:00<2:28:41,  1.82s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1605/6501 [46:02<2:36:03,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1974, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1605/6501 [46:02<2:36:03,  1.91s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1606/6501 [46:04<2:43:32,  2.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1704, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1606/6501 [46:04<2:43:32,  2.00s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1607/6501 [46:06<2:35:03,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9757, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1607/6501 [46:06<2:35:03,  1.90s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1608/6501 [46:08<2:34:24,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1511, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1608/6501 [46:08<2:34:24,  1.89s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1609/6501 [46:09<2:28:24,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1125, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1609/6501 [46:09<2:28:24,  1.82s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1610/6501 [46:11<2:27:45,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1875, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1610/6501 [46:11<2:27:45,  1.81s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1611/6501 [46:12<2:15:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.27, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1611/6501 [46:12<2:15:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1612/6501 [46:13<1:56:51,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.329, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1612/6501 [46:13<1:56:51,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:44,385] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1613/6501 [46:15<2:07:37,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1694, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1613/6501 [46:15<2:07:37,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:45,982] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1614/6501 [46:17<2:08:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1094, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1614/6501 [46:17<2:08:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:47,837] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1615/6501 [46:19<2:15:07,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3756, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1615/6501 [46:19<2:15:07,  1.66s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1616/6501 [46:20<2:11:00,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1638, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1616/6501 [46:20<2:11:00,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:51,277] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1617/6501 [46:22<2:19:16,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0576, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1617/6501 [46:22<2:19:16,  1.71s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1618/6501 [46:24<2:16:55,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4059, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1618/6501 [46:24<2:16:55,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:54,688] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1619/6501 [46:25<2:19:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.166, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1619/6501 [46:25<2:19:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:56,049] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1620/6501 [46:27<2:10:56,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2207, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1620/6501 [46:27<2:10:56,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:05:57,870] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1621/6501 [46:29<2:16:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0181, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1621/6501 [46:29<2:16:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1622/6501 [46:30<2:16:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1885, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1622/6501 [46:30<2:16:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:01,486] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1623/6501 [46:32<2:22:28,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.103, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1623/6501 [46:32<2:22:28,  1.75s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1624/6501 [46:34<2:17:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0394, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1624/6501 [46:34<2:17:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:04,727] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1625/6501 [46:35<2:17:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0717, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 1625/6501 [46:35<2:17:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1626/6501 [46:37<2:09:50,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5072, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1626/6501 [46:37<2:09:50,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:07,776] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1627/6501 [46:38<2:11:33,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0596, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1627/6501 [46:38<2:11:33,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:09,561] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1628/6501 [46:40<2:15:33,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.287, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1628/6501 [46:40<2:15:33,  1.67s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1629/6501 [46:42<2:15:04,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1169, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1629/6501 [46:42<2:15:04,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:12,766] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1630/6501 [46:43<2:12:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1161, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1630/6501 [46:43<2:12:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:14,397] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1631/6501 [46:45<2:12:22,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3497, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1631/6501 [46:45<2:12:22,  1.63s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1632/6501 [46:47<2:13:10,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1831, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1632/6501 [46:47<2:13:10,  1.64s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1633/6501 [46:48<1:58:45,  1.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2043, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1633/6501 [46:48<1:58:45,  1.46s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:18,852] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1634/6501 [46:50<2:05:27,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0139, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1634/6501 [46:50<2:05:27,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:20,673] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1635/6501 [46:51<2:12:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3278, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1635/6501 [46:51<2:12:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1636/6501 [46:53<2:14:23,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0842, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1636/6501 [46:53<2:14:23,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:24,286] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1637/6501 [46:55<2:19:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3665, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1637/6501 [46:55<2:19:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:25,990] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1638/6501 [46:57<2:19:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3653, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1638/6501 [46:57<2:19:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1639/6501 [46:58<2:18:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1639/6501 [46:58<2:18:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5908, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:29,547] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1640/6501 [47:00<2:22:24,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4663, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1640/6501 [47:00<2:22:24,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:31,326] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1641/6501 [47:02<2:22:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1955, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1641/6501 [47:02<2:22:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:33,130] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1642/6501 [47:04<2:23:50,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2078, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1642/6501 [47:04<2:23:50,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:34,888] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1643/6501 [47:06<2:23:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0124, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1643/6501 [47:06<2:23:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1644/6501 [47:07<2:20:30,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0342, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1644/6501 [47:07<2:20:30,  1.74s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1645/6501 [47:09<2:18:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1412, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1645/6501 [47:09<2:18:29,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:39,885] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1646/6501 [47:11<2:17:55,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1866, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1646/6501 [47:11<2:17:55,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:41,922] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1647/6501 [47:13<2:25:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1751, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1647/6501 [47:13<2:25:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:43,846] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1648/6501 [47:15<2:28:50,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1532, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1648/6501 [47:15<2:28:50,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:45,742] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1649/6501 [47:16<2:30:09,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6235, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1649/6501 [47:16<2:30:09,  1.86s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1650/6501 [47:18<2:31:07,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.235, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1650/6501 [47:18<2:31:07,  1.87s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1651/6501 [47:20<2:26:14,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4482, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1651/6501 [47:20<2:26:14,  1.81s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1652/6501 [47:22<2:18:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9595, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1652/6501 [47:22<2:18:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:52,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1653/6501 [47:23<2:16:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1656, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1653/6501 [47:23<2:16:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:54,044] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1654/6501 [47:25<2:14:23,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1549, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1654/6501 [47:25<2:14:23,  1.66s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1655/6501 [47:26<2:12:45,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5463, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1655/6501 [47:26<2:12:45,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:57,326] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1656/6501 [47:28<2:13:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9603, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1656/6501 [47:28<2:13:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:06:58,910] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1657/6501 [47:30<2:11:57,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4256, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 1657/6501 [47:30<2:11:57,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:00,726] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1658/6501 [47:31<2:16:19,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8044, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1658/6501 [47:31<2:16:19,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:02,595] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1659/6501 [47:33<2:20:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3399, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1659/6501 [47:33<2:20:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:04,321] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1660/6501 [47:35<2:20:13,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1206, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1660/6501 [47:35<2:20:13,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:05,825] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1661/6501 [47:37<2:14:30,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8985, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1661/6501 [47:37<2:14:30,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:07,703] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1662/6501 [47:38<2:19:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1742, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1662/6501 [47:38<2:19:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1663/6501 [47:40<2:12:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4697, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1663/6501 [47:40<2:12:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1664/6501 [47:42<2:19:36,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.526, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1664/6501 [47:42<2:19:36,  1.73s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1665/6501 [47:43<2:16:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9839, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1665/6501 [47:43<2:16:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:14,600] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1666/6501 [47:45<2:21:38,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2054, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1666/6501 [47:45<2:21:38,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:16,350] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1667/6501 [47:47<2:21:25,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0321, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1667/6501 [47:47<2:21:25,  1.76s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1668/6501 [47:49<2:18:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.805, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1668/6501 [47:49<2:18:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:19,329] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1669/6501 [47:50<2:09:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4324, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1669/6501 [47:50<2:09:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:21,272] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1670/6501 [47:52<2:17:26,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2624, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1670/6501 [47:52<2:17:26,  1.71s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1671/6501 [47:53<2:06:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4399, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1671/6501 [47:53<2:06:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:24,156] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1672/6501 [47:55<2:07:44,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1387, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1672/6501 [47:55<2:07:44,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:26,023] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1673/6501 [47:57<2:14:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0047, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1673/6501 [47:57<2:14:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:28,135] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1674/6501 [47:59<2:25:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2676, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1674/6501 [47:59<2:25:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:29,874] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1675/6501 [48:01<2:23:29,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0731, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1675/6501 [48:01<2:23:29,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1676/6501 [48:02<2:19:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1508, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1676/6501 [48:02<2:19:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:33,680] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1677/6501 [48:04<2:30:24,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.382, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1677/6501 [48:04<2:30:24,  1.87s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1678/6501 [48:06<2:12:32,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9943, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1678/6501 [48:06<2:12:32,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:36,572] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1679/6501 [48:07<2:15:13,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0625, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1679/6501 [48:07<2:15:13,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:38,234] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1680/6501 [48:09<2:14:41,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1198, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1680/6501 [48:09<2:14:41,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:39,998] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1681/6501 [48:11<2:16:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1655, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1681/6501 [48:11<2:16:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:42,156] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1682/6501 [48:13<2:27:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4214, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1682/6501 [48:13<2:27:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:43,925] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1683/6501 [48:15<2:25:59,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0659, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1683/6501 [48:15<2:25:59,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:45,679] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1684/6501 [48:16<2:24:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2739, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1684/6501 [48:16<2:24:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1685/6501 [48:18<2:19:18,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0101, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1685/6501 [48:18<2:19:18,  1.74s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1686/6501 [48:20<2:16:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3889, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1686/6501 [48:20<2:16:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1687/6501 [48:22<2:22:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9624, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1687/6501 [48:22<2:22:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:52,614] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1688/6501 [48:23<2:22:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3409, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1688/6501 [48:23<2:22:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1689/6501 [48:24<2:05:44,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3405, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1689/6501 [48:24<2:05:44,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:55,401] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1690/6501 [48:26<2:08:53,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7393, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1690/6501 [48:26<2:08:53,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:07:57,094] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1691/6501 [48:28<2:10:56,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9206, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1691/6501 [48:28<2:10:56,  1.63s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1692/6501 [48:30<2:15:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0734, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1692/6501 [48:30<2:15:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:00,958] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1693/6501 [48:32<2:23:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.123, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1693/6501 [48:32<2:23:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:02,633] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1694/6501 [48:33<2:20:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2165, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1694/6501 [48:33<2:20:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:04,720] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1695/6501 [48:35<2:28:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2127, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1695/6501 [48:35<2:28:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:06,522] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1696/6501 [48:37<2:27:24,  1.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1696/6501 [48:37<2:27:24,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9633, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:08,331] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1697/6501 [48:39<2:26:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4653, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1697/6501 [48:39<2:26:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:10,052] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1698/6501 [48:41<2:23:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1521, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1698/6501 [48:41<2:23:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:12,069] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1699/6501 [48:43<2:29:09,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1303, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1699/6501 [48:43<2:29:09,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1700/6501 [48:45<2:35:37,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4648, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1700/6501 [48:45<2:35:37,  1.94s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:16,365] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1701/6501 [48:47<2:40:48,  2.01s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1402, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1701/6501 [48:47<2:40:48,  2.01s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1702/6501 [48:49<2:31:48,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3472, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1702/6501 [48:49<2:31:48,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:19,890] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1703/6501 [48:51<2:31:32,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4235, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1703/6501 [48:51<2:31:32,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:21,674] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1704/6501 [48:52<2:28:50,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2987, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1704/6501 [48:52<2:28:50,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:23,568] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1705/6501 [48:54<2:29:34,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3792, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1705/6501 [48:54<2:29:34,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:25,409] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1706/6501 [48:56<2:28:49,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6144, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 1706/6501 [48:56<2:28:49,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1707/6501 [48:58<2:27:32,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9827, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1707/6501 [48:58<2:27:32,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:29,157] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1708/6501 [49:00<2:29:43,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9506, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1708/6501 [49:00<2:29:43,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:30,743] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1709/6501 [49:01<2:22:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6182, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1709/6501 [49:01<2:22:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:32,637] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1710/6501 [49:03<2:25:17,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0005, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1710/6501 [49:03<2:25:17,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:34,533] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1711/6501 [49:05<2:27:05,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2465, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1711/6501 [49:05<2:27:05,  1.84s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1712/6501 [49:07<2:21:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4859, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1712/6501 [49:07<2:21:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:37,840] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1713/6501 [49:09<2:19:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1992, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1713/6501 [49:09<2:19:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:39,972] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1714/6501 [49:11<2:28:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0399, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1714/6501 [49:11<2:28:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:41,705] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1715/6501 [49:12<2:25:36,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2073, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1715/6501 [49:12<2:25:36,  1.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1716/6501 [49:14<2:26:18,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0557, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1716/6501 [49:14<2:26:18,  1.83s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1717/6501 [49:16<2:18:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2738, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1717/6501 [49:16<2:18:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:46,795] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1718/6501 [49:17<2:18:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0411, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1718/6501 [49:17<2:18:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:48,526] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1719/6501 [49:19<2:18:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4282, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1719/6501 [49:19<2:18:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:50,078] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1720/6501 [49:21<2:13:41,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.114, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1720/6501 [49:21<2:13:41,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:51,738] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1721/6501 [49:22<2:13:14,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3456, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1721/6501 [49:22<2:13:14,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:08:53,482] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1722/6501 [49:24<2:14:55,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3161, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 1722/6501 [49:24<2:14:55,  1.69s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1723/6501 [49:26<2:11:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4441, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1723/6501 [49:26<2:11:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1724/6501 [49:27<2:11:53,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1493, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1724/6501 [49:27<2:11:53,  1.66s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1725/6501 [49:29<2:12:51,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5494, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1725/6501 [49:29<2:12:51,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:00,032] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1726/6501 [49:31<2:11:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0154, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1726/6501 [49:31<2:11:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1727/6501 [49:33<2:15:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1289, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1727/6501 [49:33<2:15:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1728/6501 [49:34<2:18:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2017, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1728/6501 [49:34<2:18:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:05,401] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1729/6501 [49:36<2:18:17,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8529, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1729/6501 [49:36<2:18:17,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:07,164] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1730/6501 [49:38<2:18:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1638, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1730/6501 [49:38<2:18:50,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:09,112] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1731/6501 [49:40<2:23:37,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9246, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1731/6501 [49:40<2:23:37,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:10,994] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1732/6501 [49:42<2:25:23,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0162, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1732/6501 [49:42<2:25:23,  1.83s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1733/6501 [49:44<2:26:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4247, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1733/6501 [49:44<2:26:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:14,516] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1734/6501 [49:45<2:21:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9256, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1734/6501 [49:45<2:21:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:16,306] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1735/6501 [49:47<2:21:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7288, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1735/6501 [49:47<2:21:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:18,208] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1736/6501 [49:49<2:24:35,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2263, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1736/6501 [49:49<2:24:35,  1.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1737/6501 [49:51<2:24:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9163, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1737/6501 [49:51<2:24:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:22,022] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1738/6501 [49:53<2:28:46,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.918, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1738/6501 [49:53<2:28:46,  1.87s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1739/6501 [49:55<2:31:49,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2073, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1739/6501 [49:55<2:31:49,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:25,810] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1740/6501 [49:57<2:28:43,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6581, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1740/6501 [49:57<2:28:43,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:27,518] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1741/6501 [49:58<2:24:44,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1741/6501 [49:58<2:24:44,  1.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1742/6501 [50:00<2:19:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.096, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1742/6501 [50:00<2:19:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:30,370] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1743/6501 [50:01<2:07:25,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.111, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1743/6501 [50:01<2:07:25,  1.61s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1744/6501 [50:03<2:15:24,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9608, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1744/6501 [50:03<2:15:24,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:34,001] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1745/6501 [50:05<2:14:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1214, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1745/6501 [50:05<2:14:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:35,586] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1746/6501 [50:06<2:12:04,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1888, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1746/6501 [50:06<2:12:04,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:37,297] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1747/6501 [50:08<2:13:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8729, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1747/6501 [50:08<2:13:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:39,000] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1748/6501 [50:10<2:13:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1904, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1748/6501 [50:10<2:13:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1749/6501 [50:11<2:08:17,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7958, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1749/6501 [50:11<2:08:17,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:42,119] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1750/6501 [50:13<2:09:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3777, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1750/6501 [50:13<2:09:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1751/6501 [50:14<2:09:52,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4212, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1751/6501 [50:14<2:09:52,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:45,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1752/6501 [50:16<2:10:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8991, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1752/6501 [50:16<2:10:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:47,147] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1753/6501 [50:18<2:11:35,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9562, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1753/6501 [50:18<2:11:35,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:48,937] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1754/6501 [50:20<2:14:36,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0076, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1754/6501 [50:20<2:14:36,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:50,833] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1755/6501 [50:22<2:19:10,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2487, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1755/6501 [50:22<2:19:10,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:52,942] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1756/6501 [50:24<2:27:26,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4637, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1756/6501 [50:24<2:27:26,  1.86s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1757/6501 [50:25<2:26:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1852, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1757/6501 [50:25<2:26:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1758/6501 [50:27<2:07:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2837, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1758/6501 [50:27<2:07:17,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:09:57,769] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1759/6501 [50:28<2:15:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2899, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1759/6501 [50:28<2:15:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1760/6501 [50:30<2:14:29,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1907, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1760/6501 [50:30<2:14:29,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:01,137] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1761/6501 [50:32<2:14:19,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2807, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1761/6501 [50:32<2:14:19,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:02,804] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1762/6501 [50:34<2:13:30,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1749, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1762/6501 [50:34<2:13:30,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:04,351] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1763/6501 [50:35<2:10:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2795, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1763/6501 [50:35<2:10:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1764/6501 [50:37<2:13:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0965, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1764/6501 [50:37<2:13:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:07,678] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1765/6501 [50:38<2:09:42,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1821, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1765/6501 [50:38<2:09:42,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:09,565] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1766/6501 [50:40<2:15:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6313, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1766/6501 [50:40<2:15:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:11,224] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1767/6501 [50:42<2:14:04,  1.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1767/6501 [50:42<2:14:04,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2993, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1768/6501 [50:44<2:19:06,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9868, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1768/6501 [50:44<2:19:06,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:15,059] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1769/6501 [50:46<2:22:49,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.347, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1769/6501 [50:46<2:22:49,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:17,112] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1770/6501 [50:48<2:28:29,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2422, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1770/6501 [50:48<2:28:29,  1.88s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1771/6501 [50:50<2:26:34,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1135, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1771/6501 [50:50<2:26:34,  1.86s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1772/6501 [50:51<2:22:28,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1042, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1772/6501 [50:51<2:22:28,  1.81s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1773/6501 [50:53<2:25:00,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3275, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1773/6501 [50:53<2:25:00,  1.84s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1774/6501 [50:55<2:18:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4623, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1774/6501 [50:55<2:18:09,  1.75s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1775/6501 [50:56<1:58:58,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8618, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1775/6501 [50:56<1:58:58,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:27,193] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1776/6501 [50:58<2:14:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5044, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1776/6501 [50:58<2:14:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1777/6501 [50:59<2:11:27,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3042, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1777/6501 [50:59<2:11:27,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:30,489] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1778/6501 [51:01<2:12:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.146, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1778/6501 [51:01<2:12:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:32,593] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1779/6501 [51:03<2:22:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2791, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1779/6501 [51:03<2:22:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:34,675] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1780/6501 [51:05<2:28:54,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3585, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1780/6501 [51:05<2:28:54,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:36,490] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1781/6501 [51:07<2:27:02,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3971, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1781/6501 [51:07<2:27:02,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:38,255] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1782/6501 [51:09<2:24:33,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1548, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1782/6501 [51:09<2:24:33,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:39,987] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1783/6501 [51:11<2:22:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4209, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1783/6501 [51:11<2:22:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:41,584] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1784/6501 [51:12<2:17:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1712, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1784/6501 [51:12<2:17:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:43,352] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1785/6501 [51:14<2:17:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2212, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1785/6501 [51:14<2:17:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1786/6501 [51:16<2:20:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3098, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1786/6501 [51:16<2:20:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1787/6501 [51:18<2:18:00,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3687, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 1787/6501 [51:18<2:18:00,  1.76s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1788/6501 [51:19<2:18:54,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0387, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1788/6501 [51:19<2:18:54,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:50,689] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1789/6501 [51:21<2:24:02,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.225, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1789/6501 [51:21<2:24:02,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:52,165] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1790/6501 [51:23<2:15:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3307, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1790/6501 [51:23<2:15:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:53,981] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1791/6501 [51:25<2:17:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2496, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1791/6501 [51:25<2:17:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:55,687] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1792/6501 [51:26<2:16:30,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9556, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1792/6501 [51:26<2:16:30,  1.74s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1793/6501 [51:28<2:18:35,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3087, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1793/6501 [51:28<2:18:35,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:10:59,310] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1794/6501 [51:30<2:19:11,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2726, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1794/6501 [51:30<2:19:11,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:00,968] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1795/6501 [51:32<2:16:26,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0541, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1795/6501 [51:32<2:16:26,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:02,898] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1796/6501 [51:34<2:20:53,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1894, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1796/6501 [51:34<2:20:53,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:04,636] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1797/6501 [51:35<2:19:28,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1544, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1797/6501 [51:35<2:19:28,  1.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1798/6501 [51:37<2:18:30,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0665, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1798/6501 [51:37<2:18:30,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:08,194] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1799/6501 [51:39<2:19:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0129, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1799/6501 [51:39<2:19:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:09,831] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1800/6501 [51:41<2:16:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1807, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1800/6501 [51:41<2:16:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1801/6501 [51:42<2:15:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3442, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1801/6501 [51:42<2:15:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:13,241] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1802/6501 [51:44<2:14:53,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5276, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1802/6501 [51:44<2:14:53,  1.72s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1803/6501 [51:45<2:08:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0516, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1803/6501 [51:45<2:08:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:16,859] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1804/6501 [51:48<2:20:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1249, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1804/6501 [51:48<2:20:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:18,746] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1805/6501 [51:49<2:22:45,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1235, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1805/6501 [51:49<2:22:45,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:20,790] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1806/6501 [51:51<2:27:54,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7648, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1806/6501 [51:51<2:27:54,  1.89s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1807/6501 [51:53<2:19:27,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2554, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1807/6501 [51:53<2:19:27,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:24,320] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1808/6501 [51:55<2:24:29,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2455, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1808/6501 [51:55<2:24:29,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:26,168] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1809/6501 [51:57<2:24:28,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0789, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1809/6501 [51:57<2:24:28,  1.85s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1810/6501 [51:58<2:07:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2035, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1810/6501 [51:58<2:07:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:29,226] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1811/6501 [52:00<2:14:24,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0545, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1811/6501 [52:00<2:14:24,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:31,049] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1812/6501 [52:02<2:16:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0232, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1812/6501 [52:02<2:16:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1813/6501 [52:04<2:21:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4775, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1813/6501 [52:04<2:21:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1814/6501 [52:05<2:17:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.168, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1814/6501 [52:05<2:17:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1815/6501 [52:07<2:09:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0641, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1815/6501 [52:07<2:09:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1816/6501 [52:08<2:10:05,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1195, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1816/6501 [52:08<2:10:05,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:39,462] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1817/6501 [52:10<2:11:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2269, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1817/6501 [52:10<2:11:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:41,149] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1818/6501 [52:12<2:11:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1818/6501 [52:12<2:11:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.318, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1819/6501 [52:13<2:09:41,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4105, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1819/6501 [52:13<2:09:41,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:44,692] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1820/6501 [52:15<2:15:52,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5205, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1820/6501 [52:15<2:15:52,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:46,573] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1821/6501 [52:17<2:19:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3237, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1821/6501 [52:17<2:19:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:48,541] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1822/6501 [52:19<2:23:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3062, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1822/6501 [52:19<2:23:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:50,376] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1823/6501 [52:21<2:23:17,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1581, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1823/6501 [52:21<2:23:17,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:52,116] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1824/6501 [52:23<2:20:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5661, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1824/6501 [52:23<2:20:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:53,834] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1825/6501 [52:25<2:18:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0948, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1825/6501 [52:25<2:18:49,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:55,688] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1826/6501 [52:26<2:20:29,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1907, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1826/6501 [52:26<2:20:29,  1.80s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1827/6501 [52:28<2:15:33,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7988, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1827/6501 [52:28<2:15:33,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:11:59,053] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1828/6501 [52:30<2:16:15,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1512, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1828/6501 [52:30<2:16:15,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:00,952] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1829/6501 [52:32<2:19:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4373, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1829/6501 [52:32<2:19:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:02,743] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1830/6501 [52:33<2:19:35,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3189, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1830/6501 [52:33<2:19:35,  1.79s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1831/6501 [52:35<2:12:44,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.334, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1831/6501 [52:35<2:12:44,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:06,022] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1832/6501 [52:37<2:14:25,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3008, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1832/6501 [52:37<2:14:25,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:07,611] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1833/6501 [52:38<2:11:10,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0374, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1833/6501 [52:38<2:11:10,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:09,209] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1834/6501 [52:40<2:09:04,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2516, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1834/6501 [52:40<2:09:04,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1835/6501 [52:42<2:09:10,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2256, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1835/6501 [52:42<2:09:10,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1836/6501 [52:43<2:09:57,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.962, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1836/6501 [52:43<2:09:57,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:14,540] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1837/6501 [52:45<2:16:53,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1747, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1837/6501 [52:45<2:16:53,  1.76s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1838/6501 [52:47<2:19:51,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4474, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1838/6501 [52:47<2:19:51,  1.80s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1839/6501 [52:49<2:18:17,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0541, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1839/6501 [52:49<2:18:17,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:19,932] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1840/6501 [52:51<2:18:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2346, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1840/6501 [52:51<2:18:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:22,080] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1841/6501 [52:53<2:26:38,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2916, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1841/6501 [52:53<2:26:38,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:23,628] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1842/6501 [52:54<2:18:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0195, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1842/6501 [52:54<2:18:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:25,220] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1843/6501 [52:56<2:14:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2285, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1843/6501 [52:56<2:14:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:27,041] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1844/6501 [52:58<2:16:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.165, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1844/6501 [52:58<2:16:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1845/6501 [53:00<2:17:30,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.404, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1845/6501 [53:00<2:17:30,  1.77s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1846/6501 [53:01<2:17:57,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2706, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1846/6501 [53:01<2:17:57,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:32,230] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1847/6501 [53:03<2:13:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2906, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1847/6501 [53:03<2:13:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:34,028] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1848/6501 [53:05<2:15:13,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0854, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1848/6501 [53:05<2:15:13,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:35,861] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1849/6501 [53:07<2:17:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1468, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1849/6501 [53:07<2:17:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:37,809] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1850/6501 [53:09<2:21:23,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4536, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1850/6501 [53:09<2:21:23,  1.82s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1851/6501 [53:10<2:21:18,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4361, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1851/6501 [53:10<2:21:18,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:41,292] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1852/6501 [53:12<2:17:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2125, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 1852/6501 [53:12<2:17:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:43,192] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1853/6501 [53:14<2:20:23,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3152, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1853/6501 [53:14<2:20:23,  1.81s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1854/6501 [53:16<2:18:21,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.226, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1854/6501 [53:16<2:18:21,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:46,732] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1855/6501 [53:17<2:18:58,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7599, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1855/6501 [53:17<2:18:58,  1.79s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1856/6501 [53:19<2:14:19,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9141, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1856/6501 [53:19<2:14:19,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:50,046] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1857/6501 [53:21<2:13:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.85, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1857/6501 [53:21<2:13:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:51,639] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1858/6501 [53:22<2:10:41,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0932, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1858/6501 [53:22<2:10:41,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:53,431] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1859/6501 [53:24<2:13:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3917, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1859/6501 [53:24<2:13:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1860/6501 [53:26<2:14:38,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.448, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1860/6501 [53:26<2:14:38,  1.74s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1861/6501 [53:27<2:08:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2671, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1861/6501 [53:27<2:08:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:12:58,678] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1862/6501 [53:29<2:15:43,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.194, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1862/6501 [53:29<2:15:43,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:00,366] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1863/6501 [53:31<2:14:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4614, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1863/6501 [53:31<2:14:07,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:01,973] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1864/6501 [53:33<2:11:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9457, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1864/6501 [53:33<2:11:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1865/6501 [53:35<2:16:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1377, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1865/6501 [53:35<2:16:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:05,614] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1866/6501 [53:36<2:15:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1866/6501 [53:36<2:15:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1867/6501 [53:38<2:13:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.068, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1867/6501 [53:38<2:13:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:08,921] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1868/6501 [53:40<2:11:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3256, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1868/6501 [53:40<2:11:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:11,049] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1869/6501 [53:42<2:21:01,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3038, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 1869/6501 [53:42<2:21:01,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:12,793] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1870/6501 [53:43<2:19:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1870/6501 [53:43<2:19:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0715, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1871/6501 [53:45<2:13:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3036, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1871/6501 [53:45<2:13:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:16,321] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1872/6501 [53:47<2:18:50,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1549, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1872/6501 [53:47<2:18:50,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:17,820] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1873/6501 [53:49<2:11:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3981, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1873/6501 [53:49<2:11:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1874/6501 [53:49<1:54:22,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4102, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1874/6501 [53:49<1:54:22,  1.48s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:20,519] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1875/6501 [53:51<2:00:23,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0746, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1875/6501 [53:51<2:00:23,  1.56s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1876/6501 [53:53<2:03:42,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8528, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1876/6501 [53:53<2:03:42,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:24,196] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1877/6501 [53:55<2:12:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2403, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1877/6501 [53:55<2:12:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:25,905] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1878/6501 [53:57<2:11:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1878/6501 [53:57<2:11:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0514, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:27,493] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1879/6501 [53:58<2:09:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.437, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1879/6501 [53:58<2:09:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:29,665] [WARNING] [stage3.py:1787:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1880/6501 [54:00<2:20:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1880/6501 [54:00<2:20:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2421, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1881/6501 [54:02<2:20:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.394, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1881/6501 [54:02<2:20:52,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:33,824] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1882/6501 [54:05<2:32:06,  1.98s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0039, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1882/6501 [54:05<2:32:06,  1.98s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1883/6501 [54:06<2:22:08,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.962, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1883/6501 [54:06<2:22:08,  1.85s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1884/6501 [54:08<2:21:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2709, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1884/6501 [54:08<2:21:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:38,977] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1885/6501 [54:10<2:20:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1064, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1885/6501 [54:10<2:20:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1886/6501 [54:12<2:21:41,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1204, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1886/6501 [54:12<2:21:41,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:42,555] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1887/6501 [54:13<2:18:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.136, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1887/6501 [54:13<2:18:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:44,330] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1888/6501 [54:15<2:17:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2389, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1888/6501 [54:15<2:17:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1889/6501 [54:17<2:13:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5156, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1889/6501 [54:17<2:13:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:47,833] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1890/6501 [54:19<2:17:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1303, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1890/6501 [54:19<2:17:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1891/6501 [54:20<2:18:20,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1604, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1891/6501 [54:20<2:18:20,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:50,826] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1892/6501 [54:22<2:03:26,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2095, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1892/6501 [54:22<2:03:26,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:52,433] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1893/6501 [54:23<2:03:24,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2751, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1893/6501 [54:23<2:03:24,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:13:54,274] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1894/6501 [54:25<2:08:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0555, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1894/6501 [54:25<2:08:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1895/6501 [54:27<2:06:59,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1598, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1895/6501 [54:27<2:06:59,  1.65s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1896/6501 [54:28<2:11:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.282, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1896/6501 [54:28<2:11:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1897/6501 [54:30<2:08:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1395, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1897/6501 [54:30<2:08:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1898/6501 [54:32<2:11:31,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2623, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1898/6501 [54:32<2:11:31,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:02,913] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1899/6501 [54:34<2:13:22,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2234, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1899/6501 [54:34<2:13:22,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:04,783] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1900/6501 [54:35<2:16:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6546, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1900/6501 [54:35<2:16:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:06,377] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1901/6501 [54:37<2:12:05,  1.72s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1901/6501 [54:37<2:12:05,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1411, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1902/6501 [54:39<2:12:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2041, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1902/6501 [54:39<2:12:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1903/6501 [54:41<2:19:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.11, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1903/6501 [54:41<2:19:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:11,806] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1904/6501 [54:43<2:15:37,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3277, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1904/6501 [54:43<2:15:37,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:13,756] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1905/6501 [54:44<2:19:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0742, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1905/6501 [54:44<2:19:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:15,690] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1906/6501 [54:46<2:22:12,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1318, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1906/6501 [54:46<2:22:12,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:17,520] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1907/6501 [54:48<2:21:33,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3381, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1907/6501 [54:48<2:21:33,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:19,243] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1908/6501 [54:50<2:18:38,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5015, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1908/6501 [54:50<2:18:38,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:21,271] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1909/6501 [54:52<2:23:36,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4797, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1909/6501 [54:52<2:23:36,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:23,122] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1910/6501 [54:54<2:22:59,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4073, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1910/6501 [54:54<2:22:59,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:25,284] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1911/6501 [54:56<2:29:41,  1.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3418, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1911/6501 [54:56<2:29:41,  1.96s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:26,929] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1912/6501 [54:58<2:22:29,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2914, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1912/6501 [54:58<2:22:29,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:28,696] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1913/6501 [54:59<2:20:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0739, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1913/6501 [54:59<2:20:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:30,610] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1914/6501 [55:01<2:22:03,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1573, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1914/6501 [55:01<2:22:03,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:32,520] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1915/6501 [55:03<2:23:12,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2149, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1915/6501 [55:03<2:23:12,  1.87s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1916/6501 [55:05<2:26:45,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.758, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1916/6501 [55:05<2:26:45,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:36,488] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1917/6501 [55:07<2:27:06,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2139, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 1917/6501 [55:07<2:27:06,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:38,537] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1918/6501 [55:09<2:29:55,  1.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6199, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1918/6501 [55:09<2:29:55,  1.96s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1919/6501 [55:11<2:25:32,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3283, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1919/6501 [55:11<2:25:32,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:42,245] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1920/6501 [55:13<2:26:09,  1.91s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1920/6501 [55:13<2:26:09,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1246, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:44,052] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1921/6501 [55:15<2:23:41,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5232, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1921/6501 [55:15<2:23:41,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:45,741] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1922/6501 [55:16<2:19:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5621, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1922/6501 [55:16<2:19:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1923/6501 [55:18<2:18:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1074, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1923/6501 [55:18<2:18:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:49,320] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1924/6501 [55:20<2:17:45,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4768, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1924/6501 [55:20<2:17:45,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:51,065] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1925/6501 [55:22<2:16:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3223, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1925/6501 [55:22<2:16:20,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:52,513] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1926/6501 [55:23<2:08:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9911, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1926/6501 [55:23<2:08:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:54,208] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1927/6501 [55:25<2:08:43,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1252, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1927/6501 [55:25<2:08:43,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:56,049] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1928/6501 [55:27<2:12:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1681, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1928/6501 [55:27<2:12:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:57,750] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1929/6501 [55:28<2:11:23,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1864, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1929/6501 [55:28<2:11:23,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:14:59,518] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1930/6501 [55:30<2:12:20,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.478, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1930/6501 [55:30<2:12:20,  1.74s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1931/6501 [55:32<2:09:04,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.449, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1931/6501 [55:32<2:09:04,  1.69s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1932/6501 [55:34<2:13:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0364, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1932/6501 [55:34<2:13:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:04,824] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1933/6501 [55:36<2:15:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5213, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1933/6501 [55:36<2:15:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1934/6501 [55:37<2:11:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1695, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1934/6501 [55:37<2:11:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1935/6501 [55:39<2:08:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3114, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1935/6501 [55:39<2:08:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:10,084] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1936/6501 [55:41<2:16:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.997, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1936/6501 [55:41<2:16:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:12,213] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1937/6501 [55:43<2:24:18,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2833, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1937/6501 [55:43<2:24:18,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:14,233] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1938/6501 [55:45<2:27:04,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6687, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1938/6501 [55:45<2:27:04,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:16,146] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1939/6501 [55:47<2:26:33,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0912, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1939/6501 [55:47<2:26:33,  1.93s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1940/6501 [55:48<2:13:57,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8969, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1940/6501 [55:48<2:13:57,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:19,338] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1941/6501 [55:50<2:15:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9394, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1941/6501 [55:50<2:15:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1942/6501 [55:52<2:12:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3267, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1942/6501 [55:52<2:12:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:22,863] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1943/6501 [55:54<2:15:02,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9249, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1943/6501 [55:54<2:15:02,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:24,544] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1944/6501 [55:55<2:12:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1145, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1944/6501 [55:55<2:12:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:26,467] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1945/6501 [55:57<2:16:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4756, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1945/6501 [55:57<2:16:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1946/6501 [55:59<2:11:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9751, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1946/6501 [55:59<2:11:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:29,864] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1947/6501 [56:01<2:13:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4943, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1947/6501 [56:01<2:13:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:31,566] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1948/6501 [56:02<2:12:10,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.228, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1948/6501 [56:02<2:12:10,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:33,647] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1949/6501 [56:04<2:19:51,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0253, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1949/6501 [56:04<2:19:51,  1.84s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1950/6501 [56:06<2:05:09,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2793, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 1950/6501 [56:06<2:05:09,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:36,769] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1951/6501 [56:07<2:11:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9476, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1951/6501 [56:07<2:11:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:38,391] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1952/6501 [56:09<2:08:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8225, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1952/6501 [56:09<2:08:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1953/6501 [56:11<2:11:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4873, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1953/6501 [56:11<2:11:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1954/6501 [56:13<2:09:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.171, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1954/6501 [56:13<2:09:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:43,634] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1955/6501 [56:14<2:11:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3365, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1955/6501 [56:14<2:11:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:45,150] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1956/6501 [56:16<2:06:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1738, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1956/6501 [56:16<2:06:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:47,019] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1957/6501 [56:18<2:10:46,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3186, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1957/6501 [56:18<2:10:46,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:48,637] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1958/6501 [56:19<2:08:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2531, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1958/6501 [56:19<2:08:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:50,294] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1959/6501 [56:21<2:07:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3597, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1959/6501 [56:21<2:07:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1960/6501 [56:23<2:03:56,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4125, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1960/6501 [56:23<2:03:56,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:53,561] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1961/6501 [56:24<2:06:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2084, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1961/6501 [56:24<2:06:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:55,159] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1962/6501 [56:26<2:04:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1183, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1962/6501 [56:26<2:04:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:15:57,027] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1963/6501 [56:28<2:09:32,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5763, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1963/6501 [56:28<2:09:32,  1.71s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1964/6501 [56:29<2:10:39,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2148, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1964/6501 [56:29<2:10:39,  1.73s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1965/6501 [56:31<2:05:37,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5527, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1965/6501 [56:31<2:05:37,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:02,003] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1966/6501 [56:33<2:06:35,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.403, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1966/6501 [56:33<2:06:35,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:03,795] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1967/6501 [56:34<2:09:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2409, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1967/6501 [56:34<2:09:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:05,677] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1968/6501 [56:36<2:13:06,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.204, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1968/6501 [56:36<2:13:06,  1.76s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1969/6501 [56:38<2:13:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9892, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1969/6501 [56:38<2:13:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:09,185] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1970/6501 [56:40<2:12:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9941, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1970/6501 [56:40<2:12:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:11,099] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1971/6501 [56:42<2:16:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1415, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1971/6501 [56:42<2:16:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1972/6501 [56:44<2:18:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.95, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1972/6501 [56:44<2:18:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:14,680] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1973/6501 [56:45<2:14:48,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9679, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1973/6501 [56:45<2:14:48,  1.79s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1974/6501 [56:47<2:13:37,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2196, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1974/6501 [56:47<2:13:37,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:18,132] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1975/6501 [56:49<2:12:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1429, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1975/6501 [56:49<2:12:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:19,961] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1976/6501 [56:51<2:13:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0414, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1976/6501 [56:51<2:13:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:21,854] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1977/6501 [56:53<2:16:37,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8365, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1977/6501 [56:53<2:16:37,  1.81s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1978/6501 [56:54<2:06:50,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.261, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1978/6501 [56:54<2:06:50,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:25,140] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1979/6501 [56:56<2:11:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4284, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1979/6501 [56:56<2:11:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:27,018] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|███       | 1980/6501 [56:58<2:14:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1980/6501 [56:58<2:14:43,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.343, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1981/6501 [57:00<2:15:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0979, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1981/6501 [57:00<2:15:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 1982/6501 [57:00<1:56:13,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9168, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 1982/6501 [57:00<1:56:13,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:31,530] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1983/6501 [57:02<2:00:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8107, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1983/6501 [57:02<2:00:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:33,440] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1984/6501 [57:04<2:07:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0531, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1984/6501 [57:04<2:07:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:35,265] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1985/6501 [57:06<2:10:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 1985/6501 [57:06<2:10:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9711, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1986/6501 [57:07<1:51:56,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.25, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1986/6501 [57:07<1:51:56,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:37,275] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1987/6501 [57:08<1:43:02,  1.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5134, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1987/6501 [57:08<1:43:02,  1.37s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:39,121] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1988/6501 [57:10<1:53:46,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5095, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1988/6501 [57:10<1:53:46,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:40,855] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1989/6501 [57:12<1:58:44,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4509, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1989/6501 [57:12<1:58:44,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:42,965] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1990/6501 [57:14<2:10:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1169, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1990/6501 [57:14<2:10:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 1991/6501 [57:15<2:08:16,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1721, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1991/6501 [57:15<2:08:16,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:46,337] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1992/6501 [57:17<2:09:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3268, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1992/6501 [57:17<2:09:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:48,230] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1993/6501 [57:19<2:12:56,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7784, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1993/6501 [57:19<2:12:56,  1.77s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 1994/6501 [57:21<2:14:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2153, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1994/6501 [57:21<2:14:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:51,777] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1995/6501 [57:22<2:12:40,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3413, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1995/6501 [57:22<2:12:40,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:53,410] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1996/6501 [57:24<2:09:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4077, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1996/6501 [57:24<2:09:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:55,717] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1997/6501 [57:26<2:22:41,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1857, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1997/6501 [57:26<2:22:41,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:57,415] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1998/6501 [57:28<2:18:05,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3249, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1998/6501 [57:28<2:18:05,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:16:59,057] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 1999/6501 [57:30<2:13:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4145, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 1999/6501 [57:30<2:13:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:00,694] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2000/6501 [57:31<2:10:20,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9351, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2000/6501 [57:31<2:10:20,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:02,369] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2001/6501 [57:33<2:08:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5474, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2001/6501 [57:33<2:08:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:04,126] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2002/6501 [57:35<2:09:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1721, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2002/6501 [57:35<2:09:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:05,874] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2003/6501 [57:37<2:10:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1744, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2003/6501 [57:37<2:10:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:08,046] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2004/6501 [57:39<2:19:53,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4502, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2004/6501 [57:39<2:19:53,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:09,717] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2005/6501 [57:40<2:15:27,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3157, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2005/6501 [57:40<2:15:27,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:11,598] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2006/6501 [57:42<2:17:04,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9483, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2006/6501 [57:42<2:17:04,  1.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2007/6501 [57:44<2:07:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1383, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2007/6501 [57:44<2:07:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:14,631] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2008/6501 [57:45<2:05:47,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4733, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2008/6501 [57:45<2:05:47,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:16,161] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2009/6501 [57:47<2:02:23,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7722, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2009/6501 [57:47<2:02:23,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:17,920] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2010/6501 [57:49<2:05:09,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0451, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2010/6501 [57:49<2:05:09,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2011/6501 [57:50<2:05:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8112, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2011/6501 [57:50<2:05:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:21,343] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2012/6501 [57:52<2:06:47,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1598, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2012/6501 [57:52<2:06:47,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:23,216] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2013/6501 [57:54<2:10:46,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8994, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2013/6501 [57:54<2:10:46,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:25,314] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2014/6501 [57:56<2:18:35,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3291, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2014/6501 [57:56<2:18:35,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:27,129] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2015/6501 [57:58<2:17:41,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9727, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2015/6501 [57:58<2:17:41,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:28,855] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2016/6501 [58:00<2:15:04,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1715, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2016/6501 [58:00<2:15:04,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:30,727] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2017/6501 [58:01<2:16:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3549, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2017/6501 [58:01<2:16:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2018/6501 [58:03<2:21:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0648, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2018/6501 [58:03<2:21:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:34,782] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2019/6501 [58:05<2:23:57,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1049, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2019/6501 [58:05<2:23:57,  1.93s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2020/6501 [58:07<2:20:49,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3353, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2020/6501 [58:07<2:20:49,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:38,438] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2021/6501 [58:09<2:20:22,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1405, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2021/6501 [58:09<2:20:22,  1.88s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2022/6501 [58:11<2:15:03,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.308, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2022/6501 [58:11<2:15:03,  1.81s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2023/6501 [58:12<2:08:54,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0959, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2023/6501 [58:12<2:08:54,  1.73s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2024/6501 [58:14<2:14:16,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2585, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2024/6501 [58:14<2:14:16,  1.80s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2025/6501 [58:16<2:07:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.48, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2025/6501 [58:16<2:07:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:46,978] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 2026/6501 [58:18<2:11:31,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2337, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2026/6501 [58:18<2:11:31,  1.76s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2027/6501 [58:19<2:02:10,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3107, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2027/6501 [58:19<2:02:10,  1.64s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2028/6501 [58:21<2:03:16,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1792, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2028/6501 [58:21<2:03:16,  1.65s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2029/6501 [58:22<1:48:09,  1.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2545, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2029/6501 [58:22<1:48:09,  1.45s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2030/6501 [58:23<1:55:39,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3528, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2030/6501 [58:23<1:55:39,  1.55s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 2031/6501 [58:24<1:42:51,  1.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3081, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 2031/6501 [58:24<1:42:51,  1.38s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:55,642] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2032/6501 [58:26<1:54:00,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9867, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2032/6501 [58:26<1:54:00,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:57,338] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2033/6501 [58:28<1:57:40,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2983, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2033/6501 [58:28<1:57:40,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:17:59,211] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2034/6501 [58:30<2:04:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.101, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2034/6501 [58:30<2:04:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2035/6501 [58:32<2:11:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3032, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2035/6501 [58:32<2:11:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:03,359] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2036/6501 [58:34<2:19:58,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4498, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2036/6501 [58:34<2:19:58,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:04,841] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2037/6501 [58:36<2:11:02,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2437, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2037/6501 [58:36<2:11:02,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:06,360] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2038/6501 [58:37<2:05:36,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9229, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2038/6501 [58:37<2:05:36,  1.69s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2039/6501 [58:39<2:05:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3148, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2039/6501 [58:39<2:05:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2040/6501 [58:40<1:51:17,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3944, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2040/6501 [58:40<1:51:17,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:10,926] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2041/6501 [58:42<1:58:38,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1443, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2041/6501 [58:42<1:58:38,  1.60s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2042/6501 [58:43<2:04:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4012, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2042/6501 [58:43<2:04:34,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:14,237] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2043/6501 [58:45<1:59:26,  1.61s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2043/6501 [58:45<1:59:26,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8464, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2044/6501 [58:47<2:08:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3314, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2044/6501 [58:47<2:08:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:18,267] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2045/6501 [58:49<2:14:48,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1819, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2045/6501 [58:49<2:14:48,  1.82s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2046/6501 [58:50<2:08:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3843, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2046/6501 [58:50<2:08:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2047/6501 [58:52<2:10:41,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8198, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 2047/6501 [58:52<2:10:41,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:23,514] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2048/6501 [58:54<2:13:31,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.262, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2048/6501 [58:54<2:13:31,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:25,226] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2049/6501 [58:56<2:11:33,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3457, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2049/6501 [58:56<2:11:33,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:27,158] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2050/6501 [58:58<2:15:03,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.408, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2050/6501 [58:58<2:15:03,  1.82s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2051/6501 [59:00<2:15:59,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2756, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2051/6501 [59:00<2:15:59,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:30,690] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2052/6501 [59:01<2:12:16,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0271, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2052/6501 [59:01<2:12:16,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2053/6501 [59:03<2:10:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8218, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2053/6501 [59:03<2:10:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:34,419] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2054/6501 [59:05<2:16:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3918, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2054/6501 [59:05<2:16:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2055/6501 [59:07<2:18:33,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4695, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2055/6501 [59:07<2:18:33,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:38,029] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2056/6501 [59:09<2:14:06,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3612, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2056/6501 [59:09<2:14:06,  1.81s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2057/6501 [59:10<2:12:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2597, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2057/6501 [59:10<2:12:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:41,478] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2058/6501 [59:12<2:10:38,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2109, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2058/6501 [59:12<2:10:38,  1.76s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2059/6501 [59:14<2:08:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1729, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2059/6501 [59:14<2:08:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:45,070] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2060/6501 [59:16<2:12:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3742, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2060/6501 [59:16<2:12:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2061/6501 [59:17<2:10:57,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6627, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2061/6501 [59:17<2:10:57,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:48,617] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2062/6501 [59:19<2:12:10,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2288, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2062/6501 [59:19<2:12:10,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:50,090] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2063/6501 [59:21<2:05:11,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2797, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2063/6501 [59:21<2:05:11,  1.69s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2064/6501 [59:22<2:02:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.355, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2064/6501 [59:22<2:02:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:53,278] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2065/6501 [59:24<2:01:28,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4883, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2065/6501 [59:24<2:01:28,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:55,262] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2066/6501 [59:26<2:09:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0429, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2066/6501 [59:26<2:09:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:57,022] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2067/6501 [59:28<2:09:17,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2623, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2067/6501 [59:28<2:09:17,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:18:58,660] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2068/6501 [59:29<2:06:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1093, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2068/6501 [59:29<2:06:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:00,411] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2069/6501 [59:31<2:07:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2419, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2069/6501 [59:31<2:07:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2070/6501 [59:33<2:09:10,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2661, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2070/6501 [59:33<2:09:10,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:04,081] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2071/6501 [59:35<2:11:47,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0358, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2071/6501 [59:35<2:11:47,  1.79s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2072/6501 [59:36<2:09:57,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5435, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2072/6501 [59:36<2:09:57,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:07,643] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2073/6501 [59:38<2:12:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2964, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2073/6501 [59:38<2:12:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2074/6501 [59:40<2:13:46,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1628, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2074/6501 [59:40<2:13:46,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:11,148] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2075/6501 [59:42<2:09:51,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0898, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2075/6501 [59:42<2:09:51,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:12,834] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2076/6501 [59:44<2:08:11,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6782, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2076/6501 [59:44<2:08:11,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:14,404] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2077/6501 [59:45<2:04:25,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4357, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2077/6501 [59:45<2:04:25,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:16,177] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2078/6501 [59:47<2:06:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.949, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2078/6501 [59:47<2:06:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2079/6501 [59:49<2:06:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6543, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2079/6501 [59:49<2:06:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:19,562] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2080/6501 [59:50<2:05:12,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8464, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2080/6501 [59:50<2:05:12,  1.70s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2081/6501 [59:52<2:05:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3657, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2081/6501 [59:52<2:05:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:23,059] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2082/6501 [59:54<2:07:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3631, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2082/6501 [59:54<2:07:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:24,598] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2083/6501 [59:55<2:02:59,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1067, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2083/6501 [59:55<2:02:59,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:26,409] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2084/6501 [59:57<2:06:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2782, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2084/6501 [59:57<2:06:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:27,870] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2085/6501 [59:59<2:00:29,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4172, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2085/6501 [59:59<2:00:29,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:29,631] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2086/6501 [1:00:00<2:03:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0153, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2086/6501 [1:00:00<2:03:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:31,224] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2087/6501 [1:00:02<2:01:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1299, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2087/6501 [1:00:02<2:01:23,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:32,980] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2088/6501 [1:00:04<2:03:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2492, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2088/6501 [1:00:04<2:03:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2089/6501 [1:00:05<2:04:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1888, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2089/6501 [1:00:05<2:04:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:36,487] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2090/6501 [1:00:07<2:06:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9367, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2090/6501 [1:00:07<2:06:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:38,531] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2091/6501 [1:00:09<2:13:42,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0271, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2091/6501 [1:00:09<2:13:42,  1.82s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2092/6501 [1:00:10<1:56:42,  1.59s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2092/6501 [1:00:10<1:56:42,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3717, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:41,329] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2093/6501 [1:00:12<2:00:13,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9334, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2093/6501 [1:00:12<2:00:13,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:43,034] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2094/6501 [1:00:14<2:01:41,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0066, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2094/6501 [1:00:14<2:01:41,  1.66s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2095/6501 [1:00:16<2:04:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.151, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2095/6501 [1:00:16<2:04:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:46,417] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2096/6501 [1:00:17<2:02:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.326, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2096/6501 [1:00:17<2:02:24,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:48,261] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2097/6501 [1:00:19<2:06:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1195, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2097/6501 [1:00:19<2:06:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:49,931] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2098/6501 [1:00:21<2:05:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9861, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2098/6501 [1:00:21<2:05:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:51,746] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2099/6501 [1:00:22<2:07:31,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6954, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2099/6501 [1:00:22<2:07:31,  1.74s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2100/6501 [1:00:24<2:05:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.295, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2100/6501 [1:00:24<2:05:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:55,395] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2101/6501 [1:00:26<2:11:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3268, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2101/6501 [1:00:26<2:11:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:19:57,105] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2102/6501 [1:00:28<2:09:47,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2863, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2102/6501 [1:00:28<2:09:47,  1.77s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2103/6501 [1:00:30<2:17:30,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3361, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2103/6501 [1:00:30<2:17:30,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:00,869] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2104/6501 [1:00:32<2:12:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.339, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2104/6501 [1:00:32<2:12:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:02,680] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2105/6501 [1:00:33<2:12:24,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1664, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2105/6501 [1:00:33<2:12:24,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:04,591] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2106/6501 [1:00:35<2:14:39,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0532, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2106/6501 [1:00:35<2:14:39,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:06,241] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2107/6501 [1:00:37<2:10:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2102, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2107/6501 [1:00:37<2:10:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:07,852] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2108/6501 [1:00:39<2:06:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0047, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2108/6501 [1:00:39<2:06:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2109/6501 [1:00:40<2:05:09,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1617, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2109/6501 [1:00:40<2:05:09,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:11,256] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2110/6501 [1:00:42<2:05:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.517, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2110/6501 [1:00:42<2:05:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2111/6501 [1:00:44<2:16:23,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1909, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2111/6501 [1:00:44<2:16:23,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:15,137] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2112/6501 [1:00:46<2:12:19,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4766, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 2112/6501 [1:00:46<2:12:19,  1.81s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2113/6501 [1:00:47<1:54:02,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.165, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2113/6501 [1:00:47<1:54:02,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:18,272] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2114/6501 [1:00:49<2:07:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8065, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2114/6501 [1:00:49<2:07:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:20,047] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2115/6501 [1:00:51<2:07:54,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.938, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2115/6501 [1:00:51<2:07:54,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:21,897] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2116/6501 [1:00:53<2:10:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8652, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2116/6501 [1:00:53<2:10:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2117/6501 [1:00:54<1:52:40,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1633, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2117/6501 [1:00:54<1:52:40,  1.54s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2118/6501 [1:00:56<2:01:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2407, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2118/6501 [1:00:56<2:01:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:26,637] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2119/6501 [1:00:57<2:04:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4583, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2119/6501 [1:00:57<2:04:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:28,247] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2120/6501 [1:00:59<2:02:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9121, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2120/6501 [1:00:59<2:02:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:30,112] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2121/6501 [1:01:01<2:06:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3252, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2121/6501 [1:01:01<2:06:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:31,868] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2122/6501 [1:01:03<2:07:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.054, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2122/6501 [1:01:03<2:07:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:33,918] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2123/6501 [1:01:05<2:13:48,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.274, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2123/6501 [1:01:05<2:13:48,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:34,970] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2124/6501 [1:01:06<1:56:39,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3006, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2124/6501 [1:01:06<1:56:39,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:36,708] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2125/6501 [1:01:07<1:59:40,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0652, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2125/6501 [1:01:07<1:59:40,  1.64s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2126/6501 [1:01:09<2:01:50,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0526, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2126/6501 [1:01:09<2:01:50,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:40,355] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2127/6501 [1:01:11<2:06:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3855, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2127/6501 [1:01:11<2:06:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2128/6501 [1:01:13<2:06:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3061, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2128/6501 [1:01:13<2:06:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:43,865] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2129/6501 [1:01:15<2:07:41,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5758, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2129/6501 [1:01:15<2:07:41,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:45,496] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2130/6501 [1:01:16<2:05:01,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2109, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2130/6501 [1:01:16<2:05:01,  1.72s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2131/6501 [1:01:18<2:05:52,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6816, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2131/6501 [1:01:18<2:05:52,  1.73s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2132/6501 [1:01:20<2:09:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3361, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2132/6501 [1:01:20<2:09:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:51,059] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2133/6501 [1:01:22<2:12:20,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3062, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2133/6501 [1:01:22<2:12:20,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:53,016] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2134/6501 [1:01:24<2:15:21,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0915, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2134/6501 [1:01:24<2:15:21,  1.86s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2135/6501 [1:01:25<2:13:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.932, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2135/6501 [1:01:25<2:13:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:56,674] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2136/6501 [1:01:27<2:14:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2909, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2136/6501 [1:01:27<2:14:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:20:58,652] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2137/6501 [1:01:29<2:17:18,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0311, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2137/6501 [1:01:29<2:17:18,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:00,458] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2138/6501 [1:01:31<2:15:29,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9159, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2138/6501 [1:01:31<2:15:29,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:02,054] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2139/6501 [1:01:33<2:09:38,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0251, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2139/6501 [1:01:33<2:09:38,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:03,760] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2140/6501 [1:01:34<2:07:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.198, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2140/6501 [1:01:34<2:07:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:05,625] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2141/6501 [1:01:36<2:10:10,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0589, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2141/6501 [1:01:36<2:10:10,  1.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2142/6501 [1:01:38<2:16:44,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1281, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2142/6501 [1:01:38<2:16:44,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:09,492] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2143/6501 [1:01:40<2:14:19,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1393, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2143/6501 [1:01:40<2:14:19,  1.85s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2144/6501 [1:01:42<2:10:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1326, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2144/6501 [1:01:42<2:10:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2145/6501 [1:01:44<2:09:52,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0825, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2145/6501 [1:01:44<2:09:52,  1.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2146/6501 [1:01:46<2:12:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.173, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2146/6501 [1:01:46<2:12:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:16,711] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2147/6501 [1:01:47<2:13:12,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1842, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2147/6501 [1:01:47<2:13:12,  1.84s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2148/6501 [1:01:49<2:11:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9979, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2148/6501 [1:01:49<2:11:29,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:20,336] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2149/6501 [1:01:51<2:12:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0957, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2149/6501 [1:01:51<2:12:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:22,157] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2150/6501 [1:01:53<2:12:26,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4901, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2150/6501 [1:01:53<2:12:26,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:24,028] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2151/6501 [1:01:55<2:13:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.353, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2151/6501 [1:01:55<2:13:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:25,805] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2152/6501 [1:01:57<2:11:59,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2311, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2152/6501 [1:01:57<2:11:59,  1.82s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2153/6501 [1:01:58<2:09:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0043, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2153/6501 [1:01:58<2:09:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:29,200] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2154/6501 [1:02:00<2:07:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9915, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2154/6501 [1:02:00<2:07:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:30,794] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2155/6501 [1:02:01<2:03:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3462, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2155/6501 [1:02:01<2:03:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:32,626] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2156/6501 [1:02:03<2:06:25,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4332, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2156/6501 [1:02:03<2:06:25,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:34,605] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2157/6501 [1:02:05<2:11:26,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0605, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2157/6501 [1:02:05<2:11:26,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:36,347] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2158/6501 [1:02:07<2:09:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1572, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2158/6501 [1:02:07<2:09:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2159/6501 [1:02:09<2:04:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9375, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2159/6501 [1:02:09<2:04:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:39,706] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2160/6501 [1:02:10<2:06:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1797, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2160/6501 [1:02:10<2:06:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2161/6501 [1:02:12<2:02:50,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2156, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2161/6501 [1:02:12<2:02:50,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:43,283] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2162/6501 [1:02:14<2:09:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.283, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2162/6501 [1:02:14<2:09:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2163/6501 [1:02:16<2:06:43,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8689, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2163/6501 [1:02:16<2:06:43,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:47,016] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2164/6501 [1:02:18<2:13:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0106, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2164/6501 [1:02:18<2:13:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2165/6501 [1:02:19<2:07:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0569, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2165/6501 [1:02:19<2:07:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:50,850] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2166/6501 [1:02:22<2:18:14,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3508, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2166/6501 [1:02:22<2:18:14,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:52,639] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2167/6501 [1:02:23<2:15:29,  1.88s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2167/6501 [1:02:23<2:15:29,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9933, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:54,435] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2168/6501 [1:02:25<2:13:44,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1619, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2168/6501 [1:02:25<2:13:44,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:56,552] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2169/6501 [1:02:27<2:19:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4351, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2169/6501 [1:02:27<2:19:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:21:58,271] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2170/6501 [1:02:29<2:14:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1447, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2170/6501 [1:02:29<2:14:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2171/6501 [1:02:31<2:08:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2867, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2171/6501 [1:02:31<2:08:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:01,450] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2172/6501 [1:02:32<2:04:37,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.265, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2172/6501 [1:02:32<2:04:37,  1.73s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2173/6501 [1:02:34<2:08:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1488, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2173/6501 [1:02:34<2:08:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:05,621] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2174/6501 [1:02:36<2:18:58,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1833, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2174/6501 [1:02:36<2:18:58,  1.93s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2175/6501 [1:02:38<2:14:09,  1.86s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2175/6501 [1:02:38<2:14:09,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0541, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:09,273] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2176/6501 [1:02:40<2:15:58,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1292, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2176/6501 [1:02:40<2:15:58,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:10,908] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2177/6501 [1:02:42<2:10:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4058, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 2177/6501 [1:02:42<2:10:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2178/6501 [1:02:43<2:06:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2178/6501 [1:02:43<2:06:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3626, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:14,092] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2179/6501 [1:02:45<2:02:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9461, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2179/6501 [1:02:45<2:02:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:15,816] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2180/6501 [1:02:47<2:02:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2731, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2180/6501 [1:02:47<2:02:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2181/6501 [1:02:48<2:00:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2532, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2181/6501 [1:02:48<2:00:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:19,439] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2182/6501 [1:02:50<2:07:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1674, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2182/6501 [1:02:50<2:07:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2183/6501 [1:02:52<1:59:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0929, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2183/6501 [1:02:52<1:59:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:22,542] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2184/6501 [1:02:53<2:00:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2607, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2184/6501 [1:02:53<2:00:46,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:23,990] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2185/6501 [1:02:55<1:55:47,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9859, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2185/6501 [1:02:55<1:55:47,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:25,716] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2186/6501 [1:02:56<1:58:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2693, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2186/6501 [1:02:56<1:58:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:27,453] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2187/6501 [1:02:58<2:00:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0739, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2187/6501 [1:02:58<2:00:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2188/6501 [1:03:00<2:00:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1739, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2188/6501 [1:03:00<2:00:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:31,112] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2189/6501 [1:03:02<2:06:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4039, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2189/6501 [1:03:02<2:06:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:32,675] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2190/6501 [1:03:03<2:02:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1626, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2190/6501 [1:03:03<2:02:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:34,807] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2191/6501 [1:03:06<2:11:42,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4756, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2191/6501 [1:03:06<2:11:42,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:36,824] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2192/6501 [1:03:08<2:15:38,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0767, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2192/6501 [1:03:08<2:15:38,  1.89s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2193/6501 [1:03:09<2:15:06,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8431, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2193/6501 [1:03:09<2:15:06,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:40,331] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2194/6501 [1:03:11<2:09:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7757, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 2194/6501 [1:03:11<2:09:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2195/6501 [1:03:12<1:51:29,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3336, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2195/6501 [1:03:12<1:51:29,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:43,168] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2196/6501 [1:03:14<1:58:30,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9843, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2196/6501 [1:03:14<1:58:30,  1.65s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2197/6501 [1:03:15<1:58:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1742, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2197/6501 [1:03:15<1:58:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:46,686] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2198/6501 [1:03:17<2:03:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4585, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2198/6501 [1:03:17<2:03:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:48,212] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2199/6501 [1:03:19<1:59:02,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3605, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2199/6501 [1:03:19<1:59:02,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:50,416] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2200/6501 [1:03:21<2:10:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9211, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2200/6501 [1:03:21<2:10:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2201/6501 [1:03:23<2:05:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5222, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2201/6501 [1:03:23<2:05:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:54,146] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2202/6501 [1:03:25<2:13:53,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2025, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2202/6501 [1:03:25<2:13:53,  1.87s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2203/6501 [1:03:26<2:06:36,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5453, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2203/6501 [1:03:26<2:06:36,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:57,428] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2204/6501 [1:03:28<2:06:13,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0596, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2204/6501 [1:03:28<2:06:13,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:22:59,078] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2205/6501 [1:03:30<2:03:46,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.357, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2205/6501 [1:03:30<2:03:46,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:01,058] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2206/6501 [1:03:32<2:09:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9828, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2206/6501 [1:03:32<2:09:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:02,959] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2207/6501 [1:03:34<2:11:11,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2167, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2207/6501 [1:03:34<2:11:11,  1.83s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2208/6501 [1:03:35<2:06:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4565, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2208/6501 [1:03:35<2:06:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:06,125] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2209/6501 [1:03:37<2:01:47,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8328, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2209/6501 [1:03:37<2:01:47,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:08,110] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2210/6501 [1:03:39<2:07:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2514, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2210/6501 [1:03:39<2:07:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:10,185] [WARNING] [stage3.py:1787:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2211/6501 [1:03:41<2:13:58,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3638, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2211/6501 [1:03:41<2:13:58,  1.87s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2212/6501 [1:03:43<2:19:18,  1.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0991, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2212/6501 [1:03:43<2:19:18,  1.95s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:14,152] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2213/6501 [1:03:45<2:17:00,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7227, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2213/6501 [1:03:45<2:17:00,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:15,968] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2214/6501 [1:03:47<2:14:47,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5984, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2214/6501 [1:03:47<2:14:47,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:17,728] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2215/6501 [1:03:48<2:12:03,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.163, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2215/6501 [1:03:48<2:12:03,  1.85s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2216/6501 [1:03:50<2:11:53,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2003, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2216/6501 [1:03:50<2:11:53,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:21,045] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2217/6501 [1:03:52<2:03:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5553, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2217/6501 [1:03:52<2:03:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:22,846] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2218/6501 [1:03:54<2:05:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8108, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2218/6501 [1:03:54<2:05:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:24,636] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2219/6501 [1:03:55<2:05:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4808, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2219/6501 [1:03:55<2:05:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2220/6501 [1:03:57<1:59:35,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9568, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2220/6501 [1:03:57<1:59:35,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:27,949] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2221/6501 [1:03:59<2:03:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3416, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2221/6501 [1:03:59<2:03:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:29,780] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2222/6501 [1:04:00<2:05:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2257, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2222/6501 [1:04:00<2:05:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:31,374] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2223/6501 [1:04:02<2:01:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3041, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2223/6501 [1:04:02<2:01:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:33,014] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2224/6501 [1:04:04<2:00:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2467, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2224/6501 [1:04:04<2:00:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:34,889] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2225/6501 [1:04:06<2:04:17,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1502, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2225/6501 [1:04:06<2:04:17,  1.74s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2226/6501 [1:04:07<1:59:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3985, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2226/6501 [1:04:07<1:59:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:37,745] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2227/6501 [1:04:08<1:52:06,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4817, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2227/6501 [1:04:08<1:52:06,  1.57s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2228/6501 [1:04:10<1:52:46,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2221, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2228/6501 [1:04:10<1:52:46,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:41,086] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2229/6501 [1:04:12<1:55:59,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0726, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2229/6501 [1:04:12<1:55:59,  1.63s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2230/6501 [1:04:13<1:41:57,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0347, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2230/6501 [1:04:13<1:41:57,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:44,188] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2231/6501 [1:04:15<1:56:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0903, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2231/6501 [1:04:15<1:56:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2232/6501 [1:04:17<2:04:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1103, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2232/6501 [1:04:17<2:04:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:48,117] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2233/6501 [1:04:19<2:08:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1506, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2233/6501 [1:04:19<2:08:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2234/6501 [1:04:21<2:08:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3206, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2234/6501 [1:04:21<2:08:05,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:51,389] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2235/6501 [1:04:22<2:00:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0973, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2235/6501 [1:04:22<2:00:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:53,128] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2236/6501 [1:04:24<2:01:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2211, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2236/6501 [1:04:24<2:01:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:54,773] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2237/6501 [1:04:25<2:00:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1474, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2237/6501 [1:04:25<2:00:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:23:56,585] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2238/6501 [1:04:27<2:02:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3336, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2238/6501 [1:04:27<2:02:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2239/6501 [1:04:29<2:04:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9244, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2239/6501 [1:04:29<2:04:49,  1.76s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2240/6501 [1:04:31<1:58:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0177, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2240/6501 [1:04:31<1:58:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2241/6501 [1:04:32<1:56:46,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1456, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2241/6501 [1:04:32<1:56:46,  1.64s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2242/6501 [1:04:34<1:53:57,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1702, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 2242/6501 [1:04:34<1:53:57,  1.61s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2243/6501 [1:04:35<1:55:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9037, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2243/6501 [1:04:35<1:55:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:06,170] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2244/6501 [1:04:37<1:53:09,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2367, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2244/6501 [1:04:37<1:53:09,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:07,837] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2245/6501 [1:04:39<1:54:38,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8799, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2245/6501 [1:04:39<1:54:38,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:09,884] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2246/6501 [1:04:41<2:03:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1032, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2246/6501 [1:04:41<2:03:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2247/6501 [1:04:42<2:02:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.286, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2247/6501 [1:04:42<2:02:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:13,441] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2248/6501 [1:04:44<2:05:24,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3573, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2248/6501 [1:04:44<2:05:24,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:15,160] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2249/6501 [1:04:46<2:04:19,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0388, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2249/6501 [1:04:46<2:04:19,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:17,042] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2250/6501 [1:04:48<2:07:00,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9263, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2250/6501 [1:04:48<2:07:00,  1.79s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2251/6501 [1:04:49<1:50:23,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9307, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2251/6501 [1:04:49<1:50:23,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:20,011] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2252/6501 [1:04:51<1:58:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4667, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2252/6501 [1:04:51<1:58:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2253/6501 [1:04:52<1:54:51,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2672, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2253/6501 [1:04:52<1:54:51,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:23,308] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2254/6501 [1:04:54<1:58:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3033, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2254/6501 [1:04:54<1:58:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2255/6501 [1:04:55<1:54:31,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9216, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2255/6501 [1:04:55<1:54:31,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:26,486] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2256/6501 [1:04:57<1:56:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7107, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2256/6501 [1:04:57<1:56:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:28,276] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2257/6501 [1:04:59<1:59:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2731, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2257/6501 [1:04:59<1:59:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2258/6501 [1:05:01<1:59:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9103, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2258/6501 [1:05:01<1:59:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:31,852] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2259/6501 [1:05:03<2:03:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7748, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2259/6501 [1:05:03<2:03:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:33,716] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2260/6501 [1:05:04<2:05:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3207, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2260/6501 [1:05:04<2:05:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2261/6501 [1:05:06<2:06:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0632, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2261/6501 [1:05:06<2:06:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:37,278] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2262/6501 [1:05:08<2:05:33,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2052, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2262/6501 [1:05:08<2:05:33,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:39,407] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2263/6501 [1:05:10<2:12:58,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1071, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2263/6501 [1:05:10<2:12:58,  1.88s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2264/6501 [1:05:12<2:04:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9877, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2264/6501 [1:05:12<2:04:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:42,842] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2265/6501 [1:05:14<2:08:25,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3196, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2265/6501 [1:05:14<2:08:25,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:44,402] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2266/6501 [1:05:15<2:02:54,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4044, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2266/6501 [1:05:15<2:02:54,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:46,150] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2267/6501 [1:05:17<2:03:01,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3723, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2267/6501 [1:05:17<2:03:01,  1.74s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2268/6501 [1:05:19<2:04:44,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2952, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2268/6501 [1:05:19<2:04:44,  1.77s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2269/6501 [1:05:20<2:00:26,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1442, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2269/6501 [1:05:20<2:00:26,  1.71s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2270/6501 [1:05:21<1:43:51,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.074, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2270/6501 [1:05:21<1:43:51,  1.47s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2271/6501 [1:05:22<1:32:04,  1.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5937, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2271/6501 [1:05:22<1:32:04,  1.31s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:53,363] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2272/6501 [1:05:24<1:46:17,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2363, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2272/6501 [1:05:24<1:46:17,  1.51s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2273/6501 [1:05:26<1:47:33,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.233, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2273/6501 [1:05:26<1:47:33,  1.53s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2274/6501 [1:05:27<1:36:05,  1.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0672, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2274/6501 [1:05:27<1:36:05,  1.36s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:24:57,925] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2275/6501 [1:05:29<1:49:39,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3337, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 2275/6501 [1:05:29<1:49:39,  1.56s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2276/6501 [1:05:30<1:45:23,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2763, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2276/6501 [1:05:30<1:45:23,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:00,995] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2277/6501 [1:05:32<1:49:57,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3135, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2277/6501 [1:05:32<1:49:57,  1.56s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2278/6501 [1:05:33<1:52:31,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0384, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2278/6501 [1:05:33<1:52:31,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:04,333] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2279/6501 [1:05:35<1:53:38,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2371, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2279/6501 [1:05:35<1:53:38,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:06,230] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2280/6501 [1:05:37<1:59:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1965, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2280/6501 [1:05:37<1:59:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2281/6501 [1:05:39<2:03:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1521, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2281/6501 [1:05:39<2:03:42,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:09,441] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2282/6501 [1:05:40<1:54:17,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1424, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2282/6501 [1:05:40<1:54:17,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:11,332] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2283/6501 [1:05:42<1:59:52,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1209, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2283/6501 [1:05:42<1:59:52,  1.71s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2284/6501 [1:05:44<1:56:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1552, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2284/6501 [1:05:44<1:56:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:14,563] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2285/6501 [1:05:45<1:57:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0993, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2285/6501 [1:05:45<1:57:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:16,398] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2286/6501 [1:05:47<2:00:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3484, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2286/6501 [1:05:47<2:00:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:18,289] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2287/6501 [1:05:49<2:04:13,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0318, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2287/6501 [1:05:49<2:04:13,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:20,226] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2288/6501 [1:05:51<2:07:44,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6542, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2288/6501 [1:05:51<2:07:44,  1.82s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2289/6501 [1:05:53<2:05:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1518, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2289/6501 [1:05:53<2:05:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2290/6501 [1:05:54<1:48:22,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2457, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2290/6501 [1:05:54<1:48:22,  1.54s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2291/6501 [1:05:55<1:50:48,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2349, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2291/6501 [1:05:55<1:50:48,  1.58s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2292/6501 [1:05:57<1:50:34,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3338, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2292/6501 [1:05:57<1:50:34,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:27,905] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2293/6501 [1:05:59<1:54:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.152, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2293/6501 [1:05:59<1:54:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:29,497] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2294/6501 [1:06:00<1:53:36,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0721, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2294/6501 [1:06:00<1:53:36,  1.62s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2295/6501 [1:06:02<2:01:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.618, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2295/6501 [1:06:02<2:01:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2296/6501 [1:06:03<1:46:16,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.513, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2296/6501 [1:06:03<1:46:16,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:34,403] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2297/6501 [1:06:05<1:54:18,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3395, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2297/6501 [1:06:05<1:54:18,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:36,213] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2298/6501 [1:06:07<1:58:02,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2229, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2298/6501 [1:06:07<1:58:02,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:38,171] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2299/6501 [1:06:09<2:03:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.338, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2299/6501 [1:06:09<2:03:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2300/6501 [1:06:11<2:02:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9595, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2300/6501 [1:06:11<2:02:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:41,767] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2301/6501 [1:06:12<2:05:29,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3402, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2301/6501 [1:06:12<2:05:29,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:43,456] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2302/6501 [1:06:14<2:03:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2115, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2302/6501 [1:06:14<2:03:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2303/6501 [1:06:16<2:02:40,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2239, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2303/6501 [1:06:16<2:02:40,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:46,880] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2304/6501 [1:06:18<2:01:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1286, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2304/6501 [1:06:18<2:01:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:48,622] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2305/6501 [1:06:19<2:01:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9762, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2305/6501 [1:06:19<2:01:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2306/6501 [1:06:21<2:03:04,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2183, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2306/6501 [1:06:21<2:03:04,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:52,123] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2307/6501 [1:06:23<2:01:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0912, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 2307/6501 [1:06:23<2:01:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:53,761] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2308/6501 [1:06:24<1:59:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9307, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2308/6501 [1:06:24<1:59:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2309/6501 [1:06:26<2:00:20,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0588, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2309/6501 [1:06:26<2:00:20,  1.72s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2310/6501 [1:06:28<1:57:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.157, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2310/6501 [1:06:28<1:57:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:25:58,817] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2311/6501 [1:06:30<1:58:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1018, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2311/6501 [1:06:30<1:58:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:00,829] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2312/6501 [1:06:32<2:04:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2279, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2312/6501 [1:06:32<2:04:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:02,617] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2313/6501 [1:06:33<2:04:47,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5159, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2313/6501 [1:06:33<2:04:47,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:04,594] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2314/6501 [1:06:35<2:08:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1456, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2314/6501 [1:06:35<2:08:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:06,308] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2315/6501 [1:06:37<2:05:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3896, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2315/6501 [1:06:37<2:05:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:07,972] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2316/6501 [1:06:39<2:02:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2316/6501 [1:06:39<2:02:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3097, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2317/6501 [1:06:40<1:57:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0414, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2317/6501 [1:06:40<1:57:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:11,122] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2318/6501 [1:06:42<1:56:43,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2085, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2318/6501 [1:06:42<1:56:43,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:13,070] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2319/6501 [1:06:44<2:02:26,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2958, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2319/6501 [1:06:44<2:02:26,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:14,681] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2320/6501 [1:06:45<1:59:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2992, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2320/6501 [1:06:45<1:59:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:16,521] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2321/6501 [1:06:47<2:01:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1083, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2321/6501 [1:06:47<2:01:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2322/6501 [1:06:48<1:45:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5498, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2322/6501 [1:06:48<1:45:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:19,528] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2323/6501 [1:06:50<1:56:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1462, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2323/6501 [1:06:50<1:56:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2324/6501 [1:06:52<1:57:13,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3376, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2324/6501 [1:06:52<1:57:13,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:23,066] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2325/6501 [1:06:54<2:00:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3367, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2325/6501 [1:06:54<2:00:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:24,603] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2326/6501 [1:06:55<1:56:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1256, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2326/6501 [1:06:55<1:56:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:25,840] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2327/6501 [1:06:57<1:47:06,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3077, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2327/6501 [1:06:57<1:47:06,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:27,594] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2328/6501 [1:06:58<1:51:34,  1.60s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2328/6501 [1:06:58<1:51:34,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0935, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2329/6501 [1:07:00<1:45:48,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1092, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2329/6501 [1:07:00<1:45:48,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:30,644] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2330/6501 [1:07:01<1:49:55,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0327, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2330/6501 [1:07:01<1:49:55,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:32,033] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2331/6501 [1:07:03<1:45:53,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1481, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2331/6501 [1:07:03<1:45:53,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:33,397] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2332/6501 [1:07:04<1:42:33,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.101, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2332/6501 [1:07:04<1:42:33,  1.48s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:35,360] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2333/6501 [1:07:06<1:52:40,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2817, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2333/6501 [1:07:06<1:52:40,  1.62s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2334/6501 [1:07:08<1:59:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7997, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2334/6501 [1:07:08<1:59:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:39,121] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2335/6501 [1:07:10<2:01:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0466, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2335/6501 [1:07:10<2:01:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:40,896] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2336/6501 [1:07:12<2:01:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4497, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2336/6501 [1:07:12<2:01:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:42,601] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2337/6501 [1:07:13<2:00:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8336, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2337/6501 [1:07:13<2:00:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2338/6501 [1:07:15<2:04:49,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0695, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2338/6501 [1:07:15<2:04:49,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:46,556] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2339/6501 [1:07:17<2:09:23,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3798, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2339/6501 [1:07:17<2:09:23,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:48,328] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2340/6501 [1:07:19<2:07:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5894, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2340/6501 [1:07:19<2:07:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2341/6501 [1:07:21<2:00:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.116, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2341/6501 [1:07:21<2:00:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:51,889] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2342/6501 [1:07:23<2:07:09,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9631, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2342/6501 [1:07:23<2:07:09,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:53,596] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2343/6501 [1:07:24<2:04:29,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2103, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2343/6501 [1:07:24<2:04:29,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:55,372] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2344/6501 [1:07:26<2:04:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1232, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2344/6501 [1:07:26<2:04:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:26:57,011] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2345/6501 [1:07:28<2:00:51,  1.74s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2345/6501 [1:07:28<2:00:51,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3977, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2346/6501 [1:07:29<2:00:18,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3386, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2346/6501 [1:07:29<2:00:18,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:00,457] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2347/6501 [1:07:31<2:00:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4588, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2347/6501 [1:07:31<2:00:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2348/6501 [1:07:33<1:57:47,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2132, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2348/6501 [1:07:33<1:57:47,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:03,691] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2349/6501 [1:07:34<1:55:48,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1021, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2349/6501 [1:07:34<1:55:48,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:05,290] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2350/6501 [1:07:36<1:54:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1795, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2350/6501 [1:07:36<1:54:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:06,971] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2351/6501 [1:07:38<1:54:49,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3752, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2351/6501 [1:07:38<1:54:49,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:08,582] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2352/6501 [1:07:39<1:53:46,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0863, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2352/6501 [1:07:39<1:53:46,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:10,219] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2353/6501 [1:07:41<1:53:34,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2016, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2353/6501 [1:07:41<1:53:34,  1.64s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2354/6501 [1:07:43<1:53:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2354/6501 [1:07:43<1:53:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3092, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:13,629] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2355/6501 [1:07:44<1:56:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1745, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2355/6501 [1:07:44<1:56:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:15,442] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2356/6501 [1:07:46<1:58:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5422, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 2356/6501 [1:07:46<1:58:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2357/6501 [1:07:48<1:53:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3407, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2357/6501 [1:07:48<1:53:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:18,526] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2358/6501 [1:07:49<1:52:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.673, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2358/6501 [1:07:49<1:52:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:20,243] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2359/6501 [1:07:51<1:54:38,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9686, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2359/6501 [1:07:51<1:54:38,  1.66s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2360/6501 [1:07:53<1:56:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2197, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2360/6501 [1:07:53<1:56:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:23,412] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2361/6501 [1:07:54<1:50:54,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2956, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2361/6501 [1:07:54<1:50:54,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:25,361] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2362/6501 [1:07:56<1:57:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.56, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2362/6501 [1:07:56<1:57:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2363/6501 [1:07:58<1:58:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2363/6501 [1:07:58<1:58:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3707, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:29,226] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2364/6501 [1:08:00<2:06:56,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2298, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2364/6501 [1:08:00<2:06:56,  1.84s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2365/6501 [1:08:02<2:06:17,  1.83s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2365/6501 [1:08:02<2:06:17,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.88, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2366/6501 [1:08:03<1:47:26,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4006, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2366/6501 [1:08:03<1:47:26,  1.56s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2367/6501 [1:08:05<2:05:38,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.856, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2367/6501 [1:08:05<2:05:38,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:35,917] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2368/6501 [1:08:07<1:59:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2527, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2368/6501 [1:08:07<1:59:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:37,869] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2369/6501 [1:08:09<2:03:48,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0525, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2369/6501 [1:08:09<2:03:48,  1.80s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2370/6501 [1:08:10<1:49:41,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1313, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2370/6501 [1:08:10<1:49:41,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:40,975] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2371/6501 [1:08:12<1:57:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3094, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2371/6501 [1:08:12<1:57:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:42,759] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2372/6501 [1:08:13<1:59:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3546, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 2372/6501 [1:08:13<1:59:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2373/6501 [1:08:15<2:01:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3351, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2373/6501 [1:08:15<2:01:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:46,460] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2374/6501 [1:08:17<2:03:22,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.388, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2374/6501 [1:08:17<2:03:22,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:48,337] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2375/6501 [1:08:19<2:05:03,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.222, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2375/6501 [1:08:19<2:05:03,  1.82s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2376/6501 [1:08:21<2:01:21,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4598, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2376/6501 [1:08:21<2:01:21,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:51,721] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2377/6501 [1:08:22<2:00:52,  1.76s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2377/6501 [1:08:22<2:00:52,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2009, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:53,601] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2378/6501 [1:08:24<2:03:22,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2592, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2378/6501 [1:08:24<2:03:22,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:55,290] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2379/6501 [1:08:26<2:01:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.164, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2379/6501 [1:08:26<2:01:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:27:57,305] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2380/6501 [1:08:28<2:06:17,  1.84s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2380/6501 [1:08:28<2:06:17,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2455, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2381/6501 [1:08:29<1:54:41,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5048, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2381/6501 [1:08:29<1:54:41,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:00,372] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2382/6501 [1:08:31<1:57:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2158, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2382/6501 [1:08:31<1:57:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:02,088] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2383/6501 [1:08:33<1:57:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3961, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2383/6501 [1:08:33<1:57:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2384/6501 [1:08:34<1:56:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2507, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2384/6501 [1:08:34<1:56:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:05,316] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2385/6501 [1:08:36<1:53:29,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0944, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2385/6501 [1:08:36<1:53:29,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:06,973] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2386/6501 [1:08:38<1:53:30,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6984, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2386/6501 [1:08:38<1:53:30,  1.66s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2387/6501 [1:08:39<1:42:32,  1.50s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2387/6501 [1:08:39<1:42:32,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.299, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:10,005] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2388/6501 [1:08:41<1:51:01,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1103, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2388/6501 [1:08:41<1:51:01,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:11,765] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2389/6501 [1:08:42<1:53:53,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.885, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2389/6501 [1:08:42<1:53:53,  1.66s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2390/6501 [1:08:43<1:39:48,  1.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0553, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2390/6501 [1:08:43<1:39:48,  1.46s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:14,750] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2391/6501 [1:08:45<1:51:04,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4146, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2391/6501 [1:08:45<1:51:04,  1.62s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2392/6501 [1:08:47<1:52:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1121, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2392/6501 [1:08:47<1:52:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2393/6501 [1:08:49<1:55:43,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2191, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2393/6501 [1:08:49<1:55:43,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:19,972] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2394/6501 [1:08:51<1:56:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0953, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2394/6501 [1:08:51<1:56:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:21,763] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2395/6501 [1:08:52<1:58:12,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2874, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2395/6501 [1:08:52<1:58:12,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:23,654] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2396/6501 [1:08:54<2:01:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8744, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2396/6501 [1:08:54<2:01:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2397/6501 [1:08:56<2:02:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4965, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2397/6501 [1:08:56<2:02:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:27,466] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2398/6501 [1:08:58<2:06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2134, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2398/6501 [1:08:58<2:06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:29,094] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2399/6501 [1:09:00<2:01:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2315, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2399/6501 [1:09:00<2:01:59,  1.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2400/6501 [1:09:02<2:02:56,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3051, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2400/6501 [1:09:02<2:02:56,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:32,518] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2401/6501 [1:09:03<1:58:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2737, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2401/6501 [1:09:03<1:58:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2402/6501 [1:09:05<2:05:27,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2321, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2402/6501 [1:09:05<2:05:27,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:36,029] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2403/6501 [1:09:07<1:57:20,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6715, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2403/6501 [1:09:07<1:57:20,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:37,641] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2404/6501 [1:09:08<1:55:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2807, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2404/6501 [1:09:08<1:55:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:39,437] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2405/6501 [1:09:10<1:57:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1267, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2405/6501 [1:09:10<1:57:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:41,117] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2406/6501 [1:09:12<1:56:32,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.189, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2406/6501 [1:09:12<1:56:32,  1.71s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2407/6501 [1:09:14<1:59:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1504, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2407/6501 [1:09:14<1:59:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:44,771] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2408/6501 [1:09:15<2:00:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0939, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2408/6501 [1:09:15<2:00:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2409/6501 [1:09:17<1:59:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2217, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2409/6501 [1:09:17<1:59:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:48,102] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2410/6501 [1:09:19<1:56:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2967, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2410/6501 [1:09:19<1:56:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:49,755] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2411/6501 [1:09:20<1:55:25,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4971, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2411/6501 [1:09:20<1:55:25,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:50,859] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2412/6501 [1:09:22<1:43:20,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1605, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2412/6501 [1:09:22<1:43:20,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:52,749] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2413/6501 [1:09:23<1:50:57,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8894, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2413/6501 [1:09:23<1:50:57,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:53,918] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2414/6501 [1:09:25<1:41:32,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.876, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2414/6501 [1:09:25<1:41:32,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:55,609] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2415/6501 [1:09:26<1:45:35,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0239, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2415/6501 [1:09:26<1:45:35,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:28:57,593] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2416/6501 [1:09:28<1:54:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.853, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2416/6501 [1:09:28<1:54:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2417/6501 [1:09:30<1:54:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8876, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2417/6501 [1:09:30<1:54:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:01,177] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2418/6501 [1:09:32<1:58:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8811, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2418/6501 [1:09:32<1:58:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:02,533] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2419/6501 [1:09:33<1:50:53,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0497, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2419/6501 [1:09:33<1:50:53,  1.63s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2420/6501 [1:09:34<1:36:34,  1.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1366, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2420/6501 [1:09:34<1:36:34,  1.42s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:05,382] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2421/6501 [1:09:36<1:46:43,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2318, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2421/6501 [1:09:36<1:46:43,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:07,135] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2422/6501 [1:09:38<1:50:26,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3398, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2422/6501 [1:09:38<1:50:26,  1.62s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2423/6501 [1:09:40<1:54:29,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2593, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2423/6501 [1:09:40<1:54:29,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:10,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2424/6501 [1:09:41<1:50:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5297, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2424/6501 [1:09:41<1:50:29,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:12,004] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2425/6501 [1:09:43<1:49:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1266, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2425/6501 [1:09:43<1:49:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:13,887] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2426/6501 [1:09:45<1:54:39,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4375, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2426/6501 [1:09:45<1:54:39,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:15,920] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2427/6501 [1:09:47<2:01:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2427/6501 [1:09:47<2:01:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1553, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:17,377] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2428/6501 [1:09:48<1:54:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0409, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2428/6501 [1:09:48<1:54:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:19,182] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2429/6501 [1:09:50<1:57:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2824, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2429/6501 [1:09:50<1:57:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:21,114] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2430/6501 [1:09:52<2:01:16,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0728, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2430/6501 [1:09:52<2:01:16,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:23,162] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2431/6501 [1:09:54<2:06:32,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8474, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2431/6501 [1:09:54<2:06:32,  1.87s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2432/6501 [1:09:55<1:52:11,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2155, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2432/6501 [1:09:55<1:52:11,  1.65s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2433/6501 [1:09:56<1:37:18,  1.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4773, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2433/6501 [1:09:56<1:37:18,  1.44s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:26,922] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2434/6501 [1:09:58<1:42:09,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5941, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2434/6501 [1:09:58<1:42:09,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:28,898] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2435/6501 [1:10:00<1:51:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0297, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2435/6501 [1:10:00<1:51:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2436/6501 [1:10:01<1:42:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4254, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2436/6501 [1:10:01<1:42:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:31,870] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2437/6501 [1:10:03<1:47:45,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5624, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 2437/6501 [1:10:03<1:47:45,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:33,561] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2438/6501 [1:10:04<1:49:46,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1368, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2438/6501 [1:10:04<1:49:46,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:35,365] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2439/6501 [1:10:06<1:53:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5083, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2439/6501 [1:10:06<1:53:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:37,188] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2440/6501 [1:10:08<1:56:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0627, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2440/6501 [1:10:08<1:56:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:38,685] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2441/6501 [1:10:09<1:51:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1457, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2441/6501 [1:10:09<1:51:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:40,485] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2442/6501 [1:10:11<1:54:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0411, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2442/6501 [1:10:11<1:54:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2443/6501 [1:10:13<1:58:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0357, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2443/6501 [1:10:13<1:58:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:43,598] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2444/6501 [1:10:14<1:47:47,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4639, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2444/6501 [1:10:14<1:47:47,  1.59s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2445/6501 [1:10:16<1:52:33,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5557, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2445/6501 [1:10:16<1:52:33,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:47,068] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2446/6501 [1:10:18<1:52:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3185, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2446/6501 [1:10:18<1:52:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:48,778] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2447/6501 [1:10:19<1:53:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3724, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2447/6501 [1:10:19<1:53:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2448/6501 [1:10:21<1:41:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4725, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2448/6501 [1:10:21<1:41:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:51,668] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2449/6501 [1:10:22<1:47:15,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6962, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2449/6501 [1:10:22<1:47:15,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:53,572] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2450/6501 [1:10:24<1:53:37,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0235, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2450/6501 [1:10:24<1:53:37,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:55,190] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2451/6501 [1:10:26<1:52:16,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1097, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2451/6501 [1:10:26<1:52:16,  1.66s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2452/6501 [1:10:28<1:54:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9694, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2452/6501 [1:10:28<1:54:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:29:58,751] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2453/6501 [1:10:29<1:56:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0781, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2453/6501 [1:10:29<1:56:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:00,289] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2454/6501 [1:10:31<1:52:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3418, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2454/6501 [1:10:31<1:52:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:01,910] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2455/6501 [1:10:33<1:51:33,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0854, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2455/6501 [1:10:33<1:51:33,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:03,625] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2456/6501 [1:10:34<1:52:45,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5054, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2456/6501 [1:10:34<1:52:45,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:05,238] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2457/6501 [1:10:36<1:51:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1247, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2457/6501 [1:10:36<1:51:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:06,994] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2458/6501 [1:10:38<1:53:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8167, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2458/6501 [1:10:38<1:53:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2459/6501 [1:10:39<1:55:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2468, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2459/6501 [1:10:39<1:55:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:10,621] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2460/6501 [1:10:41<1:57:58,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3267, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2460/6501 [1:10:41<1:57:58,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:12,189] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2461/6501 [1:10:43<1:54:13,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2373, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2461/6501 [1:10:43<1:54:13,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:14,114] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2462/6501 [1:10:45<1:58:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2462/6501 [1:10:45<1:58:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5967, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:16,039] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2463/6501 [1:10:47<2:02:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.991, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2463/6501 [1:10:47<2:02:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:17,584] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2464/6501 [1:10:48<1:56:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3531, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2464/6501 [1:10:48<1:56:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:19,472] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2465/6501 [1:10:50<1:59:40,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0589, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2465/6501 [1:10:50<1:59:40,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:21,372] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2466/6501 [1:10:52<2:02:05,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0083, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2466/6501 [1:10:52<2:02:05,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:23,138] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2467/6501 [1:10:54<2:01:04,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6814, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2467/6501 [1:10:54<2:01:04,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:25,020] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2468/6501 [1:10:56<2:02:40,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1688, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2468/6501 [1:10:56<2:02:40,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:26,694] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2469/6501 [1:10:57<1:59:35,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6356, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2469/6501 [1:10:57<1:59:35,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:28,937] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2470/6501 [1:11:00<2:08:54,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.11, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2470/6501 [1:11:00<2:08:54,  1.92s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2471/6501 [1:11:01<2:06:17,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4053, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2471/6501 [1:11:01<2:06:17,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:32,290] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2472/6501 [1:11:03<1:59:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3206, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2472/6501 [1:11:03<1:59:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2473/6501 [1:11:05<2:11:40,  1.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1271, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2473/6501 [1:11:05<2:11:40,  1.96s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:36,842] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2474/6501 [1:11:08<2:16:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3602, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2474/6501 [1:11:08<2:16:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2475/6501 [1:11:09<2:11:48,  1.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1552, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2475/6501 [1:11:09<2:11:48,  1.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2476/6501 [1:11:11<2:05:34,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2316, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2476/6501 [1:11:11<2:05:34,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:41,921] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2477/6501 [1:11:13<2:00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3551, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2477/6501 [1:11:13<2:00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:43,694] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2478/6501 [1:11:14<1:59:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.403, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2478/6501 [1:11:14<1:59:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:45,439] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2479/6501 [1:11:16<1:58:53,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1693, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2479/6501 [1:11:16<1:58:53,  1.77s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2480/6501 [1:11:18<1:53:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0859, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2480/6501 [1:11:18<1:53:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:48,756] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2481/6501 [1:11:19<1:55:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3897, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2481/6501 [1:11:19<1:55:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2482/6501 [1:11:21<1:52:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.085, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2482/6501 [1:11:21<1:52:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:52,140] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2483/6501 [1:11:23<1:55:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1497, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2483/6501 [1:11:23<1:55:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:53,858] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2484/6501 [1:11:25<1:55:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4391, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2484/6501 [1:11:25<1:55:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:55,718] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2485/6501 [1:11:26<1:57:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2485/6501 [1:11:26<1:57:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3773, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2486/6501 [1:11:28<2:04:20,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5699, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2486/6501 [1:11:28<2:04:20,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:30:59,452] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2487/6501 [1:11:30<2:00:11,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4404, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2487/6501 [1:11:30<2:00:11,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:01,312] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2488/6501 [1:11:32<2:01:26,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4357, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2488/6501 [1:11:32<2:01:26,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:02,860] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2489/6501 [1:11:34<1:56:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4525, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2489/6501 [1:11:34<1:56:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:04,708] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2490/6501 [1:11:35<1:58:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3821, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2490/6501 [1:11:35<1:58:16,  1.77s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2491/6501 [1:11:37<1:57:41,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.058, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2491/6501 [1:11:37<1:57:41,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:08,149] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2492/6501 [1:11:39<1:56:25,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2441, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2492/6501 [1:11:39<1:56:25,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:09,730] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2493/6501 [1:11:40<1:53:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9518, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2493/6501 [1:11:40<1:53:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:11,740] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2494/6501 [1:11:42<1:59:27,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5594, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2494/6501 [1:11:42<1:59:27,  1.79s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2495/6501 [1:11:44<1:53:30,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1863, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2495/6501 [1:11:44<1:53:30,  1.70s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2496/6501 [1:11:45<1:38:24,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3393, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2496/6501 [1:11:45<1:38:24,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:16,239] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2497/6501 [1:11:47<1:50:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3864, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2497/6501 [1:11:47<1:50:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:18,165] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2498/6501 [1:11:49<1:55:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1749, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2498/6501 [1:11:49<1:55:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:20,212] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2499/6501 [1:11:51<2:01:51,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4064, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2499/6501 [1:11:51<2:01:51,  1.83s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2500/6501 [1:11:53<1:57:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2674, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2500/6501 [1:11:53<1:57:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2501/6501 [1:11:54<1:54:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9788, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2501/6501 [1:11:54<1:54:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:25,183] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2502/6501 [1:11:56<1:55:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0224, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 2502/6501 [1:11:56<1:55:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2503/6501 [1:11:57<1:51:53,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0725, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2503/6501 [1:11:57<1:51:53,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:28,531] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2504/6501 [1:11:59<1:53:49,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5949, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2504/6501 [1:11:59<1:53:49,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:30,215] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2505/6501 [1:12:01<1:53:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0418, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2505/6501 [1:12:01<1:53:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:32,181] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2506/6501 [1:12:03<1:58:33,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.588, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2506/6501 [1:12:03<1:58:33,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:33,593] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2507/6501 [1:12:04<1:51:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4204, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2507/6501 [1:12:04<1:51:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:35,589] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2508/6501 [1:12:06<1:57:39,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2199, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2508/6501 [1:12:06<1:57:39,  1.77s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2509/6501 [1:12:08<1:50:01,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1135, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2509/6501 [1:12:08<1:50:01,  1.65s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2510/6501 [1:12:09<1:44:06,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1584, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2510/6501 [1:12:09<1:44:06,  1.57s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2511/6501 [1:12:11<1:51:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1743, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2511/6501 [1:12:11<1:51:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:41,890] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2512/6501 [1:12:13<1:50:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3207, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2512/6501 [1:12:13<1:50:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:43,830] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2513/6501 [1:12:15<1:55:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2416, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2513/6501 [1:12:15<1:55:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:45,695] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2514/6501 [1:12:16<1:58:20,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2159, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2514/6501 [1:12:16<1:58:20,  1.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2515/6501 [1:12:18<1:56:53,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3388, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2515/6501 [1:12:18<1:56:53,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:49,152] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2516/6501 [1:12:20<1:56:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2105, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2516/6501 [1:12:20<1:56:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:50,906] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2517/6501 [1:12:22<1:56:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2002, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2517/6501 [1:12:22<1:56:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2518/6501 [1:12:23<1:56:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9804, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2518/6501 [1:12:23<1:56:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:54,106] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2519/6501 [1:12:25<1:50:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1709, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 2519/6501 [1:12:25<1:50:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:55,931] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2520/6501 [1:12:27<1:53:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7837, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2520/6501 [1:12:27<1:53:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2521/6501 [1:12:28<1:52:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.04, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2521/6501 [1:12:28<1:52:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:31:59,306] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2522/6501 [1:12:30<1:52:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.977, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2522/6501 [1:12:30<1:52:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2523/6501 [1:12:32<1:53:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8402, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2523/6501 [1:12:32<1:53:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:03,019] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2524/6501 [1:12:34<1:58:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.475, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2524/6501 [1:12:34<1:58:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:04,843] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2525/6501 [1:12:36<1:59:22,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8476, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2525/6501 [1:12:36<1:59:22,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:06,254] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2526/6501 [1:12:37<1:51:35,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2716, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2526/6501 [1:12:37<1:51:35,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:08,365] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2527/6501 [1:12:39<2:00:01,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2113, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2527/6501 [1:12:39<2:00:01,  1.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2528/6501 [1:12:40<1:52:02,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4543, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2528/6501 [1:12:40<1:52:02,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:11,513] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2529/6501 [1:12:42<1:52:53,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1904, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2529/6501 [1:12:42<1:52:53,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:13,515] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2530/6501 [1:12:44<1:58:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9684, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2530/6501 [1:12:44<1:58:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2531/6501 [1:12:46<1:57:17,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6169, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2531/6501 [1:12:46<1:57:17,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:17,009] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2532/6501 [1:12:48<1:57:14,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1288, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2532/6501 [1:12:48<1:57:14,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:18,670] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2533/6501 [1:12:49<1:55:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9654, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2533/6501 [1:12:49<1:55:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:20,512] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2534/6501 [1:12:51<1:57:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5429, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2534/6501 [1:12:51<1:57:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2535/6501 [1:12:53<2:00:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4849, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2535/6501 [1:12:53<2:00:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:24,285] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2536/6501 [1:12:55<2:00:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1031, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2536/6501 [1:12:55<2:00:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:25,954] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2537/6501 [1:12:57<1:57:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3735, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2537/6501 [1:12:57<1:57:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:27,561] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2538/6501 [1:12:58<1:54:04,  1.73s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2538/6501 [1:12:58<1:54:04,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1909, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2539/6501 [1:13:00<1:57:21,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9877, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2539/6501 [1:13:00<1:57:21,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:31,150] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2540/6501 [1:13:02<1:55:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.956, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2540/6501 [1:13:02<1:55:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:32,849] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2541/6501 [1:13:04<1:54:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7195, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2541/6501 [1:13:04<1:54:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:34,846] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2542/6501 [1:13:06<1:59:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5262, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2542/6501 [1:13:06<1:59:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:36,683] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2543/6501 [1:13:07<2:00:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9907, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2543/6501 [1:13:07<2:00:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:38,316] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2544/6501 [1:13:09<1:56:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1931, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2544/6501 [1:13:09<1:56:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:40,256] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2545/6501 [1:13:11<1:59:49,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6051, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2545/6501 [1:13:11<1:59:49,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:41,752] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2546/6501 [1:13:12<1:53:26,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0858, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2546/6501 [1:13:12<1:53:26,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:43,511] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2547/6501 [1:13:14<1:54:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0152, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2547/6501 [1:13:14<1:54:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2548/6501 [1:13:16<1:58:38,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2736, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2548/6501 [1:13:16<1:58:38,  1.80s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2549/6501 [1:13:17<1:45:04,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0628, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2549/6501 [1:13:17<1:45:04,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:48,527] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2550/6501 [1:13:19<1:51:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0714, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2550/6501 [1:13:19<1:51:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:50,320] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2551/6501 [1:13:21<1:53:41,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7079, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2551/6501 [1:13:21<1:53:41,  1.73s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2552/6501 [1:13:22<1:38:26,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2023, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2552/6501 [1:13:22<1:38:26,  1.50s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2553/6501 [1:13:24<1:42:42,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3612, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2553/6501 [1:13:24<1:42:42,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:54,506] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2554/6501 [1:13:25<1:41:48,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2203, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2554/6501 [1:13:25<1:41:48,  1.55s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2555/6501 [1:13:27<1:49:30,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1028, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2555/6501 [1:13:27<1:49:30,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:58,049] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2556/6501 [1:13:29<1:48:16,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9524, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2556/6501 [1:13:29<1:48:16,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:32:59,916] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2557/6501 [1:13:31<1:52:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8704, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2557/6501 [1:13:31<1:52:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:01,953] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2558/6501 [1:13:33<1:58:56,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1304, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2558/6501 [1:13:33<1:58:56,  1.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2559/6501 [1:13:34<1:57:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.296, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2559/6501 [1:13:34<1:57:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2560/6501 [1:13:36<1:53:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7551, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2560/6501 [1:13:36<1:53:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:07,460] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2561/6501 [1:13:38<2:02:18,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1118, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2561/6501 [1:13:38<2:02:18,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:09,527] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2562/6501 [1:13:40<2:06:19,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8055, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2562/6501 [1:13:40<2:06:19,  1.92s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2563/6501 [1:13:42<2:03:26,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9406, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2563/6501 [1:13:42<2:03:26,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:12,957] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2564/6501 [1:13:44<1:58:52,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0397, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2564/6501 [1:13:44<1:58:52,  1.81s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2565/6501 [1:13:46<2:02:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4375, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2565/6501 [1:13:46<2:02:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2566/6501 [1:13:47<1:45:20,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5895, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2566/6501 [1:13:47<1:45:20,  1.61s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2567/6501 [1:13:49<1:50:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5737, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 2567/6501 [1:13:49<1:50:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:19,523] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2568/6501 [1:13:50<1:50:47,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9176, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2568/6501 [1:13:50<1:50:47,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:21,512] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2569/6501 [1:13:52<1:56:38,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9041, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2569/6501 [1:13:52<1:56:38,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:23,379] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2570/6501 [1:13:54<1:58:19,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.339, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2570/6501 [1:13:54<1:58:19,  1.81s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2571/6501 [1:13:55<1:47:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3314, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2571/6501 [1:13:55<1:47:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2572/6501 [1:13:57<1:50:05,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0395, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2572/6501 [1:13:57<1:50:05,  1.68s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2573/6501 [1:13:59<1:46:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2429, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2573/6501 [1:13:59<1:46:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:29,861] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2574/6501 [1:14:01<1:52:38,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1989, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2574/6501 [1:14:01<1:52:38,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:31,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2575/6501 [1:14:02<1:50:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9467, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2575/6501 [1:14:02<1:50:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2576/6501 [1:14:03<1:37:40,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0555, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2576/6501 [1:14:03<1:37:40,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:34,721] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2577/6501 [1:14:05<1:51:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9985, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2577/6501 [1:14:05<1:51:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2578/6501 [1:14:07<1:49:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9857, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2578/6501 [1:14:07<1:49:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:37,994] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2579/6501 [1:14:09<1:49:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2911, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2579/6501 [1:14:09<1:49:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2580/6501 [1:14:10<1:48:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7894, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2580/6501 [1:14:10<1:48:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:41,378] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2581/6501 [1:14:12<1:50:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9374, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2581/6501 [1:14:12<1:50:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:43,191] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2582/6501 [1:14:14<1:52:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.304, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2582/6501 [1:14:14<1:52:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2583/6501 [1:14:16<1:51:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1474, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2583/6501 [1:14:16<1:51:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:46,349] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2584/6501 [1:14:17<1:47:14,  1.64s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2584/6501 [1:14:17<1:47:14,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3566, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2585/6501 [1:14:18<1:33:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1409, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2585/6501 [1:14:18<1:33:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:49,250] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2586/6501 [1:14:20<1:43:49,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1317, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2586/6501 [1:14:20<1:43:49,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:50,874] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2587/6501 [1:14:22<1:44:25,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1025, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2587/6501 [1:14:22<1:44:25,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:52,563] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2588/6501 [1:14:23<1:46:08,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3908, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2588/6501 [1:14:23<1:46:08,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:54,282] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2589/6501 [1:14:25<1:47:53,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2589/6501 [1:14:25<1:47:53,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4772, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:56,028] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2590/6501 [1:14:27<1:49:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4545, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2590/6501 [1:14:27<1:49:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2591/6501 [1:14:29<1:54:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1282, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2591/6501 [1:14:29<1:54:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:33:59,808] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2592/6501 [1:14:31<1:56:07,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.248, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2592/6501 [1:14:31<1:56:07,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:01,762] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2593/6501 [1:14:32<1:59:27,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1351, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2593/6501 [1:14:32<1:59:27,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:03,665] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2594/6501 [1:14:34<2:00:47,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6541, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2594/6501 [1:14:34<2:00:47,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:05,517] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2595/6501 [1:14:36<2:00:41,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0034, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2595/6501 [1:14:36<2:00:41,  1.85s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2596/6501 [1:14:38<2:08:05,  1.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1111, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2596/6501 [1:14:38<2:08:05,  1.97s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:09,460] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2597/6501 [1:14:40<2:02:58,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0984, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2597/6501 [1:14:40<2:02:58,  1.89s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2598/6501 [1:14:42<2:04:06,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2084, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2598/6501 [1:14:42<2:04:06,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:13,057] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2599/6501 [1:14:44<1:58:59,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2172, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2599/6501 [1:14:44<1:58:59,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:14,867] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2600/6501 [1:14:46<1:58:34,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4276, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 2600/6501 [1:14:46<1:58:34,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:16,792] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2601/6501 [1:14:47<2:00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1335, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2601/6501 [1:14:47<2:00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:18,513] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2602/6501 [1:14:49<1:57:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4825, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2602/6501 [1:14:49<1:57:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2603/6501 [1:14:51<1:56:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7432, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2603/6501 [1:14:51<1:56:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2604/6501 [1:14:53<1:58:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.837, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2604/6501 [1:14:53<1:58:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2605/6501 [1:14:55<1:59:11,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9963, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2605/6501 [1:14:55<1:59:11,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:25,642] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2606/6501 [1:14:56<1:55:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3084, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2606/6501 [1:14:56<1:55:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:27,430] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2607/6501 [1:14:58<1:55:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1095, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2607/6501 [1:14:58<1:55:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2608/6501 [1:15:00<1:56:30,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2994, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2608/6501 [1:15:00<1:56:30,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:30,854] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2609/6501 [1:15:02<1:52:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.177, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2609/6501 [1:15:02<1:52:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:32,587] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2610/6501 [1:15:03<1:52:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0577, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2610/6501 [1:15:03<1:52:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:34,394] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2611/6501 [1:15:05<1:53:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5341, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2611/6501 [1:15:05<1:53:48,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:36,271] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2612/6501 [1:15:07<1:56:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3958, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2612/6501 [1:15:07<1:56:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:37,979] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2613/6501 [1:15:09<1:54:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1227, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2613/6501 [1:15:09<1:54:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:39,785] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2614/6501 [1:15:10<1:55:13,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3013, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2614/6501 [1:15:10<1:55:13,  1.78s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2615/6501 [1:15:12<1:55:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1227, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2615/6501 [1:15:12<1:55:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:43,336] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2616/6501 [1:15:14<1:55:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3204, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2616/6501 [1:15:14<1:55:03,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:45,071] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2617/6501 [1:15:16<1:54:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1909, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2617/6501 [1:15:16<1:54:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:47,265] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2618/6501 [1:15:18<2:02:31,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3383, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2618/6501 [1:15:18<2:02:31,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:48,949] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2619/6501 [1:15:20<1:58:25,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1743, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2619/6501 [1:15:20<1:58:25,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:50,894] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2620/6501 [1:15:22<2:00:37,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.809, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2620/6501 [1:15:22<2:00:37,  1.86s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2621/6501 [1:15:23<1:57:54,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9523, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2621/6501 [1:15:23<1:57:54,  1.82s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2622/6501 [1:15:25<1:56:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6022, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2622/6501 [1:15:25<1:56:41,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:34:56,195] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2623/6501 [1:15:27<1:56:48,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2352, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2623/6501 [1:15:27<1:56:48,  1.81s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2624/6501 [1:15:29<1:54:53,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5166, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2624/6501 [1:15:29<1:54:53,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:00,166] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2625/6501 [1:15:31<2:04:13,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9137, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2625/6501 [1:15:31<2:04:13,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:01,740] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2626/6501 [1:15:32<1:57:25,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2719, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2626/6501 [1:15:32<1:57:25,  1.82s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2627/6501 [1:15:34<1:55:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0394, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2627/6501 [1:15:34<1:55:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:05,194] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2628/6501 [1:15:36<1:54:20,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3932, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2628/6501 [1:15:36<1:54:20,  1.77s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2629/6501 [1:15:38<1:57:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2176, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2629/6501 [1:15:38<1:57:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:09,124] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2630/6501 [1:15:40<2:00:52,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2061, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2630/6501 [1:15:40<2:00:52,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:11,065] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2631/6501 [1:15:42<2:02:08,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.189, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2631/6501 [1:15:42<2:02:08,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:12,754] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m40%|████      | 2632/6501 [1:15:43<1:58:09,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.948, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 2632/6501 [1:15:43<1:58:09,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:14,453] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2633/6501 [1:15:45<1:55:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0653, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2633/6501 [1:15:45<1:55:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2634/6501 [1:15:47<1:54:04,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1985, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2634/6501 [1:15:47<1:54:04,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:17,970] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2635/6501 [1:15:49<1:54:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8811, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2635/6501 [1:15:49<1:54:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2636/6501 [1:15:50<1:49:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6075, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2636/6501 [1:15:50<1:49:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:21,080] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2637/6501 [1:15:52<1:47:17,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8286, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2637/6501 [1:15:52<1:47:17,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:22,929] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2638/6501 [1:15:54<1:50:47,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2884, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2638/6501 [1:15:54<1:50:47,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:24,369] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2639/6501 [1:15:55<1:45:20,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3554, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2639/6501 [1:15:55<1:45:20,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:26,094] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2640/6501 [1:15:57<1:47:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4514, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2640/6501 [1:15:57<1:47:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:27,958] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2641/6501 [1:15:59<1:50:52,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0406, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2641/6501 [1:15:59<1:50:52,  1.72s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2642/6501 [1:16:00<1:45:10,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4199, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2642/6501 [1:16:00<1:45:10,  1.64s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2643/6501 [1:16:02<1:44:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1496, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2643/6501 [1:16:02<1:44:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:32,635] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2644/6501 [1:16:03<1:44:53,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3626, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2644/6501 [1:16:03<1:44:53,  1.63s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2645/6501 [1:16:05<1:45:24,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7276, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2645/6501 [1:16:05<1:45:24,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:36,154] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2646/6501 [1:16:07<1:49:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5898, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2646/6501 [1:16:07<1:49:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:37,925] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2647/6501 [1:16:09<1:50:49,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.827, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2647/6501 [1:16:09<1:50:49,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:40,053] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2648/6501 [1:16:11<1:58:32,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2212, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2648/6501 [1:16:11<1:58:32,  1.85s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2649/6501 [1:16:12<1:52:28,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4852, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2649/6501 [1:16:12<1:52:28,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:43,429] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2650/6501 [1:16:14<1:54:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1427, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2650/6501 [1:16:14<1:54:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:45,380] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2651/6501 [1:16:16<1:57:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1629, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2651/6501 [1:16:16<1:57:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:47,278] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2652/6501 [1:16:18<1:58:44,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1668, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2652/6501 [1:16:18<1:58:44,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:49,131] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2653/6501 [1:16:20<1:58:45,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5712, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2653/6501 [1:16:20<1:58:45,  1.85s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2654/6501 [1:16:21<1:54:15,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9732, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2654/6501 [1:16:21<1:54:15,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:52,417] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2655/6501 [1:16:23<1:52:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9093, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2655/6501 [1:16:23<1:52:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:54,060] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2656/6501 [1:16:25<1:49:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3191, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2656/6501 [1:16:25<1:49:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2657/6501 [1:16:27<1:55:14,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2007, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2657/6501 [1:16:27<1:55:14,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:57,902] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2658/6501 [1:16:29<1:56:12,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1494, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2658/6501 [1:16:29<1:56:12,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:35:59,682] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2659/6501 [1:16:30<1:55:30,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.163, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2659/6501 [1:16:30<1:55:30,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:01,541] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2660/6501 [1:16:32<1:56:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0405, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2660/6501 [1:16:32<1:56:32,  1.82s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2661/6501 [1:16:34<1:52:56,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2395, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2661/6501 [1:16:34<1:52:56,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:04,632] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2662/6501 [1:16:35<1:46:58,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4426, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2662/6501 [1:16:35<1:46:58,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:06,339] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2663/6501 [1:16:37<1:47:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9558, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2663/6501 [1:16:37<1:47:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:08,492] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2664/6501 [1:16:39<1:56:38,  1.82s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2664/6501 [1:16:39<1:56:38,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9761, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:10,270] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2665/6501 [1:16:41<1:55:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3541, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2665/6501 [1:16:41<1:55:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:12,072] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2666/6501 [1:16:43<1:55:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.335, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2666/6501 [1:16:43<1:55:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:13,717] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2667/6501 [1:16:44<1:52:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1451, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2667/6501 [1:16:44<1:52:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:15,460] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2668/6501 [1:16:46<1:52:03,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3687, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2668/6501 [1:16:46<1:52:03,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:17,144] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2669/6501 [1:16:48<1:50:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3443, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2669/6501 [1:16:48<1:50:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2670/6501 [1:16:50<1:55:20,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.762, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2670/6501 [1:16:50<1:55:20,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:20,795] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2671/6501 [1:16:51<1:52:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2776, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2671/6501 [1:16:51<1:52:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:22,842] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2672/6501 [1:16:54<1:58:06,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2221, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2672/6501 [1:16:54<1:58:06,  1.85s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2673/6501 [1:16:55<1:54:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3029, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2673/6501 [1:16:55<1:54:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2674/6501 [1:16:57<1:49:53,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4587, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2674/6501 [1:16:57<1:49:53,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:27,937] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2675/6501 [1:16:59<1:52:42,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0322, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2675/6501 [1:16:59<1:52:42,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:29,726] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2676/6501 [1:17:00<1:53:06,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7871, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2676/6501 [1:17:00<1:53:06,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:31,271] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2677/6501 [1:17:02<1:48:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0638, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2677/6501 [1:17:02<1:48:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2678/6501 [1:17:04<1:51:56,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2615, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2678/6501 [1:17:04<1:51:56,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:35,010] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2679/6501 [1:17:06<1:53:54,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1727, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2679/6501 [1:17:06<1:53:54,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:36,621] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 2680/6501 [1:17:07<1:50:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3248, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2680/6501 [1:17:07<1:50:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 2681/6501 [1:17:09<1:47:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2322, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 2681/6501 [1:17:09<1:47:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:39,816] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2682/6501 [1:17:11<1:46:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0637, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2682/6501 [1:17:11<1:46:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:41,169] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2683/6501 [1:17:12<1:40:02,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2964, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2683/6501 [1:17:12<1:40:02,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:43,201] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2684/6501 [1:17:14<1:48:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5292, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2684/6501 [1:17:14<1:48:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2685/6501 [1:17:16<1:48:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5613, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2685/6501 [1:17:16<1:48:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:46,602] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2686/6501 [1:17:17<1:48:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0447, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2686/6501 [1:17:17<1:48:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2687/6501 [1:17:19<1:41:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1383, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2687/6501 [1:17:19<1:41:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2688/6501 [1:17:20<1:39:24,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1852, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2688/6501 [1:17:20<1:39:24,  1.56s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2689/6501 [1:17:22<1:38:27,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1124, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2689/6501 [1:17:22<1:38:27,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:52,709] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2690/6501 [1:17:23<1:42:12,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0149, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2690/6501 [1:17:23<1:42:12,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:54,519] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2691/6501 [1:17:25<1:46:00,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1682, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2691/6501 [1:17:25<1:46:00,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:56,325] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2692/6501 [1:17:27<1:48:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2509, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2692/6501 [1:17:27<1:48:34,  1.71s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2693/6501 [1:17:29<1:48:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2023, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2693/6501 [1:17:29<1:48:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:36:59,841] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2694/6501 [1:17:31<1:50:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2421, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2694/6501 [1:17:31<1:50:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:01,815] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2695/6501 [1:17:33<1:54:42,  1.81s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2695/6501 [1:17:33<1:54:42,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.838, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2696/6501 [1:17:34<1:55:56,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1553, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2696/6501 [1:17:34<1:55:56,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:05,553] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2697/6501 [1:17:36<1:56:35,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1756, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 2697/6501 [1:17:36<1:56:35,  1.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2698/6501 [1:17:38<1:50:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0755, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2698/6501 [1:17:38<1:50:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:09,133] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2699/6501 [1:17:40<1:56:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2699/6501 [1:17:40<1:56:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2432, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2700/6501 [1:17:41<1:50:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.262, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2700/6501 [1:17:41<1:50:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:12,736] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2701/6501 [1:17:43<1:56:38,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3758, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2701/6501 [1:17:43<1:56:38,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:14,646] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2702/6501 [1:17:45<1:57:53,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0855, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2702/6501 [1:17:45<1:57:53,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:16,314] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2703/6501 [1:17:47<1:54:11,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.963, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2703/6501 [1:17:47<1:54:11,  1.80s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2704/6501 [1:17:49<1:55:01,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3643, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2704/6501 [1:17:49<1:55:01,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:19,776] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2705/6501 [1:17:50<1:51:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0641, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2705/6501 [1:17:50<1:51:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:21,566] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2706/6501 [1:17:52<1:51:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3751, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2706/6501 [1:17:52<1:51:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:23,175] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2707/6501 [1:17:54<1:48:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4529, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2707/6501 [1:17:54<1:48:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:24,870] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2708/6501 [1:17:56<1:48:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3894, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2708/6501 [1:17:56<1:48:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2709/6501 [1:17:57<1:44:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0628, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2709/6501 [1:17:57<1:44:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:28,098] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2710/6501 [1:17:59<1:45:40,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.652, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2710/6501 [1:17:59<1:45:40,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:29,521] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2711/6501 [1:18:00<1:40:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1986, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2711/6501 [1:18:00<1:40:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:31,450] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2712/6501 [1:18:02<1:47:10,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0274, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2712/6501 [1:18:02<1:47:10,  1.70s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2713/6501 [1:18:04<1:49:43,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8686, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2713/6501 [1:18:04<1:49:43,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:35,013] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2714/6501 [1:18:06<1:49:32,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2186, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2714/6501 [1:18:06<1:49:32,  1.74s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2715/6501 [1:18:07<1:44:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.694, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2715/6501 [1:18:07<1:44:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:38,399] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2716/6501 [1:18:09<1:49:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9372, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2716/6501 [1:18:09<1:49:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:40,082] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2717/6501 [1:18:11<1:48:22,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1982, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2717/6501 [1:18:11<1:48:22,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:42,075] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2718/6501 [1:18:13<1:53:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8919, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2718/6501 [1:18:13<1:53:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:44,098] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2719/6501 [1:18:15<1:57:43,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2649, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2719/6501 [1:18:15<1:57:43,  1.87s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2720/6501 [1:18:17<1:55:34,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5729, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2720/6501 [1:18:17<1:55:34,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:47,562] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2721/6501 [1:18:18<1:53:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0478, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2721/6501 [1:18:18<1:53:09,  1.80s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2722/6501 [1:18:20<1:49:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2161, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2722/6501 [1:18:20<1:49:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:50,921] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2723/6501 [1:18:22<1:49:55,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9146, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2723/6501 [1:18:22<1:49:55,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:52,429] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2724/6501 [1:18:23<1:45:23,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2949, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2724/6501 [1:18:23<1:45:23,  1.67s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2725/6501 [1:18:25<1:44:14,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9932, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2725/6501 [1:18:25<1:44:14,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:55,881] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2726/6501 [1:18:27<1:47:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4323, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2726/6501 [1:18:27<1:47:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:57,638] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2727/6501 [1:18:28<1:48:29,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.37, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2727/6501 [1:18:28<1:48:29,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:37:59,528] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2728/6501 [1:18:30<1:51:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4473, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2728/6501 [1:18:30<1:51:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:01,416] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2729/6501 [1:18:32<1:53:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1718, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2729/6501 [1:18:32<1:53:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:03,034] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2730/6501 [1:18:34<1:50:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.057, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2730/6501 [1:18:34<1:50:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:05,046] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2731/6501 [1:18:36<1:54:57,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3616, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2731/6501 [1:18:36<1:54:57,  1.83s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2732/6501 [1:18:38<1:55:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0734, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2732/6501 [1:18:38<1:55:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:08,146] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2733/6501 [1:18:39<1:44:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.956, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2733/6501 [1:18:39<1:44:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:09,894] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2734/6501 [1:18:41<1:45:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1456, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2734/6501 [1:18:41<1:45:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2735/6501 [1:18:42<1:46:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.426, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2735/6501 [1:18:42<1:46:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:13,440] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2736/6501 [1:18:44<1:48:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3517, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2736/6501 [1:18:44<1:48:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:15,355] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2737/6501 [1:18:46<1:52:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5022, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2737/6501 [1:18:46<1:52:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:17,143] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2738/6501 [1:18:48<1:52:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.996, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2738/6501 [1:18:48<1:52:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:18,830] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2739/6501 [1:18:50<1:50:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1284, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2739/6501 [1:18:50<1:50:12,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:20,429] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2740/6501 [1:18:51<1:47:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9447, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2740/6501 [1:18:51<1:47:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:22,123] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2741/6501 [1:18:53<1:46:52,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1938, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2741/6501 [1:18:53<1:46:52,  1.71s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2742/6501 [1:18:55<1:47:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3729, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2742/6501 [1:18:55<1:47:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:25,823] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2743/6501 [1:18:57<1:52:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.148, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2743/6501 [1:18:57<1:52:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:27,459] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2744/6501 [1:18:58<1:49:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9672, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2744/6501 [1:18:58<1:49:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2745/6501 [1:19:00<1:43:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1864, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2745/6501 [1:19:00<1:43:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:30,556] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2746/6501 [1:19:01<1:43:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0842, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2746/6501 [1:19:01<1:43:31,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:32,073] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2747/6501 [1:19:03<1:40:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2726, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2747/6501 [1:19:03<1:40:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:33,798] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2748/6501 [1:19:04<1:42:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8502, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2748/6501 [1:19:04<1:42:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:35,614] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2749/6501 [1:19:06<1:46:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3727, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2749/6501 [1:19:06<1:46:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:37,319] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2750/6501 [1:19:08<1:46:15,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7423, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2750/6501 [1:19:08<1:46:15,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:38,999] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2751/6501 [1:19:10<1:45:51,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9393, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2751/6501 [1:19:10<1:45:51,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:40,938] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2752/6501 [1:19:12<1:50:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7859, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2752/6501 [1:19:12<1:50:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:42,851] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2753/6501 [1:19:14<1:53:07,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1698, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2753/6501 [1:19:14<1:53:07,  1.81s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2754/6501 [1:19:15<1:47:56,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2305, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2754/6501 [1:19:15<1:47:56,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:46,274] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2755/6501 [1:19:17<1:50:52,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1586, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2755/6501 [1:19:17<1:50:52,  1.78s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2756/6501 [1:19:19<1:48:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.313, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2756/6501 [1:19:19<1:48:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:49,759] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2757/6501 [1:19:20<1:50:15,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9573, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2757/6501 [1:19:20<1:50:15,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:51,816] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2758/6501 [1:19:23<1:55:38,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2026, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2758/6501 [1:19:23<1:55:38,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:53,785] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2759/6501 [1:19:24<1:57:46,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1772, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2759/6501 [1:19:24<1:57:46,  1.89s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2760/6501 [1:19:26<1:52:10,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4301, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2760/6501 [1:19:26<1:52:10,  1.80s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2761/6501 [1:19:28<1:52:03,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8904, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2761/6501 [1:19:28<1:52:03,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:38:58,785] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2762/6501 [1:19:29<1:48:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2918, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 2762/6501 [1:19:29<1:48:36,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:00,659] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2763/6501 [1:19:31<1:51:02,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.252, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2763/6501 [1:19:31<1:51:02,  1.78s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2764/6501 [1:19:33<1:48:21,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0678, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2764/6501 [1:19:33<1:48:21,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:04,014] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2765/6501 [1:19:35<1:47:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2527, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2765/6501 [1:19:35<1:47:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:05,890] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2766/6501 [1:19:37<1:50:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1971, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2766/6501 [1:19:37<1:50:30,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:07,774] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2767/6501 [1:19:38<1:52:30,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0157, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2767/6501 [1:19:38<1:52:30,  1.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2768/6501 [1:19:40<1:54:16,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4801, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2768/6501 [1:19:40<1:54:16,  1.84s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2769/6501 [1:19:42<1:49:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1529, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2769/6501 [1:19:42<1:49:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2770/6501 [1:19:44<1:51:13,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1512, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2770/6501 [1:19:44<1:51:13,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:14,736] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2771/6501 [1:19:45<1:47:55,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.35, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2771/6501 [1:19:45<1:47:55,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:16,361] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2772/6501 [1:19:47<1:45:49,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2747, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2772/6501 [1:19:47<1:45:49,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:18,081] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2773/6501 [1:19:49<1:46:06,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1291, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2773/6501 [1:19:49<1:46:06,  1.71s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2774/6501 [1:19:51<1:48:11,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3513, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2774/6501 [1:19:51<1:48:11,  1.74s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2775/6501 [1:19:52<1:49:19,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1111, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2775/6501 [1:19:52<1:49:19,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:23,521] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2776/6501 [1:19:54<1:50:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3127, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2776/6501 [1:19:54<1:50:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2777/6501 [1:19:56<1:51:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1392, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2777/6501 [1:19:56<1:51:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:27,005] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2778/6501 [1:19:58<1:48:36,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9443, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2778/6501 [1:19:58<1:48:36,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:28,650] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2779/6501 [1:19:59<1:46:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1895, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2779/6501 [1:19:59<1:46:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2780/6501 [1:20:01<1:48:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9486, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2780/6501 [1:20:01<1:48:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:32,329] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2781/6501 [1:20:03<1:50:27,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1551, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2781/6501 [1:20:03<1:50:27,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:34,158] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2782/6501 [1:20:05<1:51:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2591, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2782/6501 [1:20:05<1:51:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:36,154] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2783/6501 [1:20:07<1:55:00,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2277, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2783/6501 [1:20:07<1:55:00,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:37,849] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2784/6501 [1:20:09<1:51:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1956, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2784/6501 [1:20:09<1:51:58,  1.81s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2785/6501 [1:20:10<1:49:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0009, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2785/6501 [1:20:10<1:49:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:41,057] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2786/6501 [1:20:12<1:45:07,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1265, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2786/6501 [1:20:12<1:45:07,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:43,165] [WARNING] [stage3.py:1787:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2787/6501 [1:20:14<1:52:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1155, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2787/6501 [1:20:14<1:52:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2788/6501 [1:20:16<1:51:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2916, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2788/6501 [1:20:16<1:51:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2789/6501 [1:20:17<1:48:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4579, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2789/6501 [1:20:17<1:48:27,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:48,482] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2790/6501 [1:20:19<1:51:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8961, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2790/6501 [1:20:19<1:51:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2791/6501 [1:20:20<1:35:26,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1124, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2791/6501 [1:20:20<1:35:26,  1.54s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2792/6501 [1:20:22<1:35:51,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.087, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2792/6501 [1:20:22<1:35:51,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:52,576] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2793/6501 [1:20:23<1:36:32,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1571, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2793/6501 [1:20:23<1:36:32,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:54,551] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2794/6501 [1:20:25<1:44:11,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.065, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2794/6501 [1:20:25<1:44:11,  1.69s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2795/6501 [1:20:27<1:45:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8527, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2795/6501 [1:20:27<1:45:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:58,089] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2796/6501 [1:20:29<1:46:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3688, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2796/6501 [1:20:29<1:46:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:39:59,866] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2797/6501 [1:20:31<1:47:41,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1694, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2797/6501 [1:20:31<1:47:41,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:01,566] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2798/6501 [1:20:32<1:46:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0565, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2798/6501 [1:20:32<1:46:51,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:03,228] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2799/6501 [1:20:34<1:45:32,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1788, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2799/6501 [1:20:34<1:45:32,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:04,974] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2800/6501 [1:20:36<1:46:10,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2038, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2800/6501 [1:20:36<1:46:10,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:06,821] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2801/6501 [1:20:38<1:48:27,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9648, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2801/6501 [1:20:38<1:48:27,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:08,690] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2802/6501 [1:20:39<1:50:28,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1013, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2802/6501 [1:20:39<1:50:28,  1.79s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2803/6501 [1:20:41<1:49:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.415, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2803/6501 [1:20:41<1:49:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:12,114] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2804/6501 [1:20:43<1:47:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2505, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2804/6501 [1:20:43<1:47:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2805/6501 [1:20:44<1:45:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2845, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2805/6501 [1:20:44<1:45:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2806/6501 [1:20:46<1:46:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1513, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2806/6501 [1:20:46<1:46:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2807/6501 [1:20:48<1:43:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3801, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2807/6501 [1:20:48<1:43:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2808/6501 [1:20:49<1:43:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9544, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2808/6501 [1:20:49<1:43:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2809/6501 [1:20:51<1:42:08,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3254, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2809/6501 [1:20:51<1:42:08,  1.66s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2810/6501 [1:20:53<1:45:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.126, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2810/6501 [1:20:53<1:45:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2811/6501 [1:20:55<1:43:58,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1525, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2811/6501 [1:20:55<1:43:58,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:25,482] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2812/6501 [1:20:56<1:42:44,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6395, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2812/6501 [1:20:56<1:42:44,  1.67s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2813/6501 [1:20:58<1:38:14,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2125, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2813/6501 [1:20:58<1:38:14,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:28,824] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2814/6501 [1:21:00<1:44:02,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2431, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2814/6501 [1:21:00<1:44:02,  1.69s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2815/6501 [1:21:01<1:43:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9458, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2815/6501 [1:21:01<1:43:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:32,353] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2816/6501 [1:21:03<1:46:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0419, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2816/6501 [1:21:03<1:46:50,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:34,198] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2817/6501 [1:21:05<1:48:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7508, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2817/6501 [1:21:05<1:48:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:36,025] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2818/6501 [1:21:07<1:49:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0608, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2818/6501 [1:21:07<1:49:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:37,924] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2819/6501 [1:21:09<1:51:45,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2522, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2819/6501 [1:21:09<1:51:45,  1.82s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2820/6501 [1:21:10<1:50:33,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5955, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2820/6501 [1:21:10<1:50:33,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:41,620] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2821/6501 [1:21:12<1:53:02,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.457, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2821/6501 [1:21:12<1:53:02,  1.84s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2822/6501 [1:21:14<1:47:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2919, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2822/6501 [1:21:14<1:47:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:44,873] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2823/6501 [1:21:16<1:46:39,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1695, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2823/6501 [1:21:16<1:46:39,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:46,694] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2824/6501 [1:21:17<1:48:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2184, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2824/6501 [1:21:17<1:48:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:48,434] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2825/6501 [1:21:19<1:47:38,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0519, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2825/6501 [1:21:19<1:47:38,  1.76s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2826/6501 [1:21:21<1:41:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.543, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2826/6501 [1:21:21<1:41:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:51,975] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2827/6501 [1:21:23<1:50:01,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6888, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 2827/6501 [1:21:23<1:50:01,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:54,068] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2828/6501 [1:21:25<1:55:26,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4019, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2828/6501 [1:21:25<1:55:26,  1.89s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2829/6501 [1:21:27<1:54:47,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0969, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2829/6501 [1:21:27<1:54:47,  1.88s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2830/6501 [1:21:28<1:39:23,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3159, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2830/6501 [1:21:28<1:39:23,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:40:58,907] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2831/6501 [1:21:30<1:45:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9831, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2831/6501 [1:21:30<1:45:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:00,644] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2832/6501 [1:21:31<1:45:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0193, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2832/6501 [1:21:31<1:45:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2833/6501 [1:21:33<1:45:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1756, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2833/6501 [1:21:33<1:45:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:04,054] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2834/6501 [1:21:35<1:44:39,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1581, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2834/6501 [1:21:35<1:44:39,  1.71s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2835/6501 [1:21:36<1:41:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0899, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2835/6501 [1:21:36<1:41:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:07,494] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2836/6501 [1:21:38<1:45:41,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.099, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2836/6501 [1:21:38<1:45:41,  1.73s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2837/6501 [1:21:40<1:44:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.336, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2837/6501 [1:21:40<1:44:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:10,889] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2838/6501 [1:21:42<1:44:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1195, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2838/6501 [1:21:42<1:44:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2839/6501 [1:21:43<1:43:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.158, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2839/6501 [1:21:43<1:43:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:14,745] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2840/6501 [1:21:45<1:52:48,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8348, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2840/6501 [1:21:45<1:52:48,  1.85s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2841/6501 [1:21:47<1:50:23,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1239, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2841/6501 [1:21:47<1:50:23,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:18,247] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2842/6501 [1:21:49<1:49:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1898, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2842/6501 [1:21:49<1:49:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2843/6501 [1:21:50<1:38:08,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3105, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2843/6501 [1:21:50<1:38:08,  1.61s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2844/6501 [1:21:52<1:40:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2069, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 2844/6501 [1:21:52<1:40:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:22,958] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2845/6501 [1:21:54<1:43:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7915, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2845/6501 [1:21:54<1:43:25,  1.70s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2846/6501 [1:21:55<1:43:21,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.219, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2846/6501 [1:21:55<1:43:21,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:26,345] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2847/6501 [1:21:57<1:43:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0624, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2847/6501 [1:21:57<1:43:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:28,232] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2848/6501 [1:21:59<1:46:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1158, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2848/6501 [1:21:59<1:46:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:30,010] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2849/6501 [1:22:01<1:47:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.425, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2849/6501 [1:22:01<1:47:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:31,707] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2850/6501 [1:22:02<1:45:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3441, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2850/6501 [1:22:02<1:45:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:33,460] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2851/6501 [1:22:04<1:46:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2589, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2851/6501 [1:22:04<1:46:08,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:35,795] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2852/6501 [1:22:06<1:56:53,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1322, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2852/6501 [1:22:06<1:56:53,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:37,477] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2853/6501 [1:22:08<1:52:28,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1859, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2853/6501 [1:22:08<1:52:28,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:39,390] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2854/6501 [1:22:10<1:53:35,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2243, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2854/6501 [1:22:10<1:53:35,  1.87s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2855/6501 [1:22:12<1:51:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1652, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2855/6501 [1:22:12<1:51:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:42,617] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2856/6501 [1:22:13<1:44:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2062, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2856/6501 [1:22:13<1:44:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:44,417] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2857/6501 [1:22:15<1:46:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1686, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2857/6501 [1:22:15<1:46:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2858/6501 [1:22:17<1:43:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3361, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2858/6501 [1:22:17<1:43:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:48,107] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2859/6501 [1:22:19<1:50:34,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4084, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2859/6501 [1:22:19<1:50:34,  1.82s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2860/6501 [1:22:20<1:46:20,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1682, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2860/6501 [1:22:20<1:46:20,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:51,705] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2861/6501 [1:22:22<1:50:56,  1.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2861/6501 [1:22:22<1:50:56,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2889, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2862/6501 [1:22:24<1:53:39,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3957, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2862/6501 [1:22:24<1:53:39,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:55,149] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2863/6501 [1:22:26<1:46:10,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9715, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2863/6501 [1:22:26<1:46:10,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:41:56,967] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2864/6501 [1:22:28<1:47:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2048, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2864/6501 [1:22:28<1:47:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2865/6501 [1:22:29<1:44:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1753, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2865/6501 [1:22:29<1:44:21,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:00,330] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2866/6501 [1:22:31<1:44:56,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.397, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2866/6501 [1:22:31<1:44:56,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:02,007] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2867/6501 [1:22:33<1:43:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0652, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2867/6501 [1:22:33<1:43:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:03,880] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2868/6501 [1:22:35<1:46:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4985, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2868/6501 [1:22:35<1:46:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:05,522] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2869/6501 [1:22:36<1:44:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1655, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2869/6501 [1:22:36<1:44:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:07,125] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2870/6501 [1:22:38<1:42:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8778, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2870/6501 [1:22:38<1:42:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2871/6501 [1:22:40<1:42:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1942, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2871/6501 [1:22:40<1:42:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:10,553] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2872/6501 [1:22:41<1:42:52,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9342, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2872/6501 [1:22:41<1:42:52,  1.70s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2873/6501 [1:22:43<1:42:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3474, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2873/6501 [1:22:43<1:42:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:14,200] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2874/6501 [1:22:45<1:47:21,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4844, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2874/6501 [1:22:45<1:47:21,  1.78s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2875/6501 [1:22:47<1:48:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2875/6501 [1:22:47<1:48:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2529, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:17,878] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2876/6501 [1:22:49<1:49:09,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2938, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2876/6501 [1:22:49<1:49:09,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:19,705] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2877/6501 [1:22:50<1:49:30,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1105, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2877/6501 [1:22:50<1:49:30,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:21,348] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2878/6501 [1:22:52<1:46:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1792, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2878/6501 [1:22:52<1:46:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:23,076] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2879/6501 [1:22:54<1:45:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1081, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2879/6501 [1:22:54<1:45:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:24,868] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2880/6501 [1:22:56<1:46:26,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0496, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2880/6501 [1:22:56<1:46:26,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:26,565] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2881/6501 [1:22:57<1:45:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4399, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2881/6501 [1:22:57<1:45:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2882/6501 [1:22:59<1:46:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2872, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2882/6501 [1:22:59<1:46:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:30,069] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2883/6501 [1:23:01<1:45:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0701, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2883/6501 [1:23:01<1:45:04,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:31,841] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2884/6501 [1:23:03<1:45:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9943, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2884/6501 [1:23:03<1:45:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2885/6501 [1:23:04<1:43:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2628, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2885/6501 [1:23:04<1:43:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:35,693] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2886/6501 [1:23:06<1:52:25,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6128, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2886/6501 [1:23:06<1:52:25,  1.87s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2887/6501 [1:23:08<1:47:27,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.51, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2887/6501 [1:23:08<1:47:27,  1.78s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2888/6501 [1:23:09<1:33:58,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3395, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2888/6501 [1:23:09<1:33:58,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:40,413] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2889/6501 [1:23:11<1:43:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3272, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2889/6501 [1:23:11<1:43:28,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:42,195] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2890/6501 [1:23:13<1:44:34,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3112, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2890/6501 [1:23:13<1:44:34,  1.74s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2891/6501 [1:23:15<1:42:20,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.324, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2891/6501 [1:23:15<1:42:20,  1.70s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2892/6501 [1:23:16<1:33:43,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2393, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 2892/6501 [1:23:16<1:33:43,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:47,269] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2893/6501 [1:23:18<1:45:53,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6157, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2893/6501 [1:23:18<1:45:53,  1.76s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2894/6501 [1:23:19<1:40:44,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0639, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2894/6501 [1:23:19<1:40:44,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:50,743] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2895/6501 [1:23:21<1:46:30,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3719, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2895/6501 [1:23:21<1:46:30,  1.77s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2896/6501 [1:23:23<1:44:44,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0347, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2896/6501 [1:23:23<1:44:44,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:54,288] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2897/6501 [1:23:25<1:46:58,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0245, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2897/6501 [1:23:25<1:46:58,  1.78s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2898/6501 [1:23:27<1:47:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2348, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2898/6501 [1:23:27<1:47:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:57,634] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2899/6501 [1:23:28<1:42:48,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2126, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2899/6501 [1:23:28<1:42:48,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:42:59,232] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2900/6501 [1:23:30<1:40:43,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1228, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2900/6501 [1:23:30<1:40:43,  1.68s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2901/6501 [1:23:32<1:40:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6223, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2901/6501 [1:23:32<1:40:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:02,617] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2902/6501 [1:23:33<1:41:14,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4141, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2902/6501 [1:23:33<1:41:14,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:04,405] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2903/6501 [1:23:35<1:43:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2096, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2903/6501 [1:23:35<1:43:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:06,223] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2904/6501 [1:23:37<1:44:46,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3093, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2904/6501 [1:23:37<1:44:46,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:08,314] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2905/6501 [1:23:39<1:50:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5566, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2905/6501 [1:23:39<1:50:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2906/6501 [1:23:41<1:45:54,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4619, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2906/6501 [1:23:41<1:45:54,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:11,577] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2907/6501 [1:23:42<1:44:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3986, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2907/6501 [1:23:42<1:44:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:13,587] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2908/6501 [1:23:44<1:49:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2187, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2908/6501 [1:23:44<1:49:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:15,334] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2909/6501 [1:23:46<1:47:48,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2729, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2909/6501 [1:23:46<1:47:48,  1.80s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2910/6501 [1:23:48<1:48:20,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0007, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2910/6501 [1:23:48<1:48:20,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:18,802] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2911/6501 [1:23:49<1:45:11,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3103, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2911/6501 [1:23:50<1:45:11,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:20,620] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2912/6501 [1:23:51<1:46:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1424, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2912/6501 [1:23:51<1:46:14,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:22,351] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2913/6501 [1:23:53<1:45:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8739, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2913/6501 [1:23:53<1:45:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:24,070] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2914/6501 [1:23:55<1:44:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3626, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2914/6501 [1:23:55<1:44:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:25,786] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2915/6501 [1:23:56<1:43:56,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4064, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2915/6501 [1:23:56<1:43:56,  1.74s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2916/6501 [1:23:58<1:33:07,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2447, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2916/6501 [1:23:58<1:33:07,  1.56s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2917/6501 [1:23:59<1:33:42,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4942, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2917/6501 [1:23:59<1:33:42,  1.57s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2918/6501 [1:24:00<1:21:48,  1.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.571, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2918/6501 [1:24:00<1:21:48,  1.37s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:31,338] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2919/6501 [1:24:02<1:31:34,  1.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9858, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2919/6501 [1:24:02<1:31:34,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:32,906] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2920/6501 [1:24:04<1:32:09,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4043, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2920/6501 [1:24:04<1:32:09,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:34,775] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2921/6501 [1:24:05<1:37:57,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5297, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2921/6501 [1:24:05<1:37:57,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:36,650] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2922/6501 [1:24:07<1:42:05,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7906, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2922/6501 [1:24:07<1:42:05,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:38,400] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2923/6501 [1:24:09<1:42:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3375, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2923/6501 [1:24:09<1:42:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:40,328] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2924/6501 [1:24:11<1:46:23,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.383, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2924/6501 [1:24:11<1:46:23,  1.78s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2925/6501 [1:24:13<1:42:56,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1117, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 2925/6501 [1:24:13<1:42:56,  1.73s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2926/6501 [1:24:14<1:43:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0176, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2926/6501 [1:24:14<1:43:59,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:45,732] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2927/6501 [1:24:16<1:48:54,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0793, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2927/6501 [1:24:16<1:48:54,  1.83s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2928/6501 [1:24:18<1:48:38,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.198, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2928/6501 [1:24:18<1:48:38,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:49,006] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2929/6501 [1:24:20<1:42:05,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7803, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2929/6501 [1:24:20<1:42:05,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:51,114] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2930/6501 [1:24:22<1:49:05,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.287, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2930/6501 [1:24:22<1:49:05,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:52,948] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2931/6501 [1:24:24<1:49:04,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4684, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2931/6501 [1:24:24<1:49:04,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:54,091] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2932/6501 [1:24:25<1:36:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.008, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2932/6501 [1:24:25<1:36:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:55,955] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2933/6501 [1:24:27<1:40:56,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2169, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2933/6501 [1:24:27<1:40:56,  1.70s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2934/6501 [1:24:28<1:34:25,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2813, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2934/6501 [1:24:28<1:34:25,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:43:59,130] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2935/6501 [1:24:30<1:38:54,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3138, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2935/6501 [1:24:30<1:38:54,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:01,011] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2936/6501 [1:24:32<1:42:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9254, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2936/6501 [1:24:32<1:42:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2937/6501 [1:24:33<1:40:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3871, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2937/6501 [1:24:33<1:40:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2938/6501 [1:24:34<1:26:06,  1.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.153, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2938/6501 [1:24:34<1:26:06,  1.45s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:05,232] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2939/6501 [1:24:36<1:31:12,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9818, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2939/6501 [1:24:36<1:31:12,  1.54s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2940/6501 [1:24:38<1:33:46,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3525, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2940/6501 [1:24:38<1:33:46,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:08,697] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2941/6501 [1:24:39<1:37:21,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0493, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2941/6501 [1:24:39<1:37:21,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2942/6501 [1:24:41<1:41:57,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3975, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2942/6501 [1:24:41<1:41:57,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:12,403] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2943/6501 [1:24:43<1:43:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9473, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2943/6501 [1:24:43<1:43:27,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:14,430] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2944/6501 [1:24:45<1:48:27,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9733, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2944/6501 [1:24:45<1:48:27,  1.83s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2945/6501 [1:24:47<1:44:13,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1439, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2945/6501 [1:24:47<1:44:13,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:17,747] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2946/6501 [1:24:48<1:43:34,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1425, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2946/6501 [1:24:48<1:43:34,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:19,452] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2947/6501 [1:24:50<1:42:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3555, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2947/6501 [1:24:50<1:42:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:21,298] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2948/6501 [1:24:52<1:44:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2137, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2948/6501 [1:24:52<1:44:43,  1.77s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2949/6501 [1:24:54<1:41:02,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4198, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2949/6501 [1:24:54<1:41:02,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:24,558] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2950/6501 [1:24:55<1:40:50,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0626, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2950/6501 [1:24:55<1:40:50,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:26,610] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2951/6501 [1:24:57<1:46:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4884, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2951/6501 [1:24:57<1:46:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:28,201] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2952/6501 [1:24:59<1:43:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9863, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2952/6501 [1:24:59<1:43:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:29,929] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2953/6501 [1:25:01<1:42:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0448, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2953/6501 [1:25:01<1:42:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2954/6501 [1:25:02<1:37:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3387, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2954/6501 [1:25:02<1:37:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2955/6501 [1:25:04<1:36:55,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2186, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2955/6501 [1:25:04<1:36:55,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2956/6501 [1:25:05<1:38:44,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3118, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2956/6501 [1:25:05<1:38:44,  1.67s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2957/6501 [1:25:07<1:39:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.356, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 2957/6501 [1:25:07<1:39:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2958/6501 [1:25:09<1:39:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4513, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2958/6501 [1:25:09<1:39:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:39,737] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2959/6501 [1:25:10<1:38:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0829, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2959/6501 [1:25:10<1:38:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:41,569] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2960/6501 [1:25:12<1:41:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2878, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2960/6501 [1:25:12<1:41:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2961/6501 [1:25:14<1:44:38,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2259, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2961/6501 [1:25:14<1:44:38,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:44,934] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2962/6501 [1:25:16<1:38:56,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5859, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2962/6501 [1:25:16<1:38:56,  1.68s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2963/6501 [1:25:17<1:36:23,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4021, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2963/6501 [1:25:17<1:36:23,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:47,878] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2964/6501 [1:25:19<1:32:23,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.261, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2964/6501 [1:25:19<1:32:23,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:49,841] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2965/6501 [1:25:21<1:39:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8657, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2965/6501 [1:25:21<1:39:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:51,350] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2966/6501 [1:25:22<1:36:11,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4304, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2966/6501 [1:25:22<1:36:11,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:53,108] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2967/6501 [1:25:24<1:38:22,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1185, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2967/6501 [1:25:24<1:38:22,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:44:55,048] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2968/6501 [1:25:26<1:43:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1482, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2968/6501 [1:25:26<1:43:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2969/6501 [1:25:27<1:42:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0885, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2969/6501 [1:25:27<1:42:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2970/6501 [1:25:29<1:40:31,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0527, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2970/6501 [1:25:29<1:40:31,  1.71s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2971/6501 [1:25:30<1:27:18,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.228, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2971/6501 [1:25:30<1:27:18,  1.48s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:01,169] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2972/6501 [1:25:32<1:32:56,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7503, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2972/6501 [1:25:32<1:32:56,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2973/6501 [1:25:34<1:36:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1105, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2973/6501 [1:25:34<1:36:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:04,611] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2974/6501 [1:25:35<1:36:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1791, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2974/6501 [1:25:35<1:36:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:06,411] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2975/6501 [1:25:37<1:39:30,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.364, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2975/6501 [1:25:37<1:39:30,  1.69s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2976/6501 [1:25:39<1:39:04,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.234, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2976/6501 [1:25:39<1:39:04,  1.69s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2977/6501 [1:25:40<1:39:06,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1557, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2977/6501 [1:25:40<1:39:06,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:11,568] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2978/6501 [1:25:42<1:41:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6392, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2978/6501 [1:25:42<1:41:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:13,287] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2979/6501 [1:25:44<1:40:57,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0588, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2979/6501 [1:25:44<1:40:57,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:15,295] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2980/6501 [1:25:46<1:46:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3373, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2980/6501 [1:25:46<1:46:00,  1.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2981/6501 [1:25:48<1:42:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3419, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2981/6501 [1:25:48<1:42:35,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:18,715] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2982/6501 [1:25:49<1:43:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9939, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2982/6501 [1:25:49<1:43:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:20,510] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2983/6501 [1:25:51<1:44:02,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.299, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2983/6501 [1:25:51<1:44:02,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:22,027] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2984/6501 [1:25:53<1:39:29,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4362, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2984/6501 [1:25:53<1:39:29,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:23,694] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2985/6501 [1:25:54<1:38:56,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3517, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2985/6501 [1:25:54<1:38:56,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:25,212] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2986/6501 [1:25:56<1:35:54,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9991, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2986/6501 [1:25:56<1:35:54,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:27,068] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2987/6501 [1:25:58<1:39:43,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5032, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2987/6501 [1:25:58<1:39:43,  1.70s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2988/6501 [1:26:00<1:41:04,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1414, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2988/6501 [1:26:00<1:41:04,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:30,492] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2989/6501 [1:26:01<1:39:34,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1146, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2989/6501 [1:26:01<1:39:34,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:32,219] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2990/6501 [1:26:03<1:40:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.7715, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2990/6501 [1:26:03<1:40:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2991/6501 [1:26:05<1:37:48,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2874, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2991/6501 [1:26:05<1:37:48,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:35,922] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2992/6501 [1:26:07<1:45:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4996, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2992/6501 [1:26:07<1:45:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:37,721] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2993/6501 [1:26:08<1:45:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1607, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2993/6501 [1:26:08<1:45:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:39,826] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2994/6501 [1:26:11<1:50:42,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7371, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2994/6501 [1:26:11<1:50:42,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:41,533] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2995/6501 [1:26:12<1:47:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3275, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2995/6501 [1:26:12<1:47:23,  1.84s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2996/6501 [1:26:14<1:42:50,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1619, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2996/6501 [1:26:14<1:42:50,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:44,769] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2997/6501 [1:26:15<1:40:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0382, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2997/6501 [1:26:15<1:40:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:46,562] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2998/6501 [1:26:17<1:42:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3328, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2998/6501 [1:26:17<1:42:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2999/6501 [1:26:19<1:43:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9221, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 2999/6501 [1:26:19<1:43:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:50,233] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3000/6501 [1:26:21<1:44:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1562, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3000/6501 [1:26:21<1:44:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:51,947] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3001/6501 [1:26:23<1:43:13,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9005, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3001/6501 [1:26:23<1:43:13,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:53,361] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3002/6501 [1:26:24<1:36:58,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3362, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3002/6501 [1:26:24<1:36:58,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:54,989] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3003/6501 [1:26:26<1:36:20,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3997, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3003/6501 [1:26:26<1:36:20,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:45:56,999] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3004/6501 [1:26:28<1:42:32,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.498, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3004/6501 [1:26:28<1:42:32,  1.76s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3005/6501 [1:26:29<1:39:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4669, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3005/6501 [1:26:29<1:39:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:00,325] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3006/6501 [1:26:31<1:40:08,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0921, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 3006/6501 [1:26:31<1:40:08,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:02,033] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3007/6501 [1:26:33<1:39:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2624, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3007/6501 [1:26:33<1:39:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:03,935] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3008/6501 [1:26:35<1:43:08,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1015, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3008/6501 [1:26:35<1:43:08,  1.77s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3009/6501 [1:26:36<1:44:02,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7077, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3009/6501 [1:26:36<1:44:02,  1.79s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3010/6501 [1:26:38<1:41:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0631, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3010/6501 [1:26:38<1:41:51,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:09,185] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3011/6501 [1:26:40<1:42:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2059, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3011/6501 [1:26:40<1:42:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3012/6501 [1:26:42<1:44:48,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1377, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3012/6501 [1:26:42<1:44:48,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:12,751] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3013/6501 [1:26:43<1:42:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3616, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3013/6501 [1:26:43<1:42:07,  1.76s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3014/6501 [1:26:45<1:42:11,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3365, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3014/6501 [1:26:45<1:42:11,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:16,149] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3015/6501 [1:26:47<1:40:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2122, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3015/6501 [1:26:47<1:40:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3016/6501 [1:26:49<1:38:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3095, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3016/6501 [1:26:49<1:38:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:19,560] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3017/6501 [1:26:50<1:39:42,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9665, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3017/6501 [1:26:50<1:39:42,  1.72s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3018/6501 [1:26:52<1:41:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2971, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3018/6501 [1:26:52<1:41:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:23,109] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3019/6501 [1:26:54<1:40:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.093, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3019/6501 [1:26:54<1:40:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:24,957] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3020/6501 [1:26:56<1:42:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9783, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3020/6501 [1:26:56<1:42:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3021/6501 [1:26:57<1:41:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3887, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3021/6501 [1:26:57<1:41:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:28,361] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3022/6501 [1:26:59<1:40:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3431, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 3022/6501 [1:26:59<1:40:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3023/6501 [1:27:00<1:26:24,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2754, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3023/6501 [1:27:00<1:26:24,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:31,292] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3024/6501 [1:27:02<1:35:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5562, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3024/6501 [1:27:02<1:35:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:33,338] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3025/6501 [1:27:04<1:42:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9742, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3025/6501 [1:27:04<1:42:22,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3026/6501 [1:27:06<1:47:49,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0795, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3026/6501 [1:27:06<1:47:49,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:37,031] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3027/6501 [1:27:08<1:43:26,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3226, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3027/6501 [1:27:08<1:43:26,  1.79s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3028/6501 [1:27:10<1:45:39,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8029, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3028/6501 [1:27:10<1:45:39,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:40,449] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3029/6501 [1:27:11<1:40:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3104, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3029/6501 [1:27:11<1:40:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:42,042] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3030/6501 [1:27:13<1:37:38,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2953, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3030/6501 [1:27:13<1:37:38,  1.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3031/6501 [1:27:15<1:39:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2842, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3031/6501 [1:27:15<1:39:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:45,709] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3032/6501 [1:27:16<1:42:04,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0318, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3032/6501 [1:27:16<1:42:04,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3033/6501 [1:27:18<1:44:25,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1582, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3033/6501 [1:27:18<1:44:25,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:49,303] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3034/6501 [1:27:20<1:42:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2401, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3034/6501 [1:27:20<1:42:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3035/6501 [1:27:22<1:40:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2581, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3035/6501 [1:27:22<1:40:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:52,958] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3036/6501 [1:27:24<1:44:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3785, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3036/6501 [1:27:24<1:44:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:54,555] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3037/6501 [1:27:25<1:40:57,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1326, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3037/6501 [1:27:25<1:40:57,  1.75s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3038/6501 [1:27:27<1:38:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1948, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3038/6501 [1:27:27<1:38:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:57,769] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3039/6501 [1:27:28<1:36:37,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2541, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3039/6501 [1:27:28<1:36:37,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:46:59,448] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3040/6501 [1:27:30<1:36:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.011, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3040/6501 [1:27:30<1:36:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3041/6501 [1:27:32<1:34:20,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1406, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3041/6501 [1:27:32<1:34:20,  1.64s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3042/6501 [1:27:33<1:26:43,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3362, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3042/6501 [1:27:33<1:26:43,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:03,968] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3043/6501 [1:27:35<1:31:28,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4078, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3043/6501 [1:27:35<1:31:28,  1.59s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3044/6501 [1:27:36<1:24:41,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1238, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3044/6501 [1:27:36<1:24:41,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:06,876] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3045/6501 [1:27:38<1:28:50,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9174, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3045/6501 [1:27:38<1:28:50,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:08,600] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3046/6501 [1:27:39<1:31:56,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2398, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3046/6501 [1:27:39<1:31:56,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:10,380] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3047/6501 [1:27:41<1:35:05,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0226, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3047/6501 [1:27:41<1:35:05,  1.65s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3048/6501 [1:27:43<1:34:39,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0897, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3048/6501 [1:27:43<1:34:39,  1.64s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3049/6501 [1:27:44<1:26:24,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4413, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3049/6501 [1:27:44<1:26:24,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:14,661] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3050/6501 [1:27:45<1:26:04,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0841, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3050/6501 [1:27:45<1:26:04,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:16,531] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3051/6501 [1:27:47<1:32:29,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2458, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3051/6501 [1:27:47<1:32:29,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:18,700] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3052/6501 [1:27:49<1:42:07,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2609, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3052/6501 [1:27:49<1:42:07,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3053/6501 [1:27:51<1:40:30,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0946, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3053/6501 [1:27:51<1:40:30,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:22,062] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3054/6501 [1:27:53<1:39:15,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4399, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3054/6501 [1:27:53<1:39:15,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:24,097] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3055/6501 [1:27:55<1:44:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1879, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3055/6501 [1:27:55<1:44:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3056/6501 [1:27:56<1:35:06,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0408, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3056/6501 [1:27:56<1:35:06,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:27,573] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3057/6501 [1:27:58<1:44:27,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6574, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3057/6501 [1:27:58<1:44:27,  1.82s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3058/6501 [1:28:00<1:42:47,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8517, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3058/6501 [1:28:00<1:42:47,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:31,086] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3059/6501 [1:28:02<1:42:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4626, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3059/6501 [1:28:02<1:42:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:32,966] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3060/6501 [1:28:04<1:44:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7434, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3060/6501 [1:28:04<1:44:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:34,592] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3061/6501 [1:28:05<1:40:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2711, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3061/6501 [1:28:05<1:40:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:36,341] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3062/6501 [1:28:07<1:40:40,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1227, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3062/6501 [1:28:07<1:40:40,  1.76s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3063/6501 [1:28:09<1:36:41,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1749, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3063/6501 [1:28:09<1:36:41,  1.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3064/6501 [1:28:10<1:35:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3488, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3064/6501 [1:28:10<1:35:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3065/6501 [1:28:12<1:32:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0846, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3065/6501 [1:28:12<1:32:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:42,657] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3066/6501 [1:28:13<1:33:31,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3784, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3066/6501 [1:28:13<1:33:31,  1.63s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3067/6501 [1:28:15<1:35:56,  1.68s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3067/6501 [1:28:15<1:35:56,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1779, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:46,429] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3068/6501 [1:28:17<1:41:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1965, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3068/6501 [1:28:17<1:41:23,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:47,921] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3069/6501 [1:28:19<1:36:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4678, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3069/6501 [1:28:19<1:36:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:49,741] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3070/6501 [1:28:20<1:38:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5661, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3070/6501 [1:28:20<1:38:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3071/6501 [1:28:22<1:34:56,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4257, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3071/6501 [1:28:22<1:34:56,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:52,642] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3072/6501 [1:28:23<1:30:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1401, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3072/6501 [1:28:23<1:30:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:54,316] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3073/6501 [1:28:25<1:31:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3987, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3073/6501 [1:28:25<1:31:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3074/6501 [1:28:27<1:32:43,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3571, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3074/6501 [1:28:27<1:32:43,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:57,598] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3075/6501 [1:28:28<1:32:44,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.988, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3075/6501 [1:28:28<1:32:44,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:47:59,427] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3076/6501 [1:28:30<1:36:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9779, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3076/6501 [1:28:30<1:36:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:01,234] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3077/6501 [1:28:32<1:38:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.117, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3077/6501 [1:28:32<1:38:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:02,937] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3078/6501 [1:28:34<1:37:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1644, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3078/6501 [1:28:34<1:37:54,  1.72s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3079/6501 [1:28:35<1:34:59,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.589, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3079/6501 [1:28:35<1:34:59,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:06,340] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3080/6501 [1:28:37<1:38:13,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9792, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3080/6501 [1:28:37<1:38:13,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:08,073] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3081/6501 [1:28:39<1:38:22,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1072, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3081/6501 [1:28:39<1:38:22,  1.73s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3082/6501 [1:28:41<1:41:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3306, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3082/6501 [1:28:41<1:41:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:11,718] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3083/6501 [1:28:42<1:40:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0502, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3083/6501 [1:28:42<1:40:34,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:13,601] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3084/6501 [1:28:44<1:42:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2856, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3084/6501 [1:28:44<1:42:32,  1.80s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3085/6501 [1:28:46<1:39:55,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4272, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3085/6501 [1:28:46<1:39:55,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:17,035] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3086/6501 [1:28:48<1:40:24,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4179, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3086/6501 [1:28:48<1:40:24,  1.76s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3087/6501 [1:28:50<1:40:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1871, 'learning_rate': 2e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 3087/6501 [1:28:50<1:40:45,  1.77s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3088/6501 [1:28:51<1:39:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1483, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3088/6501 [1:28:51<1:39:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3089/6501 [1:28:53<1:39:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2103, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3089/6501 [1:28:53<1:39:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:23,819] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3090/6501 [1:28:55<1:36:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3997, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3090/6501 [1:28:55<1:36:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:25,688] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3091/6501 [1:28:56<1:39:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6012, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3091/6501 [1:28:56<1:39:06,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:27,545] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3092/6501 [1:28:58<1:41:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1725, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3092/6501 [1:28:58<1:41:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:29,573] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3093/6501 [1:29:00<1:45:14,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9457, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3093/6501 [1:29:00<1:45:14,  1.85s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3094/6501 [1:29:02<1:43:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3242, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3094/6501 [1:29:02<1:43:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:33,180] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3095/6501 [1:29:04<1:44:05,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2024, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3095/6501 [1:29:04<1:44:05,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:35,649] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3096/6501 [1:29:06<1:54:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2151, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3096/6501 [1:29:06<1:54:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3097/6501 [1:29:08<1:48:46,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5033, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3097/6501 [1:29:08<1:48:46,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:39,182] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3098/6501 [1:29:10<1:47:52,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1013, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3098/6501 [1:29:10<1:47:52,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:41,018] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3099/6501 [1:29:12<1:46:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4154, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3099/6501 [1:29:12<1:46:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:42,688] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3100/6501 [1:29:13<1:43:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2276, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3100/6501 [1:29:13<1:43:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3101/6501 [1:29:15<1:39:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2324, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3101/6501 [1:29:15<1:39:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3102/6501 [1:29:17<1:36:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2684, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3102/6501 [1:29:17<1:36:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:47,709] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3103/6501 [1:29:18<1:38:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1978, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3103/6501 [1:29:18<1:38:53,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:49,270] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3104/6501 [1:29:20<1:35:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2803, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3104/6501 [1:29:20<1:35:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:50,983] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3105/6501 [1:29:22<1:36:04,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4715, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3105/6501 [1:29:22<1:36:04,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:52,663] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3106/6501 [1:29:23<1:35:44,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3072, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3106/6501 [1:29:23<1:35:44,  1.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3107/6501 [1:29:25<1:37:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1938, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3107/6501 [1:29:25<1:37:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:55,878] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3108/6501 [1:29:27<1:32:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3349, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3108/6501 [1:29:27<1:32:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:57,880] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3109/6501 [1:29:29<1:38:35,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9044, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3109/6501 [1:29:29<1:38:35,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:48:59,703] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3110/6501 [1:29:30<1:39:53,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9032, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3110/6501 [1:29:30<1:39:53,  1.77s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3111/6501 [1:29:32<1:35:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2072, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3111/6501 [1:29:32<1:35:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:03,098] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3112/6501 [1:29:34<1:38:47,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3168, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3112/6501 [1:29:34<1:38:47,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:04,870] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3113/6501 [1:29:36<1:39:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.277, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3113/6501 [1:29:36<1:39:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:06,771] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3114/6501 [1:29:37<1:41:34,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.648, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3114/6501 [1:29:37<1:41:34,  1.80s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3115/6501 [1:29:39<1:37:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.369, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3115/6501 [1:29:39<1:37:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:09,899] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3116/6501 [1:29:41<1:34:45,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3724, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3116/6501 [1:29:41<1:34:45,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:11,760] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3117/6501 [1:29:42<1:37:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2193, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3117/6501 [1:29:42<1:37:48,  1.73s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3118/6501 [1:29:44<1:36:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1957, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3118/6501 [1:29:44<1:36:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:14,893] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3119/6501 [1:29:46<1:32:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0421, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3119/6501 [1:29:46<1:32:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:16,726] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3120/6501 [1:29:47<1:35:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0906, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3120/6501 [1:29:47<1:35:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:18,423] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3121/6501 [1:29:49<1:35:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1672, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3121/6501 [1:29:49<1:35:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:19,953] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3122/6501 [1:29:51<1:32:49,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0195, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3122/6501 [1:29:51<1:32:49,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:21,928] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3123/6501 [1:29:53<1:38:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0308, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3123/6501 [1:29:53<1:38:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3124/6501 [1:29:55<1:45:24,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1087, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3124/6501 [1:29:55<1:45:24,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:25,922] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3125/6501 [1:29:57<1:44:34,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0421, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3125/6501 [1:29:57<1:44:34,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:27,906] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3126/6501 [1:29:59<1:46:39,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9712, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3126/6501 [1:29:59<1:46:39,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:29,736] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3127/6501 [1:30:00<1:45:30,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1045, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3127/6501 [1:30:00<1:45:30,  1.88s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3128/6501 [1:30:02<1:42:06,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2812, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3128/6501 [1:30:02<1:42:06,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:32,970] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3129/6501 [1:30:04<1:37:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4651, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3129/6501 [1:30:04<1:37:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:34,396] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3130/6501 [1:30:05<1:32:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2683, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3130/6501 [1:30:05<1:32:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:36,308] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3131/6501 [1:30:07<1:36:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1168, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3131/6501 [1:30:07<1:36:53,  1.73s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3132/6501 [1:30:08<1:31:40,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5124, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3132/6501 [1:30:08<1:31:40,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:39,528] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3133/6501 [1:30:10<1:34:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1863, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3133/6501 [1:30:10<1:34:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:41,210] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3134/6501 [1:30:12<1:34:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.149, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3134/6501 [1:30:12<1:34:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:43,063] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3135/6501 [1:30:14<1:37:17,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.657, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3135/6501 [1:30:14<1:37:17,  1.73s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3136/6501 [1:30:15<1:36:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0317, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3136/6501 [1:30:15<1:36:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:46,501] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3137/6501 [1:30:17<1:36:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9386, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3137/6501 [1:30:17<1:36:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:48,284] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3138/6501 [1:30:19<1:37:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2732, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3138/6501 [1:30:19<1:37:42,  1.74s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3139/6501 [1:30:20<1:26:16,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2852, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3139/6501 [1:30:20<1:26:16,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:51,322] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3140/6501 [1:30:22<1:33:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.221, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3140/6501 [1:30:22<1:33:32,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:53,126] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3141/6501 [1:30:24<1:35:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4512, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3141/6501 [1:30:24<1:35:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3142/6501 [1:30:26<1:39:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.138, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3142/6501 [1:30:26<1:39:22,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:49:57,089] [WARNING] [stage3.py:1787:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3143/6501 [1:30:28<1:43:43,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1165, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3143/6501 [1:30:28<1:43:43,  1.85s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3144/6501 [1:30:30<1:41:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1513, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3144/6501 [1:30:30<1:41:31,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:00,576] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3145/6501 [1:30:31<1:40:38,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0363, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3145/6501 [1:30:31<1:40:38,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:02,098] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3146/6501 [1:30:33<1:35:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3146/6501 [1:30:33<1:35:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3474, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3147/6501 [1:30:35<1:35:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3633, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3147/6501 [1:30:35<1:35:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3148/6501 [1:30:36<1:34:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7919, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3148/6501 [1:30:36<1:34:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:07,144] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3149/6501 [1:30:38<1:34:38,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3153, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3149/6501 [1:30:38<1:34:38,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:08,960] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3150/6501 [1:30:40<1:36:39,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3956, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3150/6501 [1:30:40<1:36:39,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:10,716] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3151/6501 [1:30:41<1:37:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1008, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3151/6501 [1:30:41<1:37:02,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:12,422] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3152/6501 [1:30:43<1:36:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0233, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 3152/6501 [1:30:43<1:36:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:14,280] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3153/6501 [1:30:45<1:38:37,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0298, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3153/6501 [1:30:45<1:38:37,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:15,997] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3154/6501 [1:30:47<1:37:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4017, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3154/6501 [1:30:47<1:37:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3155/6501 [1:30:48<1:37:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0217, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3155/6501 [1:30:48<1:37:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:19,688] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3156/6501 [1:30:50<1:40:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1505, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3156/6501 [1:30:50<1:40:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:21,215] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3157/6501 [1:30:52<1:36:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3661, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3157/6501 [1:30:52<1:36:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3158/6501 [1:30:53<1:32:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5765, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3158/6501 [1:30:53<1:32:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:24,816] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3159/6501 [1:30:56<1:39:52,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8935, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3159/6501 [1:30:56<1:39:52,  1.79s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3160/6501 [1:30:57<1:37:31,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0993, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3160/6501 [1:30:57<1:37:31,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:28,097] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3161/6501 [1:30:59<1:35:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2027, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3161/6501 [1:30:59<1:35:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:30,152] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3162/6501 [1:31:01<1:41:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.426, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3162/6501 [1:31:01<1:41:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:32,075] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3163/6501 [1:31:03<1:42:49,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3271, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3163/6501 [1:31:03<1:42:49,  1.85s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3164/6501 [1:31:04<1:29:18,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8215, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3164/6501 [1:31:04<1:29:18,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:35,330] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3165/6501 [1:31:06<1:39:26,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0099, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3165/6501 [1:31:06<1:39:26,  1.79s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3166/6501 [1:31:08<1:36:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1953, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3166/6501 [1:31:08<1:36:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:38,862] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3167/6501 [1:31:10<1:39:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1194, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3167/6501 [1:31:10<1:39:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:40,617] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3168/6501 [1:31:11<1:38:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3164, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3168/6501 [1:31:11<1:38:56,  1.78s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3169/6501 [1:31:13<1:38:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.305, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 3169/6501 [1:31:13<1:38:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:44,249] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3170/6501 [1:31:15<1:40:14,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4406, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3170/6501 [1:31:15<1:40:14,  1.81s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3171/6501 [1:31:17<1:36:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3364, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3171/6501 [1:31:17<1:36:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:47,686] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3172/6501 [1:31:18<1:38:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2512, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3172/6501 [1:31:18<1:38:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3173/6501 [1:31:20<1:35:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1261, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3173/6501 [1:31:20<1:35:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:51,035] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3174/6501 [1:31:22<1:35:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3114, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3174/6501 [1:31:22<1:35:42,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:52,627] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3175/6501 [1:31:23<1:33:26,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.132, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3175/6501 [1:31:23<1:33:26,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:54,245] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3176/6501 [1:31:25<1:32:17,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2404, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3176/6501 [1:31:25<1:32:17,  1.67s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3177/6501 [1:31:26<1:26:56,  1.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0831, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3177/6501 [1:31:26<1:26:56,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:50:57,602] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3178/6501 [1:31:28<1:34:16,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5624, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3178/6501 [1:31:28<1:34:16,  1.70s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3179/6501 [1:31:30<1:34:44,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7089, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3179/6501 [1:31:30<1:34:44,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:01,017] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3180/6501 [1:31:32<1:34:13,  1.70s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3180/6501 [1:31:32<1:34:13,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1252, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:02,958] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3181/6501 [1:31:34<1:38:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3326, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3181/6501 [1:31:34<1:38:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:04,492] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3182/6501 [1:31:35<1:34:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3468, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3182/6501 [1:31:35<1:34:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3183/6501 [1:31:37<1:35:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3735, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3183/6501 [1:31:37<1:35:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:07,974] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3184/6501 [1:31:39<1:35:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0696, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3184/6501 [1:31:39<1:35:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:09,596] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3185/6501 [1:31:40<1:33:24,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1505, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3185/6501 [1:31:40<1:33:24,  1.69s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3186/6501 [1:31:42<1:33:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3079, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3186/6501 [1:31:42<1:33:33,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:12,976] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3187/6501 [1:31:44<1:33:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4014, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3187/6501 [1:31:44<1:33:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:14,525] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3188/6501 [1:31:45<1:30:56,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2415, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3188/6501 [1:31:45<1:30:56,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:16,153] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3189/6501 [1:31:47<1:30:36,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5209, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3189/6501 [1:31:47<1:30:36,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:18,070] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3190/6501 [1:31:49<1:35:08,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1176, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3190/6501 [1:31:49<1:35:08,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:19,837] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3191/6501 [1:31:51<1:35:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2788, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3191/6501 [1:31:51<1:35:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:21,755] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3192/6501 [1:31:52<1:38:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2232, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3192/6501 [1:31:52<1:38:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3193/6501 [1:31:54<1:34:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3667, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3193/6501 [1:31:54<1:34:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3194/6501 [1:31:55<1:21:23,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4165, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3194/6501 [1:31:55<1:21:23,  1.48s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:26,049] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3195/6501 [1:31:57<1:27:12,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2232, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3195/6501 [1:31:57<1:27:12,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:27,803] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3196/6501 [1:31:58<1:30:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0688, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3196/6501 [1:31:59<1:30:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3197/6501 [1:32:00<1:30:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4338, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3197/6501 [1:32:00<1:30:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:31,185] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3198/6501 [1:32:02<1:31:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1137, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3198/6501 [1:32:02<1:31:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:33,410] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3199/6501 [1:32:04<1:40:47,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9648, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3199/6501 [1:32:04<1:40:47,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:35,374] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3200/6501 [1:32:06<1:42:56,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2739, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3200/6501 [1:32:06<1:42:56,  1.87s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3201/6501 [1:32:08<1:40:55,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1815, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3201/6501 [1:32:08<1:40:55,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:38,961] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3202/6501 [1:32:10<1:40:55,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4524, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3202/6501 [1:32:10<1:40:55,  1.84s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3203/6501 [1:32:11<1:33:34,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0601, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3203/6501 [1:32:11<1:33:34,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:42,146] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3204/6501 [1:32:13<1:35:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8981, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3204/6501 [1:32:13<1:35:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:43,855] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3205/6501 [1:32:15<1:34:40,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.186, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3205/6501 [1:32:15<1:34:40,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:45,638] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3206/6501 [1:32:16<1:35:37,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6219, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3206/6501 [1:32:16<1:35:37,  1.74s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3207/6501 [1:32:17<1:23:20,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1869, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3207/6501 [1:32:17<1:23:20,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:48,498] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3208/6501 [1:32:19<1:28:59,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9293, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3208/6501 [1:32:19<1:28:59,  1.62s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3209/6501 [1:32:21<1:26:56,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3182, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3209/6501 [1:32:21<1:26:56,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:51,843] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3210/6501 [1:32:23<1:31:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.134, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3210/6501 [1:32:23<1:31:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:53,584] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3211/6501 [1:32:24<1:32:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7256, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3211/6501 [1:32:24<1:32:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3212/6501 [1:32:26<1:32:57,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9449, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3212/6501 [1:32:26<1:32:57,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:57,022] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3213/6501 [1:32:28<1:33:20,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1684, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3213/6501 [1:32:28<1:33:20,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:51:58,785] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3214/6501 [1:32:29<1:34:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.498, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3214/6501 [1:32:29<1:34:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:00,420] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3215/6501 [1:32:31<1:32:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4998, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3215/6501 [1:32:31<1:32:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:02,214] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3216/6501 [1:32:33<1:34:26,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4394, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3216/6501 [1:32:33<1:34:26,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:04,140] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3217/6501 [1:32:35<1:37:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3486, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 3217/6501 [1:32:35<1:37:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:05,371] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3218/6501 [1:32:36<1:28:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3732, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3218/6501 [1:32:36<1:28:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3219/6501 [1:32:38<1:28:45,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3036, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3219/6501 [1:32:38<1:28:45,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:08,869] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3220/6501 [1:32:40<1:32:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1491, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3220/6501 [1:32:40<1:32:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:10,595] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3221/6501 [1:32:41<1:33:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.46, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3221/6501 [1:32:41<1:33:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:12,181] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3222/6501 [1:32:43<1:31:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5682, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3222/6501 [1:32:43<1:31:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:14,120] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3223/6501 [1:32:45<1:35:36,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9426, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3223/6501 [1:32:45<1:35:36,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:15,968] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3224/6501 [1:32:47<1:37:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2955, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3224/6501 [1:32:47<1:37:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:17,614] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3225/6501 [1:32:48<1:34:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2444, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3225/6501 [1:32:48<1:34:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3226/6501 [1:32:50<1:34:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7979, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3226/6501 [1:32:50<1:34:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:20,921] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3227/6501 [1:32:52<1:32:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1845, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3227/6501 [1:32:52<1:32:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:22,686] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3228/6501 [1:32:53<1:33:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2333, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3228/6501 [1:32:53<1:33:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:24,237] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3229/6501 [1:32:55<1:30:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.651, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3229/6501 [1:32:55<1:30:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:26,056] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3230/6501 [1:32:57<1:33:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4056, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3230/6501 [1:32:57<1:33:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:27,829] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3231/6501 [1:32:59<1:34:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3649, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3231/6501 [1:32:59<1:34:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3232/6501 [1:33:00<1:33:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0963, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3232/6501 [1:33:00<1:33:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:31,207] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3233/6501 [1:33:02<1:33:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0453, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3233/6501 [1:33:02<1:33:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:33,006] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3234/6501 [1:33:04<1:34:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3749, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3234/6501 [1:33:04<1:34:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3235/6501 [1:33:06<1:35:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2922, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3235/6501 [1:33:06<1:35:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3236/6501 [1:33:07<1:34:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1378, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3236/6501 [1:33:07<1:34:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:38,296] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3237/6501 [1:33:09<1:35:31,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.514, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3237/6501 [1:33:09<1:35:31,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:39,732] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3238/6501 [1:33:10<1:30:16,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4418, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3238/6501 [1:33:10<1:30:16,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:41,551] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3239/6501 [1:33:12<1:32:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.376, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3239/6501 [1:33:12<1:32:50,  1.71s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3240/6501 [1:33:14<1:27:05,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3426, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3240/6501 [1:33:14<1:27:05,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:44,950] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3241/6501 [1:33:16<1:34:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1859, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3241/6501 [1:33:16<1:34:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:46,805] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3242/6501 [1:33:18<1:36:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1508, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3242/6501 [1:33:18<1:36:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:48,725] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3243/6501 [1:33:19<1:38:33,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.33, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3243/6501 [1:33:19<1:38:33,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:50,705] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3244/6501 [1:33:21<1:41:13,  1.86s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3244/6501 [1:33:21<1:41:13,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9117, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:51,859] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3245/6501 [1:33:23<1:29:37,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9509, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3245/6501 [1:33:23<1:29:37,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:53,807] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3246/6501 [1:33:25<1:34:25,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5422, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3246/6501 [1:33:25<1:34:25,  1.74s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3247/6501 [1:33:26<1:30:38,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.017, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3247/6501 [1:33:26<1:30:38,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:52:57,059] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3248/6501 [1:33:28<1:31:45,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5809, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3248/6501 [1:33:28<1:31:45,  1.69s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3249/6501 [1:33:30<1:37:19,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2562, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3249/6501 [1:33:30<1:37:19,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:00,965] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3250/6501 [1:33:32<1:38:30,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1358, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 3250/6501 [1:33:32<1:38:30,  1.82s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3251/6501 [1:33:34<1:49:33,  2.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8407, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3251/6501 [1:33:34<1:49:33,  2.02s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:05,383] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3252/6501 [1:33:36<1:47:49,  1.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4979, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3252/6501 [1:33:36<1:47:49,  1.99s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3253/6501 [1:33:37<1:33:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5571, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3253/6501 [1:33:37<1:33:57,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:08,530] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3254/6501 [1:33:39<1:38:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1904, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3254/6501 [1:33:39<1:38:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3255/6501 [1:33:41<1:41:14,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5271, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3255/6501 [1:33:41<1:41:14,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:12,199] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3256/6501 [1:33:43<1:37:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3317, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3256/6501 [1:33:43<1:37:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:14,097] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3257/6501 [1:33:45<1:39:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6639, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3257/6501 [1:33:45<1:39:20,  1.84s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3258/6501 [1:33:47<1:43:40,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4929, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3258/6501 [1:33:47<1:43:40,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:18,066] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3259/6501 [1:33:49<1:42:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1781, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3259/6501 [1:33:49<1:42:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:20,027] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3260/6501 [1:33:51<1:43:40,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6182, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3260/6501 [1:33:51<1:43:40,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:21,939] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3261/6501 [1:33:53<1:43:30,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4864, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3261/6501 [1:33:53<1:43:30,  1.92s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3262/6501 [1:33:54<1:39:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0807, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3262/6501 [1:33:54<1:39:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:25,594] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3263/6501 [1:33:56<1:41:48,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3322, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3263/6501 [1:33:56<1:41:48,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:26,656] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3264/6501 [1:33:57<1:28:25,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9946, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3264/6501 [1:33:57<1:28:25,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:28,491] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3265/6501 [1:33:59<1:31:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3961, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3265/6501 [1:33:59<1:31:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:30,466] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3266/6501 [1:34:01<1:36:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1389, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3266/6501 [1:34:01<1:36:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3267/6501 [1:34:03<1:36:35,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3207, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3267/6501 [1:34:03<1:36:35,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:33,962] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3268/6501 [1:34:05<1:34:43,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4523, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3268/6501 [1:34:05<1:34:43,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:35,879] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3269/6501 [1:34:07<1:37:16,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6851, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3269/6501 [1:34:07<1:37:16,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:37,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3270/6501 [1:34:08<1:33:26,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5471, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3270/6501 [1:34:08<1:33:26,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:39,029] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3271/6501 [1:34:10<1:30:53,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4591, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3271/6501 [1:34:10<1:30:53,  1.69s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3272/6501 [1:34:11<1:31:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9281, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3272/6501 [1:34:11<1:31:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:42,724] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3273/6501 [1:34:13<1:35:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8932, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3273/6501 [1:34:13<1:35:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3274/6501 [1:34:15<1:38:17,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4202, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3274/6501 [1:34:15<1:38:17,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:46,543] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3275/6501 [1:34:17<1:39:04,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8867, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3275/6501 [1:34:17<1:39:04,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:48,265] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3276/6501 [1:34:19<1:37:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4563, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3276/6501 [1:34:19<1:37:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:50,250] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3277/6501 [1:34:21<1:39:56,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3384, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3277/6501 [1:34:21<1:39:56,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:52,012] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3278/6501 [1:34:23<1:38:20,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9575, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3278/6501 [1:34:23<1:38:20,  1.83s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3279/6501 [1:34:24<1:34:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9773, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3279/6501 [1:34:24<1:34:15,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:55,439] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3280/6501 [1:34:26<1:35:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2313, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3280/6501 [1:34:26<1:35:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:53:57,140] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 3281/6501 [1:34:28<1:34:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0284, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3281/6501 [1:34:28<1:34:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3282/6501 [1:34:30<1:36:28,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1689, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3282/6501 [1:34:30<1:36:28,  1.80s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 3283/6501 [1:34:31<1:35:18,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4779, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 3283/6501 [1:34:31<1:35:18,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:02,470] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3284/6501 [1:34:33<1:34:13,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3013, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3284/6501 [1:34:33<1:34:13,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:04,626] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3285/6501 [1:34:35<1:40:36,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2594, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3285/6501 [1:34:35<1:40:36,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:06,685] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3286/6501 [1:34:37<1:43:30,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0083, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3286/6501 [1:34:37<1:43:30,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:08,450] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3287/6501 [1:34:39<1:40:47,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4357, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3287/6501 [1:34:39<1:40:47,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:10,038] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3288/6501 [1:34:41<1:36:02,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2582, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3288/6501 [1:34:41<1:36:02,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:12,008] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3289/6501 [1:34:43<1:38:50,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1523, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3289/6501 [1:34:43<1:38:50,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:13,869] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3290/6501 [1:34:45<1:39:03,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0826, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3290/6501 [1:34:45<1:39:03,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:15,746] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3291/6501 [1:34:46<1:39:26,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6973, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3291/6501 [1:34:46<1:39:26,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:17,385] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3292/6501 [1:34:48<1:35:52,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2628, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3292/6501 [1:34:48<1:35:52,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:19,233] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3293/6501 [1:34:50<1:36:44,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2302, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3293/6501 [1:34:50<1:36:44,  1.81s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3294/6501 [1:34:52<1:36:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9469, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3294/6501 [1:34:52<1:36:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:22,919] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3295/6501 [1:34:54<1:37:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3738, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3295/6501 [1:34:54<1:37:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:24,661] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3296/6501 [1:34:55<1:36:19,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1541, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3296/6501 [1:34:55<1:36:19,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:26,669] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3297/6501 [1:34:57<1:39:34,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3176, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3297/6501 [1:34:57<1:39:34,  1.86s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3298/6501 [1:34:59<1:38:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3451, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3298/6501 [1:34:59<1:38:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3299/6501 [1:35:01<1:40:09,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4185, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3299/6501 [1:35:01<1:40:09,  1.88s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3300/6501 [1:35:03<1:37:43,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1642, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3300/6501 [1:35:03<1:37:43,  1.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3301/6501 [1:35:05<1:38:42,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4005, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3301/6501 [1:35:05<1:38:42,  1.85s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3302/6501 [1:35:07<1:41:39,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1024, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3302/6501 [1:35:07<1:41:39,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:37,955] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3303/6501 [1:35:09<1:41:08,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5482, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3303/6501 [1:35:09<1:41:08,  1.90s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3304/6501 [1:35:10<1:35:49,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1105, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3304/6501 [1:35:10<1:35:49,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:41,313] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3305/6501 [1:35:12<1:35:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8283, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3305/6501 [1:35:12<1:35:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:42,902] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3306/6501 [1:35:14<1:32:20,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.907, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3306/6501 [1:35:14<1:32:20,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:44,733] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3307/6501 [1:35:15<1:33:51,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3074, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3307/6501 [1:35:15<1:33:51,  1.76s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3308/6501 [1:35:17<1:30:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2044, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3308/6501 [1:35:17<1:30:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:48,186] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3309/6501 [1:35:19<1:33:32,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1398, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3309/6501 [1:35:19<1:33:32,  1.76s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3310/6501 [1:35:21<1:32:31,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3458, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3310/6501 [1:35:21<1:32:31,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3311/6501 [1:35:22<1:30:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3492, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3311/6501 [1:35:22<1:30:45,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:53,494] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3312/6501 [1:35:24<1:35:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3192, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3312/6501 [1:35:24<1:35:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3313/6501 [1:35:26<1:31:23,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.113, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3313/6501 [1:35:26<1:31:23,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:56,716] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3314/6501 [1:35:27<1:30:27,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9303, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3314/6501 [1:35:27<1:30:27,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:54:58,824] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3315/6501 [1:35:30<1:36:52,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3778, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3315/6501 [1:35:30<1:36:52,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:00,636] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3316/6501 [1:35:31<1:36:39,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3279, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3316/6501 [1:35:31<1:36:39,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:02,437] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3317/6501 [1:35:33<1:36:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.22, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3317/6501 [1:35:33<1:36:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:04,394] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3318/6501 [1:35:35<1:38:32,  1.86s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3318/6501 [1:35:35<1:38:32,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0792, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:06,005] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3319/6501 [1:35:37<1:34:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7061, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3319/6501 [1:35:37<1:34:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:07,968] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3320/6501 [1:35:39<1:37:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1699, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3320/6501 [1:35:39<1:37:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:09,720] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3321/6501 [1:35:40<1:36:01,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2201, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3321/6501 [1:35:40<1:36:01,  1.81s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3322/6501 [1:35:42<1:38:28,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9407, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3322/6501 [1:35:42<1:38:28,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:13,354] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3323/6501 [1:35:44<1:35:22,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2039, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3323/6501 [1:35:44<1:35:22,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:15,081] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3324/6501 [1:35:46<1:34:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1903, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3324/6501 [1:35:46<1:34:10,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:17,249] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3325/6501 [1:35:48<1:40:20,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6735, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3325/6501 [1:35:48<1:40:20,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:19,048] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3326/6501 [1:35:50<1:38:46,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0718, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3326/6501 [1:35:50<1:38:46,  1.87s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3327/6501 [1:35:52<1:38:07,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0761, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3327/6501 [1:35:52<1:38:07,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:22,918] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3328/6501 [1:35:54<1:41:03,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1588, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3328/6501 [1:35:54<1:41:03,  1.91s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3329/6501 [1:35:55<1:40:24,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2867, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3329/6501 [1:35:55<1:40:24,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:26,889] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████     | 3330/6501 [1:35:58<1:43:33,  1.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3330/6501 [1:35:58<1:43:33,  1.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 3331/6501 [1:35:59<1:38:42,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4319, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 3331/6501 [1:35:59<1:38:42,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:29,883] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3332/6501 [1:36:01<1:30:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3436, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3332/6501 [1:36:01<1:30:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:31,595] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3333/6501 [1:36:02<1:30:17,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3012, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3333/6501 [1:36:02<1:30:17,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:33,437] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3334/6501 [1:36:04<1:32:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2947, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3334/6501 [1:36:04<1:32:21,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:35,337] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3335/6501 [1:36:06<1:34:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9294, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3335/6501 [1:36:06<1:34:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:37,105] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3336/6501 [1:36:08<1:34:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8929, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3336/6501 [1:36:08<1:34:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:38,698] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3337/6501 [1:36:09<1:31:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2499, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3337/6501 [1:36:09<1:31:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:40,565] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3338/6501 [1:36:11<1:33:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0111, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3338/6501 [1:36:11<1:33:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:42,425] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3339/6501 [1:36:13<1:34:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.531, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3339/6501 [1:36:13<1:34:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3340/6501 [1:36:15<1:35:59,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0561, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3340/6501 [1:36:15<1:35:59,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:45,763] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3341/6501 [1:36:16<1:30:12,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1876, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3341/6501 [1:36:16<1:30:12,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:47,666] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3342/6501 [1:36:18<1:33:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6715, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3342/6501 [1:36:18<1:33:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:49,708] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3343/6501 [1:36:20<1:37:26,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2154, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3343/6501 [1:36:20<1:37:26,  1.85s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3344/6501 [1:36:22<1:36:50,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4226, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3344/6501 [1:36:22<1:36:50,  1.84s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3345/6501 [1:36:24<1:32:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9915, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3345/6501 [1:36:24<1:32:52,  1.77s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3346/6501 [1:36:25<1:19:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9696, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3346/6501 [1:36:25<1:19:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:55,903] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3347/6501 [1:36:27<1:25:06,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2372, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3347/6501 [1:36:27<1:25:06,  1.62s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3348/6501 [1:36:28<1:26:30,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.108, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 3348/6501 [1:36:28<1:26:30,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:55:59,366] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3349/6501 [1:36:30<1:28:10,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0944, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3349/6501 [1:36:30<1:28:10,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:01,124] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3350/6501 [1:36:32<1:29:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9582, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3350/6501 [1:36:32<1:29:23,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:02,943] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3351/6501 [1:36:34<1:31:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4462, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3351/6501 [1:36:34<1:31:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3352/6501 [1:36:35<1:30:19,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1328, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3352/6501 [1:36:35<1:30:19,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:06,341] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3353/6501 [1:36:37<1:30:11,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2601, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3353/6501 [1:36:37<1:30:11,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:08,113] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3354/6501 [1:36:39<1:31:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6503, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3354/6501 [1:36:39<1:31:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:09,827] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3355/6501 [1:36:41<1:30:37,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2613, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3355/6501 [1:36:41<1:30:37,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:11,417] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3356/6501 [1:36:42<1:28:25,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2553, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3356/6501 [1:36:42<1:28:25,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:13,270] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3357/6501 [1:36:44<1:31:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3496, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3357/6501 [1:36:44<1:31:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3358/6501 [1:36:46<1:28:34,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4064, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3358/6501 [1:36:46<1:28:34,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:16,869] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3359/6501 [1:36:48<1:33:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.049, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3359/6501 [1:36:48<1:33:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3360/6501 [1:36:49<1:26:26,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9143, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3360/6501 [1:36:49<1:26:26,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:20,148] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3361/6501 [1:36:51<1:31:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3382, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3361/6501 [1:36:51<1:31:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3362/6501 [1:36:52<1:29:36,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2758, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3362/6501 [1:36:52<1:29:36,  1.71s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3363/6501 [1:36:54<1:29:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9124, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3363/6501 [1:36:54<1:29:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:25,043] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3364/6501 [1:36:56<1:26:39,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4515, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3364/6501 [1:36:56<1:26:39,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:26,781] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3365/6501 [1:36:57<1:27:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2148, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3365/6501 [1:36:57<1:27:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:27,769] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3366/6501 [1:36:58<1:16:59,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4011, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3366/6501 [1:36:58<1:16:59,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:29,686] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3367/6501 [1:37:00<1:23:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1866, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3367/6501 [1:37:00<1:23:55,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:31,369] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3368/6501 [1:37:02<1:25:04,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8699, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3368/6501 [1:37:02<1:25:04,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:33,364] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3369/6501 [1:37:04<1:30:47,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2394, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3369/6501 [1:37:04<1:30:47,  1.74s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3370/6501 [1:37:06<1:30:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5862, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3370/6501 [1:37:06<1:30:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:36,926] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3371/6501 [1:37:08<1:31:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2938, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3371/6501 [1:37:08<1:31:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:38,803] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3372/6501 [1:37:09<1:33:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1551, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3372/6501 [1:37:10<1:33:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:40,477] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3373/6501 [1:37:11<1:31:46,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7022, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3373/6501 [1:37:11<1:31:46,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:42,310] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3374/6501 [1:37:13<1:32:52,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5496, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3374/6501 [1:37:13<1:32:52,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:44,011] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3375/6501 [1:37:15<1:31:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0838, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3375/6501 [1:37:15<1:31:34,  1.76s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3376/6501 [1:37:16<1:28:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3118, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3376/6501 [1:37:16<1:28:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:47,173] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3377/6501 [1:37:18<1:26:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5236, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3377/6501 [1:37:18<1:26:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3378/6501 [1:37:19<1:22:01,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2266, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3378/6501 [1:37:19<1:22:01,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:49,643] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3379/6501 [1:37:20<1:14:44,  1.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3485, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3379/6501 [1:37:20<1:14:44,  1.44s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:51,583] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3380/6501 [1:37:22<1:22:34,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0867, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3380/6501 [1:37:22<1:22:34,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:53,358] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3381/6501 [1:37:24<1:25:28,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4482, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3381/6501 [1:37:24<1:25:28,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:55,098] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3382/6501 [1:37:26<1:26:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9361, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3382/6501 [1:37:26<1:26:56,  1.67s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3383/6501 [1:37:27<1:25:42,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0889, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3383/6501 [1:37:27<1:25:42,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:56:58,572] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3384/6501 [1:37:29<1:29:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0075, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3384/6501 [1:37:29<1:29:15,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:00,288] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3385/6501 [1:37:31<1:29:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.087, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3385/6501 [1:37:31<1:29:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:02,416] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3386/6501 [1:37:33<1:35:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8945, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3386/6501 [1:37:33<1:35:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3387/6501 [1:37:35<1:34:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0394, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3387/6501 [1:37:35<1:34:13,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:06,489] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3388/6501 [1:37:37<1:41:58,  1.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1489, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3388/6501 [1:37:37<1:41:58,  1.97s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:08,129] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3389/6501 [1:37:39<1:36:53,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2143, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3389/6501 [1:37:39<1:36:53,  1.87s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3390/6501 [1:37:41<1:36:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.115, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3390/6501 [1:37:41<1:36:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:11,522] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3391/6501 [1:37:42<1:31:26,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4395, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3391/6501 [1:37:42<1:31:26,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:13,364] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3392/6501 [1:37:44<1:32:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.726, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3392/6501 [1:37:44<1:32:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3393/6501 [1:37:46<1:29:56,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6603, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3393/6501 [1:37:46<1:29:56,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:16,505] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3394/6501 [1:37:47<1:26:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0691, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3394/6501 [1:37:47<1:26:36,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:18,222] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3395/6501 [1:37:49<1:27:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6001, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3395/6501 [1:37:49<1:27:17,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:19,732] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3396/6501 [1:37:50<1:24:31,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4202, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3396/6501 [1:37:50<1:24:31,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:21,461] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3397/6501 [1:37:52<1:25:58,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4104, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3397/6501 [1:37:52<1:25:58,  1.66s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3398/6501 [1:37:54<1:30:33,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2128, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3398/6501 [1:37:54<1:30:33,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:25,351] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3399/6501 [1:37:56<1:33:19,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1658, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3399/6501 [1:37:56<1:33:19,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:26,957] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3400/6501 [1:37:58<1:30:11,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4187, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3400/6501 [1:37:58<1:30:11,  1.75s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3401/6501 [1:37:59<1:31:19,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.264, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3401/6501 [1:37:59<1:31:19,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:30,605] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3402/6501 [1:38:01<1:32:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4436, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3402/6501 [1:38:01<1:32:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:32,793] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3403/6501 [1:38:03<1:38:26,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.096, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3403/6501 [1:38:03<1:38:26,  1.91s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3404/6501 [1:38:05<1:36:28,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2025, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3404/6501 [1:38:05<1:36:28,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:36,723] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3405/6501 [1:38:07<1:40:45,  1.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1373, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3405/6501 [1:38:07<1:40:45,  1.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3406/6501 [1:38:09<1:28:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0711, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3406/6501 [1:38:09<1:28:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:39,521] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3407/6501 [1:38:10<1:27:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4078, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3407/6501 [1:38:10<1:27:20,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:41,713] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3408/6501 [1:38:12<1:35:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2494, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3408/6501 [1:38:12<1:35:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3409/6501 [1:38:14<1:32:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0021, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3409/6501 [1:38:14<1:32:57,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:45,268] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3410/6501 [1:38:16<1:33:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9955, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3410/6501 [1:38:16<1:33:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:46,950] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3411/6501 [1:38:18<1:31:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5229, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3411/6501 [1:38:18<1:31:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:48,478] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3412/6501 [1:38:19<1:27:34,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4243, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3412/6501 [1:38:19<1:27:34,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:50,205] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3413/6501 [1:38:21<1:27:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5897, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 3413/6501 [1:38:21<1:27:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:51,829] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3414/6501 [1:38:23<1:26:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2829, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3414/6501 [1:38:23<1:26:36,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:53,701] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3415/6501 [1:38:24<1:29:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.47, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3415/6501 [1:38:24<1:29:29,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:55,657] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3416/6501 [1:38:26<1:32:47,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8148, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3416/6501 [1:38:26<1:32:47,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:57,403] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3417/6501 [1:38:28<1:31:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2313, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3417/6501 [1:38:28<1:31:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:57:58,901] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3418/6501 [1:38:30<1:27:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.049, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3418/6501 [1:38:30<1:27:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:00,780] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3419/6501 [1:38:31<1:30:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2941, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3419/6501 [1:38:31<1:30:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:02,534] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3420/6501 [1:38:33<1:30:03,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9537, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3420/6501 [1:38:33<1:30:03,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:04,872] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3421/6501 [1:38:36<1:39:01,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0849, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3421/6501 [1:38:36<1:39:01,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:06,682] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3422/6501 [1:38:37<1:37:09,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6981, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3422/6501 [1:38:37<1:37:09,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:08,719] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3423/6501 [1:38:39<1:39:20,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5517, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3423/6501 [1:38:39<1:39:20,  1.94s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:10,352] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3424/6501 [1:38:41<1:34:38,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8594, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3424/6501 [1:38:41<1:34:38,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:12,205] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3425/6501 [1:38:43<1:34:43,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0731, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3425/6501 [1:38:43<1:34:43,  1.85s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3426/6501 [1:38:45<1:33:11,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5806, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3426/6501 [1:38:45<1:33:11,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:15,556] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3427/6501 [1:38:46<1:29:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2838, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3427/6501 [1:38:46<1:29:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:17,218] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3428/6501 [1:38:48<1:28:23,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3498, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3428/6501 [1:38:48<1:28:23,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:18,923] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3429/6501 [1:38:50<1:28:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0504, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3429/6501 [1:38:50<1:28:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:20,624] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3430/6501 [1:38:51<1:27:43,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9001, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3430/6501 [1:38:51<1:27:43,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:22,451] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3431/6501 [1:38:53<1:29:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1285, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3431/6501 [1:38:53<1:29:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3432/6501 [1:38:55<1:27:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4297, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3432/6501 [1:38:55<1:27:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:26,054] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3433/6501 [1:38:57<1:31:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9559, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3433/6501 [1:38:57<1:31:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:28,011] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3434/6501 [1:38:59<1:34:09,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0259, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3434/6501 [1:38:59<1:34:09,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:29,860] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3435/6501 [1:39:01<1:34:13,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0737, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3435/6501 [1:39:01<1:34:13,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:31,729] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3436/6501 [1:39:02<1:34:34,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2228, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3436/6501 [1:39:02<1:34:34,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:33,554] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3437/6501 [1:39:04<1:34:08,  1.84s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3437/6501 [1:39:04<1:34:08,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3173, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3438/6501 [1:39:06<1:33:53,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3105, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3438/6501 [1:39:06<1:33:53,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:37,109] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3439/6501 [1:39:08<1:32:07,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9498, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3439/6501 [1:39:08<1:32:07,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:38,817] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3440/6501 [1:39:10<1:30:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4193, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3440/6501 [1:39:10<1:30:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3441/6501 [1:39:11<1:26:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2739, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3441/6501 [1:39:11<1:26:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:42,045] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3442/6501 [1:39:13<1:26:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1811, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3442/6501 [1:39:13<1:26:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:44,070] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3443/6501 [1:39:15<1:31:38,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1751, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3443/6501 [1:39:15<1:31:38,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:45,845] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3444/6501 [1:39:17<1:31:15,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1926, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3444/6501 [1:39:17<1:31:15,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:47,434] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3445/6501 [1:39:18<1:28:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2651, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3445/6501 [1:39:18<1:28:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:49,091] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3446/6501 [1:39:20<1:26:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0768, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3446/6501 [1:39:20<1:26:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:50,965] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3447/6501 [1:39:22<1:29:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0759, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3447/6501 [1:39:22<1:29:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:52,817] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3448/6501 [1:39:24<1:30:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1891, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3448/6501 [1:39:24<1:30:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3449/6501 [1:39:25<1:28:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1223, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3449/6501 [1:39:25<1:28:48,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:56,294] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3450/6501 [1:39:27<1:29:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1475, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3450/6501 [1:39:27<1:29:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3451/6501 [1:39:29<1:27:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9338, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3451/6501 [1:39:29<1:27:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:58:59,689] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3452/6501 [1:39:30<1:28:32,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2008, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3452/6501 [1:39:30<1:28:32,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:01,666] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3453/6501 [1:39:32<1:32:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0188, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3453/6501 [1:39:32<1:32:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3454/6501 [1:39:34<1:27:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0938, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3454/6501 [1:39:34<1:27:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:05,124] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3455/6501 [1:39:36<1:30:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3649, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3455/6501 [1:39:36<1:30:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3456/6501 [1:39:38<1:33:11,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5959, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3456/6501 [1:39:38<1:33:11,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:08,341] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3457/6501 [1:39:39<1:24:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2152, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3457/6501 [1:39:39<1:24:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:10,061] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3458/6501 [1:39:41<1:25:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3169, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3458/6501 [1:39:41<1:25:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3459/6501 [1:39:42<1:25:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.096, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3459/6501 [1:39:42<1:25:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3460/6501 [1:39:44<1:27:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3134, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3460/6501 [1:39:44<1:27:17,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:15,027] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3461/6501 [1:39:46<1:23:17,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4377, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3461/6501 [1:39:46<1:23:17,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:16,879] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3462/6501 [1:39:48<1:26:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4664, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3462/6501 [1:39:48<1:26:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3463/6501 [1:39:49<1:25:05,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9552, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3463/6501 [1:39:49<1:25:05,  1.68s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3464/6501 [1:39:51<1:23:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7413, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3464/6501 [1:39:51<1:23:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:21,869] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3465/6501 [1:39:53<1:25:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4857, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3465/6501 [1:39:53<1:25:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:23,677] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3466/6501 [1:39:54<1:27:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.009, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3466/6501 [1:39:54<1:27:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3467/6501 [1:39:56<1:26:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2809, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3467/6501 [1:39:56<1:26:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:27,245] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3468/6501 [1:39:58<1:29:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5914, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3468/6501 [1:39:58<1:29:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:29,081] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3469/6501 [1:40:00<1:30:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1243, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3469/6501 [1:40:00<1:30:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:30,894] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3470/6501 [1:40:02<1:30:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.188, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3470/6501 [1:40:02<1:30:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:32,554] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3471/6501 [1:40:03<1:28:33,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1358, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3471/6501 [1:40:03<1:28:33,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:34,555] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3472/6501 [1:40:05<1:32:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.307, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3472/6501 [1:40:05<1:32:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:36,629] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3473/6501 [1:40:07<1:35:58,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.241, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3473/6501 [1:40:07<1:35:58,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:38,325] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3474/6501 [1:40:09<1:32:49,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4128, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3474/6501 [1:40:09<1:32:49,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:39,938] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3475/6501 [1:40:11<1:29:21,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0573, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3475/6501 [1:40:11<1:29:21,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:41,906] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3476/6501 [1:40:13<1:32:17,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.11, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3476/6501 [1:40:13<1:32:17,  1.83s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3477/6501 [1:40:14<1:30:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.813, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3477/6501 [1:40:14<1:30:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:45,576] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3478/6501 [1:40:16<1:32:49,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1246, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 3478/6501 [1:40:16<1:32:49,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:47,412] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3479/6501 [1:40:18<1:32:41,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0486, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3479/6501 [1:40:18<1:32:41,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:49,287] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3480/6501 [1:40:20<1:33:10,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8375, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3480/6501 [1:40:20<1:33:10,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:50,979] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3481/6501 [1:40:22<1:30:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.144, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3481/6501 [1:40:22<1:30:45,  1.80s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3482/6501 [1:40:24<1:31:25,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2629, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3482/6501 [1:40:24<1:31:25,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:54,416] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3483/6501 [1:40:25<1:27:56,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.639, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3483/6501 [1:40:25<1:27:56,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:56,310] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3484/6501 [1:40:27<1:30:06,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1594, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3484/6501 [1:40:27<1:30:06,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 09:59:58,425] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3485/6501 [1:40:29<1:34:56,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8727, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3485/6501 [1:40:29<1:34:56,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:00,061] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3486/6501 [1:40:31<1:31:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3486/6501 [1:40:31<1:31:05,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1597, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:02,252] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3487/6501 [1:40:33<1:36:46,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0663, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3487/6501 [1:40:33<1:36:46,  1.93s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3488/6501 [1:40:35<1:36:59,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2508, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3488/6501 [1:40:35<1:36:59,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:06,261] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3489/6501 [1:40:37<1:38:59,  1.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1353, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3489/6501 [1:40:37<1:38:59,  1.97s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:08,076] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3490/6501 [1:40:39<1:36:35,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3238, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3490/6501 [1:40:39<1:36:35,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:09,605] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3491/6501 [1:40:40<1:30:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1224, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3491/6501 [1:40:40<1:30:35,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:11,360] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3492/6501 [1:40:42<1:29:48,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4261, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3492/6501 [1:40:42<1:29:48,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:12,910] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3493/6501 [1:40:44<1:26:09,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4366, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3493/6501 [1:40:44<1:26:09,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:14,856] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3494/6501 [1:40:46<1:29:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8486, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 3494/6501 [1:40:46<1:29:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:16,737] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3495/6501 [1:40:47<1:30:56,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1156, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3495/6501 [1:40:47<1:30:56,  1.82s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3496/6501 [1:40:49<1:28:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4612, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3496/6501 [1:40:49<1:28:54,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:20,364] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3497/6501 [1:40:51<1:31:25,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1777, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3497/6501 [1:40:51<1:31:25,  1.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3498/6501 [1:40:53<1:26:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.029, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3498/6501 [1:40:53<1:26:34,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:23,764] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3499/6501 [1:40:54<1:29:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2047, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3499/6501 [1:40:54<1:29:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:25,406] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3500/6501 [1:40:56<1:26:55,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0477, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3500/6501 [1:40:56<1:26:55,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:27,388] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3501/6501 [1:40:58<1:30:33,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3324, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3501/6501 [1:40:58<1:30:33,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:28,942] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3502/6501 [1:41:00<1:26:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9747, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3502/6501 [1:41:00<1:26:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:30,815] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3503/6501 [1:41:02<1:28:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3159, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3503/6501 [1:41:02<1:28:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:32,504] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3504/6501 [1:41:03<1:27:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1127, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3504/6501 [1:41:03<1:27:23,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:34,514] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3505/6501 [1:41:05<1:31:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4503, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3505/6501 [1:41:05<1:31:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3506/6501 [1:41:07<1:30:06,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4175, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3506/6501 [1:41:07<1:30:06,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:37,982] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3507/6501 [1:41:09<1:28:44,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4423, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3507/6501 [1:41:09<1:28:44,  1.78s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3508/6501 [1:41:11<1:30:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3233, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3508/6501 [1:41:11<1:30:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:41,956] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3509/6501 [1:41:13<1:34:15,  1.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2835, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3509/6501 [1:41:13<1:34:15,  1.89s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3510/6501 [1:41:14<1:20:22,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3531, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3510/6501 [1:41:14<1:20:22,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:44,712] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3511/6501 [1:41:15<1:23:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3267, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3511/6501 [1:41:15<1:23:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3512/6501 [1:41:17<1:21:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7256, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3512/6501 [1:41:17<1:21:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:48,049] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3513/6501 [1:41:19<1:23:23,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6936, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3513/6501 [1:41:19<1:23:23,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:49,756] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3514/6501 [1:41:20<1:23:51,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1699, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3514/6501 [1:41:20<1:23:51,  1.68s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3515/6501 [1:41:22<1:20:04,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2234, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3515/6501 [1:41:22<1:20:04,  1.61s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3516/6501 [1:41:23<1:19:08,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5156, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3516/6501 [1:41:23<1:19:08,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:54,546] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3517/6501 [1:41:25<1:22:21,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9554, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3517/6501 [1:41:25<1:22:21,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:00:56,433] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3518/6501 [1:41:27<1:25:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1026, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3518/6501 [1:41:27<1:25:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3519/6501 [1:41:29<1:31:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1733, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3519/6501 [1:41:29<1:31:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:00,707] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3520/6501 [1:41:31<1:36:16,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2148, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3520/6501 [1:41:31<1:36:16,  1.94s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:02,511] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3521/6501 [1:41:33<1:34:15,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4503, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3521/6501 [1:41:33<1:34:15,  1.90s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3522/6501 [1:41:35<1:32:56,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.269, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3522/6501 [1:41:35<1:32:56,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:06,101] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3523/6501 [1:41:37<1:31:31,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.335, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3523/6501 [1:41:37<1:31:31,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:07,839] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3524/6501 [1:41:39<1:29:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7133, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3524/6501 [1:41:39<1:29:54,  1.81s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3525/6501 [1:41:40<1:28:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1098, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3525/6501 [1:41:40<1:28:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:11,302] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3526/6501 [1:41:42<1:27:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4775, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3526/6501 [1:41:42<1:27:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:12,770] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3527/6501 [1:41:43<1:23:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1314, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3527/6501 [1:41:43<1:23:16,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:14,521] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3528/6501 [1:41:45<1:24:17,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1807, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3528/6501 [1:41:45<1:24:17,  1.70s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3529/6501 [1:41:47<1:27:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.043, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3529/6501 [1:41:47<1:27:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3530/6501 [1:41:49<1:23:30,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1989, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3530/6501 [1:41:49<1:23:30,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:19,930] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3531/6501 [1:41:51<1:28:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1896, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3531/6501 [1:41:51<1:28:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:21,514] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3532/6501 [1:41:52<1:25:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3532/6501 [1:41:52<1:25:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.66, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3533/6501 [1:41:54<1:24:44,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2098, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3533/6501 [1:41:54<1:24:44,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:24,801] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3534/6501 [1:41:55<1:22:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2087, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3534/6501 [1:41:55<1:22:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:26,597] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3535/6501 [1:41:57<1:24:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4849, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3535/6501 [1:41:57<1:24:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3536/6501 [1:41:58<1:16:35,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2181, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3536/6501 [1:41:58<1:16:35,  1.55s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3537/6501 [1:42:00<1:13:32,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1771, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3537/6501 [1:42:00<1:13:32,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:30,815] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3538/6501 [1:42:02<1:16:39,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1803, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3538/6501 [1:42:02<1:16:39,  1.55s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3539/6501 [1:42:03<1:21:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1449, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3539/6501 [1:42:03<1:21:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:34,887] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3540/6501 [1:42:06<1:29:25,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1293, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3540/6501 [1:42:06<1:29:25,  1.81s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3541/6501 [1:42:07<1:30:53,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1932, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3541/6501 [1:42:07<1:30:53,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:38,378] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3542/6501 [1:42:09<1:26:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6406, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3542/6501 [1:42:09<1:26:55,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:40,036] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3543/6501 [1:42:11<1:25:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0387, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 3543/6501 [1:42:11<1:25:21,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:41,909] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3544/6501 [1:42:13<1:27:25,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4045, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3544/6501 [1:42:13<1:27:25,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:43,839] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3545/6501 [1:42:15<1:29:41,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5829, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3545/6501 [1:42:15<1:29:41,  1.82s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3546/6501 [1:42:16<1:30:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.181, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3546/6501 [1:42:16<1:30:00,  1.83s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3547/6501 [1:42:18<1:25:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0663, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3547/6501 [1:42:18<1:25:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:48,868] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3548/6501 [1:42:20<1:24:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0092, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3548/6501 [1:42:20<1:24:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:50,669] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3549/6501 [1:42:21<1:25:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3982, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3549/6501 [1:42:21<1:25:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:52,436] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3550/6501 [1:42:23<1:26:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3791, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3550/6501 [1:42:23<1:26:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3551/6501 [1:42:25<1:21:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1293, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3551/6501 [1:42:25<1:21:53,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:55,584] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3552/6501 [1:42:26<1:22:01,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0987, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3552/6501 [1:42:26<1:22:01,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:57,636] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3553/6501 [1:42:28<1:27:39,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2016, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3553/6501 [1:42:28<1:27:39,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:01:59,282] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3554/6501 [1:42:30<1:25:35,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.07, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3554/6501 [1:42:30<1:25:35,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:01,080] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3555/6501 [1:42:32<1:26:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0093, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3555/6501 [1:42:32<1:26:22,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:02,894] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3556/6501 [1:42:34<1:27:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1293, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3556/6501 [1:42:34<1:27:09,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:04,707] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3557/6501 [1:42:35<1:27:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1636, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3557/6501 [1:42:35<1:27:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3558/6501 [1:42:37<1:24:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3345, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3558/6501 [1:42:37<1:24:43,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:07,919] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3559/6501 [1:42:39<1:23:10,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2228, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3559/6501 [1:42:39<1:23:10,  1.70s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3560/6501 [1:42:40<1:20:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5174, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3560/6501 [1:42:40<1:20:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:11,097] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3561/6501 [1:42:42<1:20:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9917, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3561/6501 [1:42:42<1:20:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:12,798] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3562/6501 [1:42:43<1:21:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.303, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3562/6501 [1:42:43<1:21:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:14,430] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3563/6501 [1:42:45<1:20:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3302, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3563/6501 [1:42:45<1:20:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3564/6501 [1:42:47<1:21:16,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0439, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3564/6501 [1:42:47<1:21:16,  1.66s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3565/6501 [1:42:49<1:22:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.719, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3565/6501 [1:42:49<1:22:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:19,494] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3566/6501 [1:42:50<1:21:57,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9266, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3566/6501 [1:42:50<1:21:57,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:21,208] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3567/6501 [1:42:52<1:22:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3737, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3567/6501 [1:42:52<1:22:29,  1.69s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3568/6501 [1:42:54<1:24:37,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8831, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3568/6501 [1:42:54<1:24:37,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:24,783] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3569/6501 [1:42:55<1:24:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1512, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3569/6501 [1:42:55<1:24:44,  1.73s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3570/6501 [1:42:57<1:24:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8106, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3570/6501 [1:42:57<1:24:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:28,199] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3571/6501 [1:42:59<1:23:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4405, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3571/6501 [1:42:59<1:23:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:29,710] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3572/6501 [1:43:00<1:20:51,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0537, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3572/6501 [1:43:00<1:20:51,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:31,327] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3573/6501 [1:43:02<1:20:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1082, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3573/6501 [1:43:02<1:20:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:33,205] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3574/6501 [1:43:04<1:23:39,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5392, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3574/6501 [1:43:04<1:23:39,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3575/6501 [1:43:06<1:28:04,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0754, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 3575/6501 [1:43:06<1:28:04,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:36,846] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3576/6501 [1:43:08<1:25:20,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2464, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3576/6501 [1:43:08<1:25:20,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:38,716] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3577/6501 [1:43:09<1:27:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3577/6501 [1:43:09<1:27:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:40,248] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3578/6501 [1:43:11<1:23:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4225, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3578/6501 [1:43:11<1:23:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3579/6501 [1:43:13<1:24:22,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.358, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3579/6501 [1:43:13<1:24:22,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:43,692] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3580/6501 [1:43:14<1:23:16,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3143, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3580/6501 [1:43:14<1:23:16,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:45,575] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3581/6501 [1:43:16<1:25:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.287, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3581/6501 [1:43:16<1:25:45,  1.76s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3582/6501 [1:43:18<1:18:42,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4001, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3582/6501 [1:43:18<1:18:42,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:48,525] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3583/6501 [1:43:19<1:19:25,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5819, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3583/6501 [1:43:19<1:19:25,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:50,186] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3584/6501 [1:43:21<1:19:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3307, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3584/6501 [1:43:21<1:19:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:52,017] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3585/6501 [1:43:23<1:22:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9962, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3585/6501 [1:43:23<1:22:32,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:54,108] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3586/6501 [1:43:25<1:28:14,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9945, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3586/6501 [1:43:25<1:28:14,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:56,082] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3587/6501 [1:43:27<1:30:30,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1846, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3587/6501 [1:43:27<1:30:30,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:02:58,185] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3588/6501 [1:43:29<1:33:57,  1.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4525, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3588/6501 [1:43:29<1:33:57,  1.94s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3589/6501 [1:43:31<1:30:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.161, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3589/6501 [1:43:31<1:30:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:01,497] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3590/6501 [1:43:32<1:26:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1874, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3590/6501 [1:43:32<1:26:45,  1.79s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3591/6501 [1:43:34<1:30:12,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0656, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3591/6501 [1:43:34<1:30:12,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:05,217] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3592/6501 [1:43:36<1:27:44,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1157, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3592/6501 [1:43:36<1:27:44,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:07,229] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3593/6501 [1:43:38<1:30:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.242, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3593/6501 [1:43:38<1:30:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:09,275] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3594/6501 [1:43:40<1:33:10,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2568, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3594/6501 [1:43:40<1:33:10,  1.92s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3595/6501 [1:43:42<1:31:55,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7522, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3595/6501 [1:43:42<1:31:55,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:13,012] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3596/6501 [1:43:44<1:31:53,  1.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8982, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3596/6501 [1:43:44<1:31:53,  1.90s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3597/6501 [1:43:45<1:29:52,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2338, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3597/6501 [1:43:45<1:29:52,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:16,354] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3598/6501 [1:43:47<1:25:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0204, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3598/6501 [1:43:47<1:25:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:18,382] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3599/6501 [1:43:49<1:29:29,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9601, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3599/6501 [1:43:49<1:29:29,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:19,839] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3600/6501 [1:43:51<1:23:45,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1317, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3600/6501 [1:43:51<1:23:45,  1.73s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3601/6501 [1:43:52<1:19:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3094, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3601/6501 [1:43:52<1:19:58,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:23,035] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3602/6501 [1:43:54<1:20:55,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0236, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3602/6501 [1:43:54<1:20:55,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:24,609] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3603/6501 [1:43:55<1:19:26,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2458, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3603/6501 [1:43:55<1:19:26,  1.64s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3604/6501 [1:43:57<1:19:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3746, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3604/6501 [1:43:57<1:19:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:28,085] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3605/6501 [1:43:59<1:21:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4427, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3605/6501 [1:43:59<1:21:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:29,644] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3606/6501 [1:44:00<1:19:56,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.952, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3606/6501 [1:44:00<1:19:56,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:31,338] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3607/6501 [1:44:02<1:20:26,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.253, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3607/6501 [1:44:02<1:20:26,  1.67s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3608/6501 [1:44:04<1:22:36,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9351, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 3608/6501 [1:44:04<1:22:36,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:35,024] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3609/6501 [1:44:06<1:24:47,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4343, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3609/6501 [1:44:06<1:24:47,  1.76s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3610/6501 [1:44:07<1:24:40,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8117, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3610/6501 [1:44:07<1:24:40,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:38,788] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3611/6501 [1:44:09<1:28:18,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4663, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3611/6501 [1:44:09<1:28:18,  1.83s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3612/6501 [1:44:11<1:26:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.235, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3612/6501 [1:44:11<1:26:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3613/6501 [1:44:12<1:14:11,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2142, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3613/6501 [1:44:12<1:14:11,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:43,364] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3614/6501 [1:44:14<1:19:45,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1584, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3614/6501 [1:44:14<1:19:45,  1.66s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3615/6501 [1:44:16<1:19:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5726, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3615/6501 [1:44:16<1:19:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:46,745] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3616/6501 [1:44:17<1:20:37,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0916, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3616/6501 [1:44:17<1:20:37,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:48,676] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3617/6501 [1:44:19<1:24:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2463, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3617/6501 [1:44:19<1:24:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3618/6501 [1:44:21<1:25:59,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0601, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3618/6501 [1:44:21<1:25:59,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:52,576] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3619/6501 [1:44:23<1:29:21,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9195, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3619/6501 [1:44:23<1:29:21,  1.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3620/6501 [1:44:25<1:27:19,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2523, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3620/6501 [1:44:25<1:27:19,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:56,141] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3621/6501 [1:44:27<1:27:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2283, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3621/6501 [1:44:27<1:27:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:57,932] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3622/6501 [1:44:29<1:27:06,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7666, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3622/6501 [1:44:29<1:27:06,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:03:59,501] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3623/6501 [1:44:30<1:23:31,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0444, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3623/6501 [1:44:30<1:23:31,  1.74s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3624/6501 [1:44:32<1:23:20,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4862, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3624/6501 [1:44:32<1:23:20,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:03,072] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3625/6501 [1:44:34<1:24:47,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0887, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3625/6501 [1:44:34<1:24:47,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:04,746] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3626/6501 [1:44:35<1:23:24,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2843, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3626/6501 [1:44:35<1:23:24,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:06,475] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3627/6501 [1:44:37<1:23:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2255, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3627/6501 [1:44:37<1:23:12,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:08,248] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3628/6501 [1:44:39<1:23:41,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5714, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3628/6501 [1:44:39<1:23:41,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:10,047] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3629/6501 [1:44:41<1:24:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3992, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3629/6501 [1:44:41<1:24:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3630/6501 [1:44:42<1:21:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3336, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3630/6501 [1:44:42<1:21:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:13,286] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3631/6501 [1:44:44<1:21:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.179, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3631/6501 [1:44:44<1:21:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:15,387] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3632/6501 [1:44:46<1:26:54,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.571, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3632/6501 [1:44:46<1:26:54,  1.82s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3633/6501 [1:44:47<1:21:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8703, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3633/6501 [1:44:47<1:21:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:18,909] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3634/6501 [1:44:50<1:26:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9919, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3634/6501 [1:44:50<1:26:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:20,513] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3635/6501 [1:44:51<1:23:51,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3139, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3635/6501 [1:44:51<1:23:51,  1.76s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3636/6501 [1:44:53<1:23:39,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2358, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3636/6501 [1:44:53<1:23:39,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:24,122] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3637/6501 [1:44:55<1:25:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8552, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3637/6501 [1:44:55<1:25:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:25,730] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3638/6501 [1:44:56<1:22:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3852, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3638/6501 [1:44:56<1:22:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:27,610] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3639/6501 [1:44:58<1:24:44,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5665, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3639/6501 [1:44:58<1:24:44,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:29,290] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3640/6501 [1:45:00<1:23:20,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8732, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3640/6501 [1:45:00<1:23:20,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:31,238] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3641/6501 [1:45:02<1:26:10,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3447, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3641/6501 [1:45:02<1:26:10,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:33,280] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3642/6501 [1:45:04<1:29:29,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3129, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3642/6501 [1:45:04<1:29:29,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:35,310] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3643/6501 [1:45:06<1:31:37,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4296, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3643/6501 [1:45:06<1:31:37,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:37,057] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3644/6501 [1:45:08<1:29:04,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1787, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3644/6501 [1:45:08<1:29:04,  1.87s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3645/6501 [1:45:10<1:27:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4552, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3645/6501 [1:45:10<1:27:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3646/6501 [1:45:11<1:22:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1877, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3646/6501 [1:45:11<1:22:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:42,041] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3647/6501 [1:45:13<1:22:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1141, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3647/6501 [1:45:13<1:22:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:43,751] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3648/6501 [1:45:14<1:22:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.389, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3648/6501 [1:45:14<1:22:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3649/6501 [1:45:16<1:21:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3649/6501 [1:45:16<1:21:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0429, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:47,475] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3650/6501 [1:45:18<1:25:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2242, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3650/6501 [1:45:18<1:25:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3651/6501 [1:45:19<1:17:53,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.282, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3651/6501 [1:45:19<1:17:53,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:50,595] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3652/6501 [1:45:21<1:21:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3652/6501 [1:45:21<1:21:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0309, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:52,501] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3653/6501 [1:45:23<1:23:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.987, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3653/6501 [1:45:23<1:23:59,  1.77s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3654/6501 [1:45:25<1:26:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8912, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3654/6501 [1:45:25<1:26:38,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:55,782] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3655/6501 [1:45:26<1:19:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0228, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3655/6501 [1:45:26<1:19:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3656/6501 [1:45:28<1:22:35,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1553, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 3656/6501 [1:45:28<1:22:35,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:04:59,564] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3657/6501 [1:45:30<1:24:35,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8402, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3657/6501 [1:45:30<1:24:35,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:01,503] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3658/6501 [1:45:32<1:26:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3398, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3658/6501 [1:45:32<1:26:45,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:03,598] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3659/6501 [1:45:34<1:30:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1932, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3659/6501 [1:45:34<1:30:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3660/6501 [1:45:36<1:27:06,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.567, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3660/6501 [1:45:36<1:27:06,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3661/6501 [1:45:38<1:25:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8767, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3661/6501 [1:45:38<1:25:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:08,414] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3662/6501 [1:45:39<1:19:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2585, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3662/6501 [1:45:39<1:19:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:10,206] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3663/6501 [1:45:41<1:21:16,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0451, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3663/6501 [1:45:41<1:21:16,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:11,922] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3664/6501 [1:45:43<1:21:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3127, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3664/6501 [1:45:43<1:21:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3665/6501 [1:45:45<1:24:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0922, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3665/6501 [1:45:45<1:24:00,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:15,714] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3666/6501 [1:45:46<1:25:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0995, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3666/6501 [1:45:46<1:25:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3667/6501 [1:45:48<1:18:34,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3619, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3667/6501 [1:45:48<1:18:34,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:18,836] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3668/6501 [1:45:50<1:20:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1205, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3668/6501 [1:45:50<1:20:22,  1.70s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3669/6501 [1:45:51<1:18:23,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9436, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3669/6501 [1:45:51<1:18:23,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:22,231] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3670/6501 [1:45:53<1:20:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1484, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3670/6501 [1:45:53<1:20:46,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:24,036] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3671/6501 [1:45:55<1:22:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5762, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3671/6501 [1:45:55<1:22:03,  1.74s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3672/6501 [1:45:57<1:23:24,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2719, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3672/6501 [1:45:57<1:23:24,  1.77s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3673/6501 [1:45:58<1:21:10,  1.72s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 3673/6501 [1:45:58<1:21:10,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.115, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:29,094] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3674/6501 [1:46:00<1:19:31,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5049, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3674/6501 [1:46:00<1:19:31,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:30,540] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3675/6501 [1:46:01<1:16:04,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2484, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3675/6501 [1:46:01<1:16:04,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:32,204] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3676/6501 [1:46:03<1:16:44,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.235, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3676/6501 [1:46:03<1:16:44,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:33,894] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3677/6501 [1:46:05<1:17:34,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.17, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3677/6501 [1:46:05<1:17:34,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:35,803] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3678/6501 [1:46:06<1:21:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9464, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3678/6501 [1:46:07<1:21:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3679/6501 [1:46:08<1:18:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.991, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3679/6501 [1:46:08<1:18:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:39,183] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3680/6501 [1:46:10<1:20:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.273, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3680/6501 [1:46:10<1:20:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3681/6501 [1:46:12<1:22:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0058, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3681/6501 [1:46:12<1:22:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:43,008] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3682/6501 [1:46:14<1:25:46,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8732, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3682/6501 [1:46:14<1:25:46,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:45,005] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3683/6501 [1:46:16<1:28:09,  1.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0316, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3683/6501 [1:46:16<1:28:09,  1.88s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:46,637] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3684/6501 [1:46:17<1:24:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3684/6501 [1:46:17<1:24:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2869, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:48,509] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3685/6501 [1:46:19<1:25:36,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2875, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3685/6501 [1:46:19<1:25:36,  1.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3686/6501 [1:46:21<1:21:58,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3895, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3686/6501 [1:46:21<1:21:58,  1.75s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3687/6501 [1:46:22<1:20:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6343, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3687/6501 [1:46:22<1:20:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:53,579] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3688/6501 [1:46:24<1:22:36,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4257, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3688/6501 [1:46:24<1:22:36,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:54,740] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3689/6501 [1:46:25<1:14:07,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.663, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3689/6501 [1:46:25<1:14:07,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:56,504] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3690/6501 [1:46:27<1:16:39,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2946, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3690/6501 [1:46:27<1:16:39,  1.64s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3691/6501 [1:46:29<1:15:16,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2339, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3691/6501 [1:46:29<1:15:16,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:05:59,925] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3692/6501 [1:46:31<1:19:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4818, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3692/6501 [1:46:31<1:19:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3693/6501 [1:46:33<1:22:05,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.899, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3693/6501 [1:46:33<1:22:05,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:03,443] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3694/6501 [1:46:34<1:20:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2194, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3694/6501 [1:46:34<1:20:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3695/6501 [1:46:36<1:21:39,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1949, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3695/6501 [1:46:36<1:21:39,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:07,055] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3696/6501 [1:46:38<1:22:11,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0509, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3696/6501 [1:46:38<1:22:11,  1.76s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3697/6501 [1:46:39<1:19:53,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.006, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3697/6501 [1:46:39<1:19:53,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:10,221] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3698/6501 [1:46:41<1:17:54,  1.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6313, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3698/6501 [1:46:41<1:17:54,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:11,831] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3699/6501 [1:46:43<1:17:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4259, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3699/6501 [1:46:43<1:17:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:13,464] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3700/6501 [1:46:44<1:16:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2635, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3700/6501 [1:46:44<1:16:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:14,999] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3701/6501 [1:46:46<1:15:13,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5477, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3701/6501 [1:46:46<1:15:13,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:16,686] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3702/6501 [1:46:47<1:16:15,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3068, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3702/6501 [1:46:47<1:16:15,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3703/6501 [1:46:49<1:16:22,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8233, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3703/6501 [1:46:49<1:16:22,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:19,974] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3704/6501 [1:46:51<1:16:25,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0472, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3704/6501 [1:46:51<1:16:25,  1.64s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3705/6501 [1:46:52<1:16:42,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3312, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3705/6501 [1:46:52<1:16:42,  1.65s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3706/6501 [1:46:54<1:16:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0762, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3706/6501 [1:46:54<1:16:12,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:24,809] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3707/6501 [1:46:56<1:15:07,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0912, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3707/6501 [1:46:56<1:15:07,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:26,729] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3708/6501 [1:46:57<1:19:23,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3724, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3708/6501 [1:46:57<1:19:23,  1.71s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3709/6501 [1:46:59<1:20:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2808, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3709/6501 [1:46:59<1:20:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:30,133] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3710/6501 [1:47:01<1:18:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3665, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3710/6501 [1:47:01<1:18:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:31,980] [WARNING] [stage3.py:1787:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3711/6501 [1:47:03<1:20:54,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6828, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3711/6501 [1:47:03<1:20:54,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:33,531] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3712/6501 [1:47:04<1:18:14,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0841, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3712/6501 [1:47:04<1:18:14,  1.68s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3713/6501 [1:47:06<1:20:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4216, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3713/6501 [1:47:06<1:20:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:37,149] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3714/6501 [1:47:08<1:21:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3047, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3714/6501 [1:47:08<1:21:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3715/6501 [1:47:10<1:23:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4794, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3715/6501 [1:47:10<1:23:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:40,722] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3716/6501 [1:47:11<1:21:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.244, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3716/6501 [1:47:11<1:21:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3717/6501 [1:47:13<1:20:52,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4047, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3717/6501 [1:47:13<1:20:52,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:44,103] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3718/6501 [1:47:15<1:19:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2162, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3718/6501 [1:47:15<1:19:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:45,933] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3719/6501 [1:47:17<1:21:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5842, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3719/6501 [1:47:17<1:21:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:47,353] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3720/6501 [1:47:18<1:16:38,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.651, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3720/6501 [1:47:18<1:16:38,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:49,092] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3721/6501 [1:47:20<1:17:48,  1.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8598, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3721/6501 [1:47:20<1:17:48,  1.68s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3722/6501 [1:47:22<1:18:56,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9578, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3722/6501 [1:47:22<1:18:56,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:52,648] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3723/6501 [1:47:23<1:20:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7422, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3723/6501 [1:47:23<1:20:07,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:54,306] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3724/6501 [1:47:25<1:19:05,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3305, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3724/6501 [1:47:25<1:19:05,  1.71s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3725/6501 [1:47:26<1:09:24,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1149, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3725/6501 [1:47:26<1:09:24,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:56,897] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3726/6501 [1:47:28<1:10:27,  1.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5359, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3726/6501 [1:47:28<1:10:27,  1.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:06:58,534] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3727/6501 [1:47:29<1:12:00,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2754, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3727/6501 [1:47:29<1:12:00,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:00,109] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3728/6501 [1:47:31<1:12:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3607, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3728/6501 [1:47:31<1:12:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:01,641] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3729/6501 [1:47:32<1:11:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1152, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3729/6501 [1:47:32<1:11:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:03,405] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3730/6501 [1:47:34<1:14:39,  1.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0942, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3730/6501 [1:47:34<1:14:39,  1.62s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3731/6501 [1:47:36<1:18:28,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.601, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3731/6501 [1:47:36<1:18:28,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:06,996] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3732/6501 [1:47:38<1:18:24,  1.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2914, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3732/6501 [1:47:38<1:18:24,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:08,918] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3733/6501 [1:47:40<1:21:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4533, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3733/6501 [1:47:40<1:21:27,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:10,850] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3734/6501 [1:47:42<1:23:44,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.108, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3734/6501 [1:47:42<1:23:44,  1.82s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3735/6501 [1:47:43<1:25:18,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0226, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3735/6501 [1:47:43<1:25:18,  1.85s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:14,403] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3736/6501 [1:47:45<1:22:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2335, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3736/6501 [1:47:45<1:22:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3737/6501 [1:47:47<1:24:58,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1699, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3737/6501 [1:47:47<1:24:58,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:18,168] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3738/6501 [1:47:49<1:23:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1446, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 3738/6501 [1:47:49<1:23:58,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:19,634] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3739/6501 [1:47:50<1:18:59,  1.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2648, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3739/6501 [1:47:50<1:18:59,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:21,413] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3740/6501 [1:47:52<1:19:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0297, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3740/6501 [1:47:52<1:19:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3741/6501 [1:47:54<1:19:52,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5398, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3741/6501 [1:47:54<1:19:52,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:24,813] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3742/6501 [1:47:56<1:18:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1953, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3742/6501 [1:47:56<1:18:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3743/6501 [1:47:57<1:17:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1869, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3743/6501 [1:47:57<1:17:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3744/6501 [1:47:58<1:11:18,  1.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6245, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3744/6501 [1:47:58<1:11:18,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:29,387] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3745/6501 [1:48:00<1:13:31,  1.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1006, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3745/6501 [1:48:00<1:13:31,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:31,185] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3746/6501 [1:48:02<1:16:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4159, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3746/6501 [1:48:02<1:16:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:32,688] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3747/6501 [1:48:03<1:14:01,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2211, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3747/6501 [1:48:03<1:14:01,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:34,276] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3748/6501 [1:48:05<1:13:40,  1.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1329, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3748/6501 [1:48:05<1:13:40,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:36,216] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3749/6501 [1:48:07<1:18:14,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6523, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3749/6501 [1:48:07<1:18:14,  1.71s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3750/6501 [1:48:09<1:21:05,  1.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0642, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3750/6501 [1:48:09<1:21:05,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:39,809] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3751/6501 [1:48:11<1:19:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6851, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3751/6501 [1:48:11<1:19:48,  1.74s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3752/6501 [1:48:12<1:20:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1644, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3752/6501 [1:48:12<1:20:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:43,284] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3753/6501 [1:48:14<1:19:25,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1777, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3753/6501 [1:48:14<1:19:25,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:44,940] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3754/6501 [1:48:16<1:18:19,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6707, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3754/6501 [1:48:16<1:18:19,  1.71s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3755/6501 [1:48:17<1:17:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0363, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3755/6501 [1:48:17<1:17:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:48,254] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3756/6501 [1:48:19<1:17:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3358, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3756/6501 [1:48:19<1:17:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3757/6501 [1:48:21<1:20:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2575, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3757/6501 [1:48:21<1:20:06,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:52,010] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3758/6501 [1:48:23<1:21:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4008, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3758/6501 [1:48:23<1:21:26,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:53,774] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3759/6501 [1:48:24<1:21:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0144, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3759/6501 [1:48:24<1:21:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3760/6501 [1:48:26<1:22:37,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1228, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3760/6501 [1:48:26<1:22:37,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:07:57,315] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3761/6501 [1:48:28<1:20:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0636, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3761/6501 [1:48:28<1:20:30,  1.76s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3762/6501 [1:48:30<1:21:15,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3667, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3762/6501 [1:48:30<1:21:15,  1.78s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3763/6501 [1:48:32<1:20:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7094, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3763/6501 [1:48:32<1:20:20,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:02,800] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3764/6501 [1:48:33<1:22:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2341, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3764/6501 [1:48:33<1:22:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:04,586] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3765/6501 [1:48:35<1:22:26,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3839, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3765/6501 [1:48:35<1:22:26,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:06,431] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3766/6501 [1:48:37<1:22:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1913, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3766/6501 [1:48:37<1:22:55,  1.82s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3767/6501 [1:48:39<1:21:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2079, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3767/6501 [1:48:39<1:21:01,  1.78s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3768/6501 [1:48:40<1:15:36,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8966, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3768/6501 [1:48:40<1:15:36,  1.66s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3769/6501 [1:48:41<1:07:44,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3469, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3769/6501 [1:48:41<1:07:44,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:12,251] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3770/6501 [1:48:43<1:10:10,  1.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3044, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3770/6501 [1:48:43<1:10:10,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:13,936] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3771/6501 [1:48:45<1:12:05,  1.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5251, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3771/6501 [1:48:45<1:12:05,  1.58s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3772/6501 [1:48:46<1:12:18,  1.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0925, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3772/6501 [1:48:46<1:12:18,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:17,271] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3773/6501 [1:48:48<1:14:14,  1.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0447, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3773/6501 [1:48:48<1:14:14,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:19,167] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3774/6501 [1:48:50<1:17:48,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3069, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3774/6501 [1:48:50<1:17:48,  1.71s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3775/6501 [1:48:52<1:18:49,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.049, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3775/6501 [1:48:52<1:18:49,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:22,706] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3776/6501 [1:48:53<1:19:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7346, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3776/6501 [1:48:53<1:19:00,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:24,690] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3777/6501 [1:48:55<1:22:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3777/6501 [1:48:55<1:22:18,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2578, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3778/6501 [1:48:57<1:22:15,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7586, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3778/6501 [1:48:57<1:22:15,  1.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:28,494] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3779/6501 [1:48:59<1:24:40,  1.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7973, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3779/6501 [1:48:59<1:24:40,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:30,094] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3780/6501 [1:49:01<1:21:00,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6252, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3780/6501 [1:49:01<1:21:00,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:31,624] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3781/6501 [1:49:02<1:17:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4048, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3781/6501 [1:49:02<1:17:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:33,269] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3782/6501 [1:49:04<1:16:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0048, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3782/6501 [1:49:04<1:16:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:35,082] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3783/6501 [1:49:06<1:18:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0843, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3783/6501 [1:49:06<1:18:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:37,463] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3784/6501 [1:49:08<1:27:05,  1.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.28, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3784/6501 [1:49:08<1:27:05,  1.92s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3785/6501 [1:49:10<1:24:20,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0292, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3785/6501 [1:49:10<1:24:20,  1.86s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3786/6501 [1:49:11<1:20:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0005, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3786/6501 [1:49:11<1:20:49,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:42,442] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3787/6501 [1:49:13<1:18:55,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1463, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3787/6501 [1:49:13<1:18:55,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:44,441] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3788/6501 [1:49:15<1:22:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.309, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3788/6501 [1:49:15<1:22:21,  1.82s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3789/6501 [1:49:17<1:18:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2261, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3789/6501 [1:49:17<1:18:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:47,759] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3790/6501 [1:49:18<1:19:07,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4169, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3790/6501 [1:49:18<1:19:07,  1.75s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3791/6501 [1:49:20<1:18:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7618, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3791/6501 [1:49:20<1:18:45,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:51,178] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3792/6501 [1:49:22<1:18:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5253, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3792/6501 [1:49:22<1:18:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:53,279] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3793/6501 [1:49:24<1:23:03,  1.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1632, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3793/6501 [1:49:24<1:23:03,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:54,824] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3794/6501 [1:49:26<1:19:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2513, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3794/6501 [1:49:26<1:19:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3795/6501 [1:49:27<1:14:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1619, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3795/6501 [1:49:27<1:14:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:08:57,869] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3796/6501 [1:49:29<1:14:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.899, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3796/6501 [1:49:29<1:14:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3797/6501 [1:49:30<1:14:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0126, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3797/6501 [1:49:30<1:14:17,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:09:01,733] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3798/6501 [1:49:32<1:21:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2276, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3798/6501 [1:49:32<1:21:43,  1.81s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3799/6501 [1:49:34<1:20:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1018, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3799/6501 [1:49:34<1:20:19,  1.78s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3800/6501 [1:49:36<1:18:22,  1.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9728, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3800/6501 [1:49:36<1:18:22,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-04-14 10:09:07,001] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3801/6501 [1:49:38<1:20:40,  1.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3061, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 3801/6501 [1:49:38<1:20:40,  1.79s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket, # The bucket to store pretrained model and fine-tune model\n",
    "              'MODEL_NAME_S3': model_name_s3\n",
    "}\n",
    "\n",
    "base_job_name = f'stanford_alpaca-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}',          \n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train-13b.sh',\n",
    "                      source_dir='./',\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "estimator.fit()\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd830bcc",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9037587",
   "metadata": {},
   "source": [
    "[SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase)\n",
    "\n",
    "[DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/)\n",
    "\n",
    "[SageMaker Examples](https://github.com/aws/amazon-sagemaker-examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
